{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 2.0.9 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is already saved in the Workspace and can be accessed at the file path provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"/data/Tennis_Linux_NoVis/Tennis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -6.65278625 -1.5        -0.          0.\n",
      "  6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: -0.004999999888241291\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):                                         # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    step = 0 \n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        step += 1\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "def parse_params(params_dir):\n",
    "    with open(params_dir) as fp:\n",
    "        params = json.load(fp)\n",
    "    return params\n",
    "\n",
    "class Actor_critic_model(nn.Module):\n",
    "    def __init__(self, params_dir, input_dim, act_size, num_agents):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.act_size = act_size\n",
    "        self.num_agents = num_agents\n",
    "        self.params = parse_params(params_dir)\n",
    "        self.mu = self.create_module_list(self.create_actor)\n",
    "        self.std =  self.create_std()\n",
    "        self.val = self.create_module_list(self.create_critic)\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_in')\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def create_module_list(self, func):\n",
    "        module_list = nn.ModuleList()\n",
    "        for _ in range(self.num_agents):\n",
    "            module_list.append(func())\n",
    "        return module_list\n",
    "    \n",
    "    def create_std(self):\n",
    "        param_list = nn.ParameterList([nn.Parameter(\n",
    "            torch.ones(1, self.act_size)) for _ in range(self.num_agents)]\n",
    "                        )\n",
    "        return param_list\n",
    "\n",
    "    def create_actor(self):\n",
    "        module_list = nn.ModuleList()\n",
    "        layer = nn.Sequential()\n",
    "        fc = nn.Linear(self.input_dim, self.params['hidden_dim'])\n",
    "        layer.add_module(f\"fc_layer_1\", fc)\n",
    "        # layer.add_module(f\"bn_layer_1\",\n",
    "                        # nn.BatchNorm1d(self.params['hidden_dim']))\n",
    "        # layer.add_module(f\"RELU_layer_1\", nn.LeakyReLU())\n",
    "        layer.add_module(f\"RELU_layer_1\", nn.ReLU())\n",
    "        module_list.append(layer)\n",
    "        # module_list.apply(self._init_weights)\n",
    "        self.add_hidden_layer(module_list, self.params['actor_h_num'],\n",
    "                         self.params['hidden_dim'], self.params['hidden_dim'])\n",
    "        module_list.append(nn.Sequential(nn.Linear(self.params['hidden_dim'],\n",
    "                                          self.act_size)))\n",
    "        # module_list.apply(self._init_weights)\n",
    "        return module_list\n",
    "    \n",
    "    def create_critic(self):\n",
    "        module_list = nn.ModuleList()\n",
    "        layer = nn.Sequential()\n",
    "        fc = nn.Linear(self.input_dim + self.act_size * 2, self.params['hidden_dim'])\n",
    "        layer.add_module(f\"fc_layer_1\", fc)\n",
    "        # layer.add_module(f\"bn_layer_1\",\n",
    "                        # nn.BatchNorm1d(self.params['hidden_dim']))\n",
    "        # layer.add_module(f\"RELU_layer_1\", nn.LeakyReLU())\n",
    "        layer.add_module(f\"RELU_layer_1\", nn.ReLU())\n",
    "        module_list.append(layer)\n",
    "        self.add_hidden_layer(module_list, self.params['critic_h_num'],\n",
    "                         self.params['hidden_dim'], self.params['hidden_dim'])\n",
    "        module_list.append(nn.Sequential(nn.Linear(self.params['hidden_dim'], 1)))\n",
    "        # module_list.apply(self._init_weights)\n",
    "        return module_list\n",
    "    \n",
    "    def add_hidden_layer(self, module_list, num_hidden_layer,\n",
    "                         input_dim, output_dim):\n",
    "        if num_hidden_layer == 0:\n",
    "            return\n",
    "        for i in range(1, num_hidden_layer+1):\n",
    "            layer = nn.Sequential()\n",
    "            fc = nn.Linear(input_dim, output_dim)\n",
    "            layer.add_module(f\"fc_layer_{i}\", fc)\n",
    "            # layer.add_module(f\"bn_layer_{i}\",\n",
    "                          #    nn.BatchNorm1d(output_dim))\n",
    "            # layer.add_module(f\"RELU_layer_{i}\", nn.LeakyReLU())\n",
    "            layer.add_module(f\"RELU_layer_{i}\", nn.ReLU())\n",
    "            module_list.append(layer)\n",
    "            \n",
    "    def forward(self, states, actor=True, train=True, index=None):\n",
    "        '''\n",
    "            If actor is True, output actions and log probabilities FloatTensor\n",
    "            If Critic (actor = False), output state value FloatTensor\n",
    "        '''\n",
    "        x_ = torch.FloatTensor(states).to(self.device)\n",
    "        if actor:\n",
    "            # forward in actor path\n",
    "            mus = torch.zeros(self.num_agents, self.act_size, dtype=torch.float)\n",
    "            dists = []\n",
    "            acts = torch.zeros(self.num_agents, self.act_size, dtype=torch.float)\n",
    "            lps = torch.zeros(self.num_agents, self.act_size, dtype=torch.float)\n",
    "            for i in range(self.num_agents):\n",
    "                \n",
    "                mu_ = x_[i]\n",
    "                for m in self.mu[i]:\n",
    "                    mu_ = m(mu_)\n",
    "                mus[i] = (mu_)\n",
    "                dists.append(torch.distributions.Normal(mus[i], self.std[i]))\n",
    "                act_ = dists[i].sample()\n",
    "                if train:\n",
    "                    # only return log probabilities in training phases\n",
    "                    lps[i] = dists[i].log_prob(act_)\n",
    "                    acts[i] = torch.clamp(act_, -1, 1)\n",
    "                    return acts, lps\n",
    "                # return only actions in executation phases\n",
    "                return torch.clamp(act_, -1, 1)\n",
    "        # forward in value path\n",
    "        for v in self.val[index]:\n",
    "            x_ = v(x_)\n",
    "        return x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dir = f\"./params.txt\"\n",
    "amodel =  Actor_critic_model(params_dir, 24, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.1985, -0.1288,  0.1789,  ..., -0.1654,  0.0376,  0.2027],\n",
       "         [ 0.1717, -0.0791,  0.1173,  ..., -0.1954,  0.1795,  0.0756],\n",
       "         [ 0.1393,  0.0430, -0.1515,  ...,  0.0009,  0.1954, -0.0147],\n",
       "         ...,\n",
       "         [-0.1946, -0.0024, -0.0642,  ...,  0.0591, -0.1154, -0.1974],\n",
       "         [ 0.1854, -0.0100, -0.0588,  ..., -0.1981,  0.0050,  0.0286],\n",
       "         [ 0.0320, -0.1381, -0.1619,  ...,  0.0542,  0.0346, -0.1930]]),\n",
       " Parameter containing:\n",
       " tensor([ 0.0354,  0.2000, -0.0518, -0.0361, -0.1515, -0.1081, -0.0692,\n",
       "          0.0736,  0.0014,  0.1593, -0.1601,  0.0491,  0.1895, -0.0391,\n",
       "          0.1125, -0.0936,  0.0335, -0.1701,  0.1948, -0.1872,  0.1438,\n",
       "          0.1375, -0.1360,  0.0201, -0.0581, -0.1437, -0.0438,  0.1718,\n",
       "         -0.1198, -0.1885, -0.0877, -0.0901, -0.0720,  0.0778, -0.0335,\n",
       "          0.1936,  0.1831, -0.0003,  0.1432, -0.0970, -0.1702,  0.1719,\n",
       "          0.0001,  0.1295, -0.1975,  0.1577, -0.1547, -0.0260,  0.0403,\n",
       "          0.1745,  0.0799, -0.1837, -0.0508,  0.1642, -0.1092, -0.0489,\n",
       "         -0.1706, -0.0340, -0.0933,  0.0849, -0.0224,  0.0619,  0.1284,\n",
       "         -0.1764, -0.2041, -0.0534, -0.1535,  0.0681,  0.1915, -0.1230,\n",
       "         -0.1136, -0.0529, -0.1225,  0.0462,  0.0275,  0.0975, -0.0871,\n",
       "         -0.1238,  0.1697, -0.1663,  0.0222, -0.0591,  0.0712, -0.0095,\n",
       "          0.1516,  0.1666,  0.0365,  0.0141,  0.1176, -0.1729, -0.0644,\n",
       "          0.0955,  0.2000,  0.0148, -0.1617,  0.0405]),\n",
       " Parameter containing:\n",
       " tensor([[-1.7252e-03,  6.2308e-02, -9.5091e-02,  ...,  7.8314e-03,\n",
       "           2.4201e-02,  5.8917e-02],\n",
       "         [-5.8233e-02,  4.8496e-02,  7.2599e-02,  ...,  4.1196e-02,\n",
       "           9.6384e-02,  5.1178e-02],\n",
       "         [-5.1492e-02, -9.8778e-02, -6.4005e-03,  ..., -4.8716e-04,\n",
       "          -6.4842e-02, -6.8142e-02],\n",
       "         ...,\n",
       "         [ 1.2853e-02, -6.7787e-02,  6.6563e-02,  ...,  2.3402e-03,\n",
       "           1.0027e-01, -8.5202e-02],\n",
       "         [ 6.6592e-02, -6.4673e-02, -8.8820e-02,  ..., -2.4608e-02,\n",
       "          -3.0082e-02, -4.6856e-02],\n",
       "         [ 8.8758e-02, -5.2792e-02,  3.0790e-02,  ..., -7.3441e-02,\n",
       "           8.3203e-02,  9.5064e-02]]),\n",
       " Parameter containing:\n",
       " tensor([-0.0035, -0.0989,  0.0104,  0.0538, -0.0079, -0.0016, -0.0708,\n",
       "          0.0699, -0.0628, -0.0573, -0.0983, -0.0075, -0.0355,  0.0728,\n",
       "         -0.0908,  0.0894,  0.0665,  0.0444,  0.0832,  0.0023, -0.0065,\n",
       "         -0.0771, -0.0161, -0.0805,  0.0824, -0.0883, -0.0662,  0.0410,\n",
       "          0.0187, -0.0495,  0.0155, -0.0512,  0.0443,  0.0089,  0.0335,\n",
       "         -0.0736,  0.0247, -0.0687,  0.0114, -0.0609,  0.0374,  0.0828,\n",
       "         -0.0540,  0.0710,  0.0182,  0.0761,  0.0674,  0.0438, -0.0947,\n",
       "          0.0338, -0.0083, -0.0415, -0.1011, -0.0062,  0.0112, -0.0755,\n",
       "          0.0285,  0.0613,  0.0227, -0.0420, -0.0428, -0.0200, -0.0199,\n",
       "          0.0397,  0.0792,  0.0658, -0.0727,  0.0457, -0.0990,  0.0598,\n",
       "          0.0214, -0.0797, -0.0174,  0.0554, -0.0153, -0.0392,  0.0699,\n",
       "         -0.0009,  0.0730, -0.0138, -0.0886,  0.0393, -0.0302, -0.0257,\n",
       "         -0.0932, -0.0235,  0.0173,  0.0918, -0.0719, -0.0636,  0.0038,\n",
       "          0.0778,  0.0086,  0.0076, -0.0665,  0.0251]),\n",
       " Parameter containing:\n",
       " tensor([[-0.0930, -0.0732,  0.0404,  ..., -0.0554,  0.1006,  0.0318],\n",
       "         [-0.0224, -0.0604, -0.1018,  ...,  0.0769,  0.0779, -0.0191],\n",
       "         [-0.0775, -0.0576,  0.0422,  ..., -0.0889,  0.0685, -0.0738],\n",
       "         ...,\n",
       "         [-0.0697, -0.0314,  0.0584,  ...,  0.0453, -0.0021, -0.0917],\n",
       "         [ 0.0003, -0.0159, -0.0880,  ...,  0.0233,  0.0848,  0.0254],\n",
       "         [ 0.0233, -0.0295, -0.0423,  ...,  0.0548,  0.0440, -0.0026]]),\n",
       " Parameter containing:\n",
       " tensor([-0.0618, -0.0322,  0.0596, -0.1013,  0.0144,  0.0649, -0.0416,\n",
       "          0.0038,  0.1014,  0.0901,  0.0511, -0.0694,  0.0549, -0.0574,\n",
       "          0.0197,  0.0466,  0.0992, -0.0929, -0.0275,  0.0983, -0.0783,\n",
       "         -0.0788, -0.0351, -0.0113,  0.0204,  0.0473,  0.0438, -0.0977,\n",
       "          0.0884, -0.0181,  0.0852,  0.0193,  0.0336,  0.0795,  0.0180,\n",
       "         -0.0040, -0.0531,  0.0065,  0.0138, -0.0213, -0.0522,  0.0004,\n",
       "         -0.0648,  0.0740,  0.0915,  0.1010, -0.0537, -0.0059, -0.0203,\n",
       "          0.0669, -0.0666,  0.0445,  0.0614, -0.0129,  0.0203,  0.0077,\n",
       "         -0.0286,  0.0564,  0.0689, -0.0248,  0.0378,  0.0966, -0.0291,\n",
       "          0.0519, -0.0126, -0.0827, -0.0548, -0.0380, -0.0445, -0.0686,\n",
       "         -0.0073,  0.0191,  0.0031, -0.0218,  0.0944, -0.0299, -0.0819,\n",
       "          0.0927, -0.0366,  0.0212, -0.0939,  0.0506,  0.0722, -0.0099,\n",
       "          0.0665,  0.0795, -0.0431,  0.0201,  0.0105, -0.0437,  0.0618,\n",
       "          0.0632, -0.0646,  0.0615,  0.0148,  0.0670]),\n",
       " Parameter containing:\n",
       " tensor([[-0.0484,  0.0190,  0.0762,  0.0859,  0.0164, -0.0077,  0.0584,\n",
       "           0.0126,  0.0705,  0.0402, -0.0188, -0.0922, -0.0519, -0.0508,\n",
       "          -0.0845,  0.0701, -0.0948, -0.0576,  0.0962, -0.0638,  0.0549,\n",
       "           0.0140,  0.1004, -0.0084, -0.0615, -0.0988, -0.0214, -0.0529,\n",
       "          -0.1011,  0.0993, -0.0971, -0.0353, -0.0083, -0.0390,  0.0575,\n",
       "          -0.0092, -0.0200,  0.0693, -0.0543, -0.0501,  0.0959, -0.0591,\n",
       "          -0.0861, -0.0600, -0.0877, -0.0971,  0.0422,  0.1002,  0.0114,\n",
       "          -0.0803, -0.0434, -0.0420,  0.0594, -0.0285, -0.1005,  0.0923,\n",
       "          -0.0976, -0.0956,  0.0469, -0.0027,  0.0642,  0.0135,  0.0763,\n",
       "          -0.0353, -0.0766,  0.0471, -0.0198, -0.0127,  0.0495,  0.0446,\n",
       "          -0.0563, -0.0176,  0.0529,  0.0679,  0.0583, -0.0450,  0.0518,\n",
       "          -0.0797, -0.0397,  0.0504,  0.0560,  0.0557, -0.0553,  0.0386,\n",
       "          -0.0366, -0.0158,  0.0983, -0.0023,  0.0100, -0.0014, -0.0647,\n",
       "           0.0368, -0.0861,  0.0340,  0.0046, -0.0533],\n",
       "         [-0.0021, -0.0893, -0.0950,  0.0354, -0.0396, -0.0139,  0.0280,\n",
       "          -0.0709,  0.0518,  0.0354,  0.0754,  0.0127,  0.0046, -0.0177,\n",
       "           0.0793, -0.0903,  0.0060,  0.0428,  0.0135, -0.0570,  0.0634,\n",
       "          -0.0991, -0.0516, -0.0531,  0.0653,  0.0139,  0.0832, -0.0658,\n",
       "           0.0860,  0.0779,  0.0620,  0.0947, -0.0914,  0.1011,  0.0468,\n",
       "           0.0884, -0.0804, -0.0644, -0.0163, -0.0522,  0.0714, -0.0874,\n",
       "           0.0937,  0.0604, -0.0536, -0.0791, -0.0477,  0.0730, -0.0337,\n",
       "          -0.0749, -0.0043,  0.0860,  0.0122, -0.0837, -0.0091, -0.0554,\n",
       "           0.0202,  0.0788,  0.0764, -0.0498, -0.0466,  0.0240,  0.0004,\n",
       "           0.0601, -0.0861, -0.0630, -0.0733, -0.0850,  0.0778, -0.0222,\n",
       "          -0.0169,  0.0550,  0.0419, -0.0986,  0.0618, -0.0200, -0.0186,\n",
       "           0.0419,  0.0732,  0.0894,  0.0075, -0.0629,  0.0849,  0.0726,\n",
       "           0.0539,  0.0052,  0.0997, -0.0286,  0.0412,  0.0851, -0.0571,\n",
       "           0.0563,  0.0653, -0.0344,  0.0908, -0.0654]]),\n",
       " Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [-9.1293,  2.7035]),\n",
       " Parameter containing:\n",
       " tensor([[-1.8490e-01, -1.6851e-01,  5.9893e-02,  ..., -1.5123e-02,\n",
       "           1.7965e-01, -7.3586e-02],\n",
       "         [-1.8757e-01, -1.0542e-01, -2.0188e-01,  ..., -8.4776e-04,\n",
       "          -1.6111e-01, -1.4240e-01],\n",
       "         [-1.3978e-01,  3.0602e-02,  1.2696e-02,  ...,  1.4848e-01,\n",
       "           4.9523e-02,  9.2322e-02],\n",
       "         ...,\n",
       "         [ 3.2516e-02,  6.8613e-02, -5.3822e-04,  ...,  1.2212e-01,\n",
       "           1.6176e-01, -1.0960e-01],\n",
       "         [ 1.0335e-01, -1.3879e-01, -1.0404e-01,  ...,  1.1951e-02,\n",
       "          -1.7552e-01, -1.8443e-01],\n",
       "         [-8.1054e-02, -2.0180e-01,  1.1869e-01,  ...,  1.9144e-01,\n",
       "          -6.5107e-02,  1.0971e-01]]),\n",
       " Parameter containing:\n",
       " tensor([ 0.1449,  0.0836,  0.0143,  0.2021,  0.0312,  0.1159,  0.0108,\n",
       "         -0.1246,  0.0602, -0.0768, -0.1064,  0.1843, -0.1578, -0.1663,\n",
       "          0.0294,  0.1978,  0.1983, -0.1073, -0.1076, -0.1985, -0.1829,\n",
       "         -0.0555, -0.1138, -0.1202, -0.1000,  0.1952, -0.0520,  0.0463,\n",
       "          0.1689,  0.1577, -0.1513,  0.1969, -0.1360, -0.0205,  0.0791,\n",
       "          0.2004,  0.1644, -0.1417,  0.0464, -0.2030, -0.0843, -0.0407,\n",
       "         -0.1192,  0.0145, -0.1812,  0.0039, -0.0409, -0.1657, -0.0909,\n",
       "          0.1956,  0.0479, -0.0663, -0.0500,  0.0488,  0.1160,  0.0008,\n",
       "         -0.1095, -0.0009,  0.1178, -0.0511, -0.1453, -0.0005,  0.1352,\n",
       "          0.0869, -0.1092, -0.1657,  0.1279,  0.1658, -0.0483, -0.0164,\n",
       "         -0.1766, -0.0961, -0.0631, -0.1596,  0.1614,  0.1335, -0.1190,\n",
       "         -0.1121, -0.1465, -0.0554,  0.0902,  0.1352,  0.0679,  0.1804,\n",
       "         -0.1400,  0.1371, -0.0437,  0.0825, -0.1635, -0.2026, -0.1385,\n",
       "         -0.1332,  0.1900,  0.1213, -0.1139, -0.0099]),\n",
       " Parameter containing:\n",
       " tensor([[ 7.9902e-02,  2.1585e-02, -4.2803e-02,  ..., -9.0435e-02,\n",
       "           8.5323e-02,  2.7463e-02],\n",
       "         [ 4.8151e-02,  7.0087e-02, -5.2767e-02,  ...,  8.8010e-03,\n",
       "          -2.8151e-02,  2.3003e-02],\n",
       "         [ 8.8185e-02,  3.7749e-02,  9.6279e-02,  ...,  3.8224e-02,\n",
       "           8.6851e-02, -2.6580e-02],\n",
       "         ...,\n",
       "         [-5.5696e-02,  2.7262e-02, -6.5616e-02,  ..., -1.7038e-02,\n",
       "           8.3070e-04,  9.1509e-02],\n",
       "         [ 8.3634e-02,  3.8178e-02, -7.2300e-02,  ..., -3.8748e-02,\n",
       "           2.5534e-02, -2.9885e-02],\n",
       "         [ 3.4653e-02,  4.0476e-02,  2.8885e-02,  ...,  6.3504e-02,\n",
       "          -4.0355e-02, -3.9655e-02]]),\n",
       " Parameter containing:\n",
       " tensor([ 0.0667,  0.0589, -0.0857,  0.0433,  0.0917, -0.0276,  0.0537,\n",
       "         -0.0880,  0.0656, -0.0004, -0.0873,  0.0674, -0.0520, -0.0367,\n",
       "          0.0548,  0.0144,  0.0469,  0.0956, -0.0599, -0.0195,  0.0682,\n",
       "          0.0629, -0.0697,  0.0158, -0.0094, -0.0813,  0.0441, -0.0553,\n",
       "          0.0325,  0.0771, -0.0422,  0.0314, -0.0211, -0.0776,  0.0983,\n",
       "          0.0165, -0.0984, -0.0928,  0.0133, -0.0138,  0.0464,  0.0826,\n",
       "          0.0267, -0.0620,  0.0775,  0.0622,  0.0874, -0.0106,  0.0525,\n",
       "         -0.0995, -0.0843, -0.0044, -0.0379,  0.0072,  0.0759, -0.1010,\n",
       "          0.0158, -0.0893, -0.0651, -0.0181,  0.0813, -0.0025, -0.0351,\n",
       "         -0.0571, -0.0309,  0.1004,  0.0571,  0.0895,  0.0902,  0.0029,\n",
       "          0.0308,  0.0325,  0.0288,  0.0030, -0.0173, -0.0250,  0.0210,\n",
       "          0.0902, -0.0456, -0.0196, -0.0663,  0.0801, -0.0732,  0.0872,\n",
       "         -0.0582,  0.0159,  0.0484,  0.0951,  0.0238,  0.0280, -0.0097,\n",
       "         -0.0018,  0.0352,  0.0827, -0.0715, -0.0094]),\n",
       " Parameter containing:\n",
       " tensor([[ 5.0960e-02,  6.3718e-02, -1.7054e-02,  ...,  3.7197e-02,\n",
       "          -4.8140e-02, -4.8264e-02],\n",
       "         [ 5.9026e-03,  5.5541e-02,  3.6171e-02,  ..., -8.1860e-02,\n",
       "           3.8061e-02,  9.6518e-02],\n",
       "         [ 3.0610e-02,  2.6859e-02,  9.2092e-02,  ...,  9.3269e-02,\n",
       "          -9.8281e-02,  8.5832e-02],\n",
       "         ...,\n",
       "         [ 6.5883e-02, -7.5531e-02,  7.6744e-02,  ...,  2.3467e-02,\n",
       "          -6.6862e-02,  5.7654e-02],\n",
       "         [ 7.7749e-02, -8.4200e-02, -4.0339e-02,  ..., -2.8260e-02,\n",
       "           3.6560e-02, -5.6336e-02],\n",
       "         [-7.6709e-02, -4.5433e-02,  1.9506e-02,  ...,  4.9379e-02,\n",
       "          -7.9920e-02, -1.0123e-01]]),\n",
       " Parameter containing:\n",
       " tensor([-0.0390, -0.0738, -0.0790, -0.0713, -0.0706, -0.0819,  0.0348,\n",
       "         -0.0231,  0.0262, -0.0878, -0.0320,  0.0888, -0.0493,  0.0552,\n",
       "          0.0854, -0.0700,  0.1008, -0.0891,  0.0027,  0.0872,  0.0910,\n",
       "          0.0431,  0.0308,  0.0185,  0.0238,  0.0873, -0.0801, -0.0904,\n",
       "          0.0728, -0.0012,  0.0394, -0.0448,  0.0059,  0.0186, -0.0963,\n",
       "         -0.0950, -0.0216,  0.0594,  0.0814, -0.0316,  0.0469, -0.0310,\n",
       "         -0.0252,  0.0755, -0.0555, -0.0617, -0.0919,  0.0382, -0.0368,\n",
       "          0.0485, -0.1004,  0.0157, -0.0373, -0.0697, -0.0563, -0.0470,\n",
       "         -0.0152, -0.0731,  0.0400,  0.0910,  0.1009, -0.0693, -0.0516,\n",
       "          0.1013, -0.0505,  0.0073,  0.0181, -0.0832, -0.0409, -0.0263,\n",
       "         -0.0780,  0.0582, -0.0102, -0.0390,  0.0049, -0.0706, -0.0674,\n",
       "         -0.0022, -0.0232, -0.0896,  0.0385,  0.0400, -0.0564,  0.0531,\n",
       "          0.0105,  0.0131,  0.0103,  0.0848, -0.0405, -0.0790, -0.0620,\n",
       "          0.0521, -0.0670, -0.0131,  0.0065, -0.0940]),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0458,  0.0178, -0.0825, -0.0007,  0.0341, -0.0756,  0.0765,\n",
       "          -0.0171, -0.0737, -0.0040, -0.0726,  0.0420,  0.0116,  0.0575,\n",
       "           0.0649, -0.0513,  0.0889,  0.0466,  0.0437, -0.0745,  0.0816,\n",
       "           0.0878, -0.0714, -0.0153, -0.0690, -0.0760,  0.0200, -0.0133,\n",
       "           0.0947,  0.0094,  0.0365, -0.0831, -0.0608, -0.0535,  0.0159,\n",
       "           0.0773, -0.0492,  0.0919,  0.0345, -0.0180, -0.0414,  0.0947,\n",
       "          -0.0255, -0.0523, -0.0369,  0.0840, -0.0619,  0.0438,  0.0456,\n",
       "           0.0792, -0.0828,  0.0373, -0.0680,  0.0101,  0.0255, -0.0009,\n",
       "           0.0652,  0.0421,  0.0436, -0.0294,  0.0113, -0.0401,  0.0225,\n",
       "          -0.0919, -0.0069,  0.0847, -0.1011, -0.0319,  0.0683,  0.0460,\n",
       "          -0.0140, -0.0473, -0.0373, -0.0306, -0.0585, -0.0965,  0.0519,\n",
       "           0.0112, -0.0996,  0.0991,  0.0293,  0.0067, -0.0943,  0.0486,\n",
       "           0.0266, -0.0872, -0.0871,  0.0858, -0.0343, -0.0837,  0.1014,\n",
       "          -0.0257,  0.0486, -0.0741,  0.0489,  0.0783],\n",
       "         [ 0.0357,  0.0330, -0.0209, -0.0651, -0.0771,  0.0379, -0.0425,\n",
       "           0.0356, -0.0521, -0.0317,  0.0019, -0.0128,  0.0649,  0.0497,\n",
       "           0.0290,  0.0247,  0.1012,  0.0935, -0.0793,  0.0386, -0.0436,\n",
       "          -0.0173, -0.0691, -0.0942,  0.0050,  0.0408,  0.0893, -0.0676,\n",
       "           0.0623, -0.1010,  0.0481, -0.0217,  0.0101, -0.0091, -0.0106,\n",
       "          -0.0851,  0.0325,  0.0666, -0.0848, -0.0308,  0.0567, -0.0407,\n",
       "           0.0720, -0.0457, -0.0002, -0.0391, -0.0400, -0.0296, -0.0806,\n",
       "          -0.0300, -0.0750, -0.0057, -0.0859,  0.0475, -0.0539, -0.0075,\n",
       "          -0.0082, -0.0712, -0.0606, -0.0046, -0.0709, -0.0321, -0.0310,\n",
       "           0.0457,  0.0670, -0.0371, -0.0786,  0.0356, -0.0449, -0.0259,\n",
       "           0.0179,  0.0296, -0.0324,  0.0541,  0.0512,  0.0351, -0.0170,\n",
       "          -0.0686,  0.0586, -0.0537,  0.0254, -0.0528,  0.0103,  0.0354,\n",
       "          -0.0239, -0.0997, -0.0915, -0.0145,  0.0384,  0.0494, -0.0325,\n",
       "           0.0906, -0.0881,  0.0818, -0.0928,  0.0774]]),\n",
       " Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [ 5.6443,  0.6441]),\n",
       " Parameter containing:\n",
       " tensor([[ 1.,  1.]]),\n",
       " Parameter containing:\n",
       " tensor([[ 1.,  1.]]),\n",
       " Parameter containing:\n",
       " tensor([[-0.1496,  0.1236, -0.0831,  ...,  0.1068, -0.0208,  0.1119],\n",
       "         [ 0.1878,  0.0395, -0.1616,  ..., -0.1562, -0.0840, -0.0274],\n",
       "         [-0.1210, -0.1199,  0.1599,  ..., -0.1492, -0.0806, -0.1244],\n",
       "         ...,\n",
       "         [ 0.1328,  0.0762, -0.1276,  ..., -0.1170, -0.1485,  0.0822],\n",
       "         [ 0.1672, -0.1664, -0.0995,  ...,  0.1161, -0.1309, -0.0481],\n",
       "         [-0.0562, -0.0404,  0.1721,  ..., -0.0201, -0.0564,  0.0706]]),\n",
       " Parameter containing:\n",
       " tensor([ 0.0167,  0.0116,  0.0131,  0.0166,  0.0687, -0.0554, -0.0298,\n",
       "         -0.0431, -0.0160, -0.1429, -0.0994,  0.1842, -0.0707,  0.1425,\n",
       "         -0.1318,  0.1126,  0.1492, -0.1717,  0.1564, -0.0334,  0.0320,\n",
       "          0.1265,  0.1080, -0.0396,  0.0657,  0.1725,  0.1697, -0.1606,\n",
       "          0.0479, -0.1498,  0.0158, -0.0005, -0.0705,  0.0581,  0.0929,\n",
       "         -0.0774, -0.1219, -0.0346,  0.1280,  0.1524,  0.1791, -0.1667,\n",
       "         -0.0751, -0.1210,  0.0255,  0.1177, -0.0600,  0.1085,  0.0301,\n",
       "          0.0194,  0.0280, -0.0341, -0.0663, -0.0199,  0.0883, -0.1429,\n",
       "         -0.0272,  0.1810, -0.0603,  0.1034,  0.1874, -0.1261,  0.0575,\n",
       "          0.0004,  0.0317, -0.1743, -0.0814, -0.1655,  0.1418, -0.0826,\n",
       "          0.0810, -0.1132,  0.0793,  0.0047,  0.1393, -0.0484, -0.1852,\n",
       "          0.1462, -0.0588, -0.1717, -0.1314, -0.0812, -0.0235,  0.1444,\n",
       "         -0.0770, -0.1530, -0.1320, -0.0398, -0.1077, -0.0625,  0.0138,\n",
       "          0.0133,  0.1778,  0.0410,  0.1204, -0.0753]),\n",
       " Parameter containing:\n",
       " tensor([[-1.9091e-02,  7.7842e-02,  7.2809e-02,  ...,  3.2999e-02,\n",
       "           2.3772e-02,  4.3625e-02],\n",
       "         [-1.9202e-02, -8.9887e-03, -4.1476e-02,  ..., -9.5261e-02,\n",
       "           7.0657e-02,  1.8171e-02],\n",
       "         [-8.6302e-02, -9.8046e-02,  8.4626e-02,  ..., -5.5712e-02,\n",
       "          -1.8523e-02, -4.1522e-02],\n",
       "         ...,\n",
       "         [ 8.3735e-02, -8.2951e-02, -6.8561e-02,  ..., -8.5294e-03,\n",
       "          -7.8847e-02,  3.5994e-02],\n",
       "         [-2.8322e-02,  7.8616e-02, -8.4294e-02,  ...,  3.8135e-02,\n",
       "          -8.6774e-02,  1.2588e-02],\n",
       "         [ 9.8042e-02,  8.0841e-02,  4.5399e-02,  ..., -1.6232e-02,\n",
       "           7.8391e-02,  6.1875e-02]]),\n",
       " Parameter containing:\n",
       " tensor([-0.0660,  0.0873,  0.0805, -0.0917,  0.0862, -0.0464,  0.0315,\n",
       "          0.0171, -0.0967,  0.0267,  0.0663, -0.0089,  0.0180, -0.0050,\n",
       "         -0.0815,  0.0427, -0.0293,  0.0288, -0.0147,  0.0241,  0.0256,\n",
       "          0.0740,  0.0575, -0.0501, -0.0248, -0.0513,  0.0882, -0.0029,\n",
       "         -0.0198, -0.0791, -0.0695, -0.0149,  0.0383,  0.0123,  0.0699,\n",
       "          0.0813, -0.0316,  0.0507, -0.0295, -0.0297, -0.0281,  0.0237,\n",
       "         -0.0097,  0.1014, -0.0189, -0.0077, -0.0534,  0.0953,  0.0139,\n",
       "         -0.0001,  0.0282, -0.0923,  0.0956, -0.0237, -0.0595,  0.0347,\n",
       "          0.0953,  0.0642, -0.0983,  0.0530,  0.0820,  0.0289,  0.0555,\n",
       "         -0.0080,  0.1007,  0.0904, -0.0616, -0.0494,  0.0017, -0.0690,\n",
       "         -0.0339, -0.0966, -0.1009, -0.0792,  0.0893, -0.0800, -0.0965,\n",
       "         -0.1019, -0.0126, -0.0164, -0.0121,  0.0184,  0.0751,  0.0097,\n",
       "         -0.0953, -0.0856, -0.0730, -0.0718, -0.0602, -0.0384,  0.0323,\n",
       "         -0.0355,  0.0376, -0.1018,  0.0486, -0.0648]),\n",
       " Parameter containing:\n",
       " tensor([[ 9.8443e-02,  9.1527e-03, -7.5973e-02,  ...,  8.0722e-02,\n",
       "           4.4070e-02, -1.5888e-03],\n",
       "         [ 4.1661e-02, -4.0464e-02, -3.6325e-02,  ...,  2.7766e-04,\n",
       "           8.3893e-02,  9.8679e-02],\n",
       "         [-6.7474e-02,  4.8507e-02,  1.4447e-02,  ...,  5.8001e-02,\n",
       "           7.6670e-02,  5.3783e-02],\n",
       "         ...,\n",
       "         [ 8.4304e-02,  3.9943e-02,  9.2258e-02,  ...,  6.6756e-02,\n",
       "           3.0897e-02, -1.0682e-02],\n",
       "         [-2.5303e-02,  5.9485e-03,  8.3343e-02,  ..., -5.4318e-02,\n",
       "          -2.8056e-02, -8.3959e-02],\n",
       "         [ 9.4753e-02, -5.5467e-02, -2.9287e-02,  ..., -2.8854e-02,\n",
       "           4.3638e-02,  4.3205e-02]]),\n",
       " Parameter containing:\n",
       " tensor([ 0.0934, -0.0529,  0.0842, -0.1006, -0.0877,  0.0953,  0.0986,\n",
       "          0.0566,  0.0902,  0.0941,  0.0896, -0.0963, -0.1011, -0.0746,\n",
       "         -0.0098, -0.0004, -0.0181, -0.0144, -0.0692, -0.0814,  0.0022,\n",
       "          0.0256, -0.0579,  0.0585, -0.0019,  0.0161,  0.0517,  0.0652,\n",
       "         -0.0399, -0.0361, -0.0903, -0.0908, -0.0725, -0.0552,  0.0177,\n",
       "          0.0803, -0.0065, -0.0027, -0.0227,  0.0876, -0.0323, -0.0964,\n",
       "          0.0407,  0.0934, -0.0243, -0.0867, -0.0385, -0.0070, -0.0160,\n",
       "          0.0080,  0.0443, -0.0909, -0.0824,  0.0578,  0.0959, -0.0349,\n",
       "         -0.1008, -0.0332, -0.0076, -0.0744,  0.0439, -0.0613, -0.0207,\n",
       "         -0.0054,  0.0549, -0.0334,  0.0532, -0.0816, -0.0109, -0.0583,\n",
       "         -0.0461, -0.0011, -0.0404,  0.0154, -0.0889, -0.0382,  0.0892,\n",
       "          0.0452,  0.0440, -0.0381,  0.0226, -0.0956, -0.0451,  0.0428,\n",
       "         -0.0087, -0.0794, -0.0836,  0.0310,  0.0958, -0.0581,  0.1017,\n",
       "         -0.0772, -0.0360,  0.0649,  0.0644,  0.0416]),\n",
       " Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [[ 7.8394, -9.3494, -5.3327,  9.0012, -4.5458, -5.9617,  5.4585,\n",
       "           9.4946, -9.7602,  7.7892, -5.4662, -0.8150, -0.3116,  4.7752,\n",
       "           2.1397, -7.2930, -8.6232, -6.2265, -3.7390,  5.8636,  0.2835,\n",
       "           3.8015,  8.9694, -9.1060, -4.7073, -6.9779,  8.7172, -0.0833,\n",
       "          -6.2762, -3.2630,  8.5403,  4.5607,  8.2960, -8.7222, -6.2175,\n",
       "          -9.2256,  3.5222, -0.0064,  4.9440,  8.3673, -0.5195, -4.3048,\n",
       "           4.9600, -6.7775, -3.4797,  6.8842, -3.1234,  2.1498, -5.5723,\n",
       "           4.1983, -0.8443, -2.1855, -2.0429,  9.2062, -9.2810, -4.1113,\n",
       "          -3.5424, -5.4772, -4.4588,  2.0837, -0.3153, -7.8977,  3.9733,\n",
       "          -7.2122,  3.1120,  1.9333, -2.2927,  6.5870,  8.3467, -2.8101,\n",
       "          -0.1741,  4.7102,  2.9060, -2.9413,  2.3496,  5.4072, -3.9486,\n",
       "           7.9003,  9.9839, -9.5953,  6.7272, -1.5527, -8.2075, -6.3547,\n",
       "           8.2729,  4.3277,  7.7513, -6.9462,  3.5915,  0.2119, -5.6030,\n",
       "          -1.1892, -3.0711, -3.3104, -7.6295, -4.4110]]),\n",
       " Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [ 4.4759]),\n",
       " Parameter containing:\n",
       " tensor([[ 9.8204e-02,  8.3517e-02, -9.0316e-02,  ..., -7.7502e-02,\n",
       "          -8.3837e-02, -1.7765e-01],\n",
       "         [-1.2017e-01, -1.6907e-01,  3.5453e-04,  ..., -1.1770e-01,\n",
       "          -1.2727e-02, -9.5076e-02],\n",
       "         [ 3.4343e-02, -5.1184e-02, -6.0254e-02,  ...,  1.7615e-01,\n",
       "          -5.8178e-02,  1.0801e-01],\n",
       "         ...,\n",
       "         [ 4.1019e-02, -1.6513e-01, -4.1170e-02,  ..., -1.6178e-01,\n",
       "          -1.2161e-01, -2.3492e-02],\n",
       "         [ 1.3374e-01, -5.0636e-02, -1.6383e-01,  ..., -9.9718e-02,\n",
       "          -1.7925e-01, -1.3709e-01],\n",
       "         [ 1.0157e-01, -6.3338e-02,  1.5852e-01,  ...,  6.2568e-02,\n",
       "          -1.3385e-01, -1.3988e-01]]),\n",
       " Parameter containing:\n",
       " tensor([-0.0257,  0.1051,  0.0149,  0.0810, -0.1613, -0.1128,  0.0723,\n",
       "         -0.1455,  0.0856,  0.0443, -0.0911, -0.1434,  0.0164, -0.1310,\n",
       "          0.1187, -0.1104, -0.1516,  0.0192, -0.1637,  0.0102, -0.0106,\n",
       "          0.0915, -0.1206,  0.1009, -0.1001,  0.1119, -0.1161, -0.0876,\n",
       "          0.0376,  0.0851, -0.0913,  0.0251, -0.0397,  0.1467,  0.1112,\n",
       "          0.1058,  0.1410,  0.1591,  0.1425, -0.1729, -0.0869, -0.1514,\n",
       "         -0.0515, -0.0331, -0.1417, -0.0715,  0.1765,  0.1383,  0.0147,\n",
       "          0.1770,  0.0435, -0.0191, -0.0968,  0.0363, -0.0534, -0.1716,\n",
       "         -0.1612,  0.0125, -0.0633,  0.1382,  0.0644,  0.1648, -0.1172,\n",
       "         -0.1669, -0.0571,  0.1403,  0.1313,  0.1686,  0.1346, -0.1063,\n",
       "          0.1803, -0.1288, -0.1721,  0.0831, -0.0050,  0.0012, -0.1412,\n",
       "          0.0595, -0.1661,  0.0965, -0.0648, -0.1096,  0.1430,  0.1712,\n",
       "         -0.0387,  0.0464, -0.0157, -0.1729,  0.1743,  0.1782,  0.1730,\n",
       "          0.1471, -0.1527, -0.0134, -0.1573,  0.0498]),\n",
       " Parameter containing:\n",
       " tensor([[ 2.2733e-02, -9.9906e-02, -1.2024e-02,  ..., -5.7118e-03,\n",
       "          -3.6952e-02, -3.2598e-02],\n",
       "         [ 2.5949e-02, -4.2911e-02, -4.1178e-03,  ..., -7.3629e-02,\n",
       "          -2.4274e-02, -6.7502e-03],\n",
       "         [-5.5204e-02, -2.6440e-02, -9.6612e-02,  ..., -5.6908e-02,\n",
       "          -9.8362e-02, -1.3523e-02],\n",
       "         ...,\n",
       "         [-7.8718e-02,  7.3989e-02, -1.0035e-02,  ...,  1.4345e-03,\n",
       "          -3.9897e-02, -3.8458e-02],\n",
       "         [ 8.0530e-02, -2.5810e-02,  8.0417e-02,  ..., -9.9560e-02,\n",
       "          -1.8399e-02,  9.9300e-02],\n",
       "         [ 9.1917e-02,  2.6841e-02,  6.0626e-02,  ..., -2.2717e-02,\n",
       "           5.7211e-02,  1.0100e-01]]),\n",
       " Parameter containing:\n",
       " tensor([ 0.0243,  0.0222, -0.0739, -0.0942,  0.0530,  0.0400, -0.0677,\n",
       "          0.0441, -0.0766,  0.0287, -0.0653,  0.0609, -0.0852, -0.0426,\n",
       "         -0.0432,  0.0361,  0.0320, -0.0141,  0.0845, -0.0220,  0.0801,\n",
       "          0.0478,  0.0267,  0.0251,  0.0484,  0.0849,  0.0041, -0.0036,\n",
       "          0.0895,  0.0525,  0.0021,  0.0558, -0.0108,  0.0659, -0.0061,\n",
       "         -0.0373, -0.0272,  0.0094,  0.0432, -0.0550, -0.0594,  0.0068,\n",
       "          0.0313,  0.0363, -0.0733, -0.0947, -0.0272,  0.1000,  0.0040,\n",
       "          0.0289, -0.0489,  0.0698,  0.0730, -0.0554, -0.0389,  0.0563,\n",
       "         -0.0281,  0.0186, -0.0204, -0.0927,  0.0177, -0.0490,  0.0981,\n",
       "          0.0162,  0.0723,  0.0267, -0.0277,  0.0283, -0.0006,  0.0368,\n",
       "         -0.0853,  0.0283, -0.0311, -0.0763, -0.0163, -0.0175, -0.0681,\n",
       "          0.0259,  0.0046,  0.1005, -0.0031, -0.0310, -0.0255, -0.0131,\n",
       "         -0.0493, -0.0516,  0.0708, -0.0580, -0.0428,  0.1000, -0.0925,\n",
       "          0.0978,  0.0336,  0.0706, -0.0510, -0.0348]),\n",
       " Parameter containing:\n",
       " tensor([[-1.9618e-02,  4.8920e-02,  1.9127e-02,  ...,  1.2713e-02,\n",
       "           8.6513e-02, -5.0041e-02],\n",
       "         [ 4.6006e-02, -3.0108e-02,  2.1620e-02,  ...,  8.4583e-02,\n",
       "          -4.9597e-02,  5.4221e-02],\n",
       "         [-3.7611e-02,  2.6608e-02,  4.7563e-02,  ..., -5.1068e-02,\n",
       "           5.2588e-02,  1.0065e-01],\n",
       "         ...,\n",
       "         [ 5.4540e-03,  7.5547e-02, -5.0599e-02,  ..., -7.5276e-02,\n",
       "          -1.5698e-02,  3.7643e-02],\n",
       "         [ 9.3970e-02, -5.3047e-02, -6.0953e-02,  ...,  4.6243e-02,\n",
       "          -1.5133e-02, -5.2537e-02],\n",
       "         [ 2.0667e-02,  4.6377e-02,  3.8970e-02,  ...,  4.8142e-02,\n",
       "          -2.5506e-03, -9.3943e-02]]),\n",
       " Parameter containing:\n",
       " tensor([ 5.7490e-03,  8.7157e-02, -3.4337e-02, -7.5176e-02,  2.6627e-02,\n",
       "          7.8041e-02,  8.1650e-02,  5.8167e-02, -5.9308e-02,  1.1484e-03,\n",
       "          1.2805e-02,  3.3639e-02,  2.7433e-02, -1.8457e-02,  8.3652e-02,\n",
       "          3.9851e-02,  1.8189e-03,  8.6479e-03, -6.7300e-03,  6.5128e-02,\n",
       "          8.4379e-02, -9.9314e-02,  1.0151e-01, -9.7123e-02, -6.4385e-02,\n",
       "         -8.3072e-02, -8.9751e-02,  3.6665e-02, -2.6405e-03, -8.8272e-02,\n",
       "         -4.9058e-02,  5.9598e-02,  5.5806e-02, -6.0179e-02, -7.4916e-02,\n",
       "         -7.0016e-03,  1.6179e-02,  1.0161e-01,  3.5933e-02, -1.8730e-02,\n",
       "         -5.8884e-02,  5.2930e-02,  5.9326e-02,  3.8950e-03,  7.9999e-02,\n",
       "          6.3568e-02, -3.8359e-02, -6.8998e-02, -2.9515e-02, -2.7312e-02,\n",
       "         -8.9550e-02, -9.9572e-02,  2.6401e-02,  7.5549e-06,  4.1460e-02,\n",
       "         -6.8116e-02,  2.0625e-02, -9.8825e-02,  5.9905e-02, -1.8482e-02,\n",
       "          4.6070e-02, -6.1049e-03, -9.1495e-02, -2.7535e-02,  6.6131e-02,\n",
       "         -4.4612e-02,  4.2617e-02, -6.9743e-02,  2.3765e-02, -8.5513e-02,\n",
       "          6.6798e-02,  5.7914e-02, -2.2895e-02, -7.1505e-02,  5.0170e-02,\n",
       "         -4.8874e-02,  3.8439e-02,  8.2235e-02, -2.0354e-02, -8.4149e-02,\n",
       "         -8.5368e-02,  8.2563e-03, -6.5242e-02, -6.6774e-02, -6.8409e-02,\n",
       "          8.7994e-02,  8.2177e-02,  2.4815e-02, -6.6429e-03,  1.0021e-01,\n",
       "          2.7255e-02,  6.9286e-02,  7.9551e-03, -2.0325e-03, -1.7648e-02,\n",
       "          6.1491e-02]),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0996, -0.0159, -0.0856, -0.0487, -0.0673, -0.0593,  0.0311,\n",
       "           0.0358, -0.0632,  0.0277, -0.0346, -0.0554, -0.0624,  0.0343,\n",
       "          -0.0304,  0.0052, -0.0887, -0.0227, -0.0104,  0.0175,  0.0358,\n",
       "           0.0808,  0.0463,  0.0450, -0.0314, -0.0980,  0.0109, -0.0170,\n",
       "           0.0500, -0.0040, -0.1007,  0.0753, -0.0004,  0.0321, -0.0790,\n",
       "           0.0746,  0.0086, -0.0966,  0.0669,  0.0538,  0.0710,  0.0799,\n",
       "          -0.0728, -0.0453,  0.0119,  0.0735,  0.0772,  0.0577,  0.0865,\n",
       "          -0.0332,  0.0205,  0.0557, -0.0096, -0.0396, -0.0642, -0.0992,\n",
       "           0.0731,  0.0949, -0.0133,  0.1002,  0.0909,  0.0893, -0.0474,\n",
       "          -0.0130, -0.0347, -0.0945,  0.0569,  0.0636,  0.0346, -0.0748,\n",
       "           0.0597, -0.0104,  0.0528, -0.0118,  0.0091, -0.0686,  0.0349,\n",
       "           0.0708,  0.0990, -0.0828, -0.0455, -0.0003,  0.1018,  0.0214,\n",
       "           0.0179, -0.0198,  0.0540, -0.0896,  0.0252, -0.0163, -0.0933,\n",
       "           0.0104, -0.0982, -0.0890, -0.0230, -0.0633]]),\n",
       " Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [ 6.9793])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(amodel.parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned_steps 1: 0.0\n",
      "learned_steps 2: 0.0\n",
      "learned_steps 3: 0.0\n",
      "learned_steps 4: 0.0\n",
      "learned_steps 5: 0.0\n",
      "learned_steps 6: 0.0\n",
      "learned_steps 7: 0.0\n",
      "learned_steps 8: 0.0\n",
      "learned_steps 9: 0.0\n",
      "learned_steps 10: 0.0\n",
      "learned_steps 11: 0.0\n",
      "learned_steps 12: 0.0\n",
      "learned_steps 13: 0.0\n",
      "learned_steps 14: 0.0\n",
      "learned_steps 15: 0.0\n",
      "learned_steps 16: 0.0\n",
      "learned_steps 17: 0.0\n",
      "learned_steps 18: 0.10000000149011612\n",
      "learned_steps 19: 0.0\n",
      "learned_steps 20: 0.0\n",
      "learned_steps 21: 0.0\n",
      "learned_steps 22: 0.0\n",
      "learned_steps 23: 0.0\n",
      "learned_steps 24: 0.0\n",
      "learned_steps 25: 0.0\n",
      "learned_steps 26: 0.0\n",
      "learned_steps 27: 0.0\n",
      "learned_steps 28: 0.0\n",
      "learned_steps 29: 0.0\n",
      "learned_steps 30: 0.0\n",
      "learned_steps 31: 0.0\n",
      "learned_steps 32: 0.0\n",
      "learned_steps 33: 0.0\n",
      "learned_steps 34: 0.0\n",
      "learned_steps 35: 0.0\n",
      "learned_steps 36: 0.0\n",
      "learned_steps 37: 0.0\n",
      "learned_steps 38: 0.0\n",
      "learned_steps 39: 0.0\n",
      "learned_steps 40: 0.0\n",
      "learned_steps 41: 0.0\n",
      "learned_steps 42: 0.0\n",
      "learned_steps 43: 0.0\n",
      "learned_steps 44: 0.0\n",
      "learned_steps 45: 0.0\n",
      "learned_steps 46: 0.0\n",
      "learned_steps 47: 0.0\n",
      "learned_steps 48: 0.0\n",
      "learned_steps 49: 0.0\n",
      "learned_steps 50: 0.0\n",
      "learned_steps 51: 0.0\n",
      "learned_steps 52: 0.0\n",
      "learned_steps 53: 0.0\n",
      "learned_steps 54: 0.0\n",
      "learned_steps 55: 0.0\n",
      "learned_steps 56: 0.0\n",
      "learned_steps 57: 0.0\n",
      "learned_steps 58: 0.0\n",
      "learned_steps 59: 0.0\n",
      "learned_steps 60: 0.0\n",
      "learned_steps 61: 0.0\n",
      "learned_steps 62: 0.0\n",
      "learned_steps 63: 0.0\n",
      "learned_steps 64: 0.0\n",
      "learned_steps 65: 0.0\n",
      "learned_steps 66: 0.0\n",
      "learned_steps 67: 0.0\n",
      "learned_steps 68: 0.0\n",
      "learned_steps 69: 0.0\n",
      "learned_steps 70: 0.0\n",
      "learned_steps 71: 0.0\n",
      "learned_steps 72: 0.0\n",
      "learned_steps 73: 0.0\n",
      "learned_steps 74: 0.0\n",
      "learned_steps 75: 0.0\n",
      "learned_steps 76: 0.0\n",
      "learned_steps 77: 0.0\n",
      "learned_steps 78: 0.0\n",
      "learned_steps 79: 0.0\n",
      "learned_steps 80: 0.0\n",
      "learned_steps 81: 0.0\n",
      "learned_steps 82: 0.0\n",
      "learned_steps 83: 0.0\n",
      "learned_steps 84: 0.0\n",
      "learned_steps 85: 0.0\n",
      "learned_steps 86: 0.0\n",
      "learned_steps 87: 0.0\n",
      "learned_steps 88: 0.0\n",
      "learned_steps 89: 0.0\n",
      "learned_steps 90: 0.0\n",
      "learned_steps 91: 0.0\n",
      "learned_steps 92: 0.0\n",
      "learned_steps 93: 0.0\n",
      "learned_steps 94: 0.0\n",
      "learned_steps 95: 0.0\n",
      "learned_steps 96: 0.0\n",
      "learned_steps 97: 0.0\n",
      "learned_steps 98: 0.0\n",
      "learned_steps 99: 0.0\n",
      "learned_steps 100: 0.0\n",
      "learned_steps 101: 0.0\n",
      "learned_steps 102: 0.0\n",
      "learned_steps 103: 0.0\n",
      "learned_steps 104: 0.0\n",
      "learned_steps 105: 0.0\n",
      "learned_steps 106: 0.0\n",
      "learned_steps 107: 0.0\n",
      "learned_steps 108: 0.0\n",
      "learned_steps 109: 0.0\n",
      "learned_steps 110: 0.0\n",
      "learned_steps 111: 0.0\n",
      "learned_steps 112: 0.0\n",
      "learned_steps 113: 0.09000000357627869\n",
      "learned_steps 114: 0.0\n",
      "learned_steps 115: 0.0\n",
      "learned_steps 116: 0.0\n",
      "learned_steps 117: 0.0\n",
      "learned_steps 118: 0.0\n",
      "learned_steps 119: 0.0\n",
      "learned_steps 120: 0.0\n",
      "learned_steps 121: 0.0\n",
      "learned_steps 122: 0.0\n",
      "learned_steps 123: 0.0\n",
      "learned_steps 124: 0.0\n",
      "learned_steps 125: 0.0\n",
      "learned_steps 126: 0.0\n",
      "learned_steps 127: 0.0\n",
      "learned_steps 128: 0.0\n",
      "learned_steps 129: 0.0\n",
      "learned_steps 130: 0.0\n",
      "learned_steps 131: 0.0\n",
      "learned_steps 132: 0.0\n",
      "learned_steps 133: 0.0\n",
      "learned_steps 134: 0.0\n",
      "learned_steps 135: 0.0\n",
      "learned_steps 136: 0.0\n",
      "learned_steps 137: 0.0\n",
      "learned_steps 138: 0.0\n",
      "learned_steps 139: 0.0\n",
      "learned_steps 140: 0.0\n",
      "learned_steps 141: 0.0\n",
      "learned_steps 142: 0.0\n",
      "learned_steps 143: 0.0\n",
      "learned_steps 144: 0.0\n",
      "learned_steps 145: 0.0\n",
      "learned_steps 146: 0.0\n",
      "learned_steps 147: 0.0\n",
      "learned_steps 148: 0.0\n",
      "learned_steps 149: 0.0\n",
      "learned_steps 150: 0.0\n",
      "learned_steps 151: 0.0\n",
      "learned_steps 152: 0.0\n",
      "learned_steps 153: 0.0\n",
      "learned_steps 154: 0.0\n",
      "learned_steps 155: 0.0\n",
      "learned_steps 156: 0.0\n",
      "learned_steps 157: 0.0\n",
      "learned_steps 158: 0.0\n",
      "learned_steps 159: 0.0\n",
      "learned_steps 160: 0.0\n",
      "learned_steps 161: 0.0\n",
      "learned_steps 162: 0.0\n",
      "learned_steps 163: 0.0\n",
      "learned_steps 164: 0.0\n",
      "learned_steps 165: 0.0\n",
      "learned_steps 166: 0.0\n",
      "learned_steps 167: 0.0\n",
      "learned_steps 168: 0.0\n",
      "learned_steps 169: 0.0\n",
      "learned_steps 170: 0.0\n",
      "learned_steps 171: 0.0\n",
      "learned_steps 172: 0.0\n",
      "learned_steps 173: 0.0\n",
      "learned_steps 174: 0.0\n",
      "learned_steps 175: 0.0\n",
      "learned_steps 176: 0.0\n",
      "learned_steps 177: 0.0\n",
      "learned_steps 178: 0.0\n",
      "learned_steps 179: 0.0\n",
      "learned_steps 180: 0.0\n",
      "learned_steps 181: 0.0\n",
      "learned_steps 182: 0.0\n",
      "learned_steps 183: 0.0\n",
      "learned_steps 184: 0.0\n",
      "learned_steps 185: 0.0\n",
      "learned_steps 186: 0.0\n",
      "learned_steps 187: 0.0\n",
      "learned_steps 188: 0.0\n",
      "learned_steps 189: 0.0\n",
      "learned_steps 190: 0.0\n",
      "learned_steps 191: 0.0\n",
      "learned_steps 192: 0.0\n",
      "learned_steps 193: 0.0\n",
      "learned_steps 194: 0.0\n",
      "learned_steps 195: 0.0\n",
      "learned_steps 196: 0.0\n",
      "learned_steps 197: 0.0\n",
      "learned_steps 198: 0.0\n",
      "learned_steps 199: 0.0\n",
      "learned_steps 200: 0.0\n",
      "learned_steps 201: 0.0\n",
      "learned_steps 202: 0.0\n",
      "learned_steps 203: 0.0\n",
      "learned_steps 204: 0.0\n",
      "learned_steps 205: 0.0\n",
      "learned_steps 206: 0.0\n",
      "learned_steps 207: 0.0\n",
      "learned_steps 208: 0.0\n",
      "learned_steps 209: 0.0\n",
      "learned_steps 210: 0.0\n",
      "learned_steps 211: 0.0\n",
      "learned_steps 212: 0.0\n",
      "learned_steps 213: 0.0\n",
      "learned_steps 214: 0.0\n",
      "learned_steps 215: 0.0\n",
      "learned_steps 216: 0.0\n",
      "learned_steps 217: 0.0\n",
      "learned_steps 218: 0.0\n",
      "learned_steps 219: 0.0\n",
      "learned_steps 220: 0.0\n",
      "learned_steps 221: 0.0\n",
      "learned_steps 222: 0.0\n",
      "learned_steps 223: 0.0\n",
      "learned_steps 224: 0.0\n",
      "learned_steps 225: 0.0\n",
      "learned_steps 226: 0.0\n",
      "learned_steps 227: 0.0\n",
      "learned_steps 228: 0.0\n",
      "learned_steps 229: 0.0\n",
      "learned_steps 230: 0.0\n",
      "learned_steps 231: 0.0\n",
      "learned_steps 232: 0.0\n",
      "learned_steps 233: 0.0\n",
      "learned_steps 234: 0.0\n",
      "learned_steps 235: 0.0\n",
      "learned_steps 236: 0.0\n",
      "learned_steps 237: 0.0\n",
      "learned_steps 238: 0.0\n",
      "learned_steps 239: 0.0\n",
      "learned_steps 240: 0.0\n",
      "learned_steps 241: 0.0\n",
      "learned_steps 242: 0.0\n",
      "learned_steps 243: 0.0\n",
      "learned_steps 244: 0.0\n",
      "learned_steps 245: 0.0\n",
      "learned_steps 246: 0.0\n",
      "learned_steps 247: 0.0\n",
      "learned_steps 248: 0.0\n",
      "learned_steps 249: 0.0\n",
      "learned_steps 250: 0.0\n",
      "learned_steps 251: 0.0\n",
      "learned_steps 252: 0.0\n",
      "learned_steps 253: 0.0\n",
      "learned_steps 254: 0.0\n",
      "learned_steps 255: 0.0\n",
      "learned_steps 256: 0.0\n",
      "learned_steps 257: 0.0\n",
      "learned_steps 258: 0.0\n",
      "learned_steps 259: 0.0\n",
      "learned_steps 260: 0.0\n",
      "learned_steps 261: 0.0\n",
      "learned_steps 262: 0.0\n",
      "learned_steps 263: 0.0\n",
      "learned_steps 264: 0.0\n",
      "learned_steps 265: 0.0\n",
      "learned_steps 266: 0.0\n",
      "learned_steps 267: 0.0\n",
      "learned_steps 268: 0.0\n",
      "learned_steps 269: 0.0\n",
      "learned_steps 270: 0.0\n",
      "learned_steps 271: 0.0\n",
      "learned_steps 272: 0.0\n",
      "learned_steps 273: 0.0\n",
      "learned_steps 274: 0.0\n",
      "learned_steps 275: 0.0\n",
      "learned_steps 276: 0.0\n",
      "learned_steps 277: 0.0\n",
      "learned_steps 278: 0.0\n",
      "learned_steps 279: 0.0\n",
      "learned_steps 280: 0.0\n",
      "learned_steps 281: 0.0\n",
      "learned_steps 282: 0.0\n",
      "learned_steps 283: 0.0\n",
      "learned_steps 284: 0.0\n",
      "learned_steps 285: 0.0\n",
      "learned_steps 286: 0.0\n",
      "learned_steps 287: 0.0\n",
      "learned_steps 288: 0.0\n",
      "learned_steps 289: 0.0\n",
      "learned_steps 290: 0.0\n",
      "learned_steps 291: 0.0\n",
      "learned_steps 292: 0.0\n",
      "learned_steps 293: 0.0\n",
      "learned_steps 294: 0.0\n",
      "learned_steps 295: 0.0\n",
      "learned_steps 296: 0.0\n",
      "learned_steps 297: 0.0\n",
      "learned_steps 298: 0.0\n",
      "learned_steps 299: 0.0\n",
      "learned_steps 300: 0.0\n",
      "learned_steps 301: 0.0\n",
      "learned_steps 302: 0.0\n",
      "learned_steps 303: 0.0\n",
      "learned_steps 304: 0.0\n",
      "learned_steps 305: 0.0\n",
      "learned_steps 306: 0.0\n",
      "learned_steps 307: 0.0\n",
      "learned_steps 308: 0.0\n",
      "learned_steps 309: 0.0\n",
      "learned_steps 310: 0.0\n",
      "learned_steps 311: 0.0\n",
      "learned_steps 312: 0.0\n",
      "learned_steps 313: 0.0\n",
      "learned_steps 314: 0.0\n",
      "learned_steps 315: 0.0\n",
      "learned_steps 316: 0.0\n",
      "learned_steps 317: 0.0\n",
      "learned_steps 318: 0.0\n",
      "learned_steps 319: 0.0\n",
      "learned_steps 320: 0.0\n",
      "learned_steps 321: 0.0\n",
      "learned_steps 322: 0.0\n",
      "learned_steps 323: 0.0\n",
      "learned_steps 324: 0.0\n",
      "learned_steps 325: 0.0\n",
      "learned_steps 326: 0.0\n",
      "learned_steps 327: 0.0\n",
      "learned_steps 328: 0.0\n",
      "learned_steps 329: 0.0\n",
      "learned_steps 330: 0.0\n",
      "learned_steps 331: 0.0\n",
      "learned_steps 332: 0.0\n",
      "learned_steps 333: 0.0\n",
      "learned_steps 334: 0.0\n",
      "learned_steps 335: 0.0\n",
      "learned_steps 336: 0.0\n",
      "learned_steps 337: 0.0\n",
      "learned_steps 338: 0.0\n",
      "learned_steps 339: 0.0\n",
      "learned_steps 340: 0.0\n",
      "learned_steps 341: 0.0\n",
      "learned_steps 342: 0.0\n",
      "learned_steps 343: 0.0\n",
      "learned_steps 344: 0.0\n",
      "learned_steps 345: 0.0\n",
      "learned_steps 346: 0.0\n",
      "learned_steps 347: 0.0\n",
      "learned_steps 348: 0.0\n",
      "learned_steps 349: 0.0\n",
      "learned_steps 350: 0.0\n",
      "learned_steps 351: 0.0\n",
      "learned_steps 352: 0.0\n",
      "learned_steps 353: 0.0\n",
      "learned_steps 354: 0.0\n",
      "learned_steps 355: 0.0\n",
      "learned_steps 356: 0.0\n",
      "learned_steps 357: 0.0\n",
      "learned_steps 358: 0.0\n",
      "learned_steps 359: 0.0\n",
      "learned_steps 360: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned_steps 361: 0.0\n",
      "learned_steps 362: 0.0\n",
      "learned_steps 363: 0.0\n",
      "learned_steps 364: 0.0\n",
      "learned_steps 365: 0.0\n",
      "learned_steps 366: 0.0\n",
      "learned_steps 367: 0.0\n",
      "learned_steps 368: 0.0\n",
      "learned_steps 369: 0.0\n",
      "learned_steps 370: 0.0\n",
      "learned_steps 371: 0.0\n",
      "learned_steps 372: 0.0\n",
      "learned_steps 373: 0.0\n",
      "learned_steps 374: 0.0\n",
      "learned_steps 375: 0.0\n",
      "learned_steps 376: 0.0\n",
      "learned_steps 377: 0.0\n",
      "learned_steps 378: 0.0\n",
      "learned_steps 379: 0.0\n",
      "learned_steps 380: 0.0\n",
      "learned_steps 381: 0.0\n",
      "learned_steps 382: 0.0\n",
      "learned_steps 383: 0.0\n",
      "learned_steps 384: 0.0\n",
      "learned_steps 385: 0.0\n",
      "learned_steps 386: 0.0\n",
      "learned_steps 387: 0.0\n",
      "learned_steps 388: 0.0\n",
      "learned_steps 389: 0.0\n",
      "learned_steps 390: 0.0\n",
      "learned_steps 391: 0.0\n",
      "learned_steps 392: 0.0\n",
      "learned_steps 393: 0.0\n",
      "learned_steps 394: 0.0\n",
      "learned_steps 395: 0.0\n",
      "learned_steps 396: 0.0\n",
      "learned_steps 397: 0.0\n",
      "learned_steps 398: 0.0\n",
      "learned_steps 399: 0.0\n",
      "learned_steps 400: 0.0\n",
      "learned_steps 401: 0.0\n",
      "learned_steps 402: 0.0\n",
      "learned_steps 403: 0.0\n",
      "learned_steps 404: 0.0\n",
      "learned_steps 405: 0.0\n",
      "learned_steps 406: 0.0\n",
      "learned_steps 407: 0.0\n",
      "learned_steps 408: 0.0\n",
      "learned_steps 409: 0.0\n",
      "learned_steps 410: 0.0\n",
      "learned_steps 411: 0.0\n",
      "learned_steps 412: 0.0\n",
      "learned_steps 413: 0.0\n",
      "learned_steps 414: 0.0\n",
      "learned_steps 415: 0.0\n",
      "learned_steps 416: 0.0\n",
      "learned_steps 417: 0.0\n",
      "learned_steps 418: 0.0\n",
      "learned_steps 419: 0.0\n",
      "learned_steps 420: 0.0\n",
      "learned_steps 421: 0.0\n",
      "learned_steps 422: 0.0\n",
      "learned_steps 423: 0.0\n",
      "learned_steps 424: 0.0\n",
      "learned_steps 425: 0.0\n",
      "learned_steps 426: 0.0\n",
      "learned_steps 427: 0.0\n",
      "learned_steps 428: 0.0\n",
      "learned_steps 429: 0.0\n",
      "learned_steps 430: 0.0\n",
      "learned_steps 431: 0.0\n",
      "learned_steps 432: 0.0\n",
      "learned_steps 433: 0.0\n",
      "learned_steps 434: 0.0\n",
      "learned_steps 435: 0.0\n",
      "learned_steps 436: 0.0\n",
      "learned_steps 437: 0.0\n",
      "learned_steps 438: 0.0\n",
      "learned_steps 439: 0.0\n",
      "learned_steps 440: 0.0\n",
      "learned_steps 441: 0.0\n",
      "learned_steps 442: 0.0\n",
      "learned_steps 443: 0.0\n",
      "learned_steps 444: 0.0\n",
      "learned_steps 445: 0.0\n",
      "learned_steps 446: 0.0\n",
      "learned_steps 447: 0.0\n",
      "learned_steps 448: 0.0\n",
      "learned_steps 449: 0.0\n",
      "learned_steps 450: 0.0\n",
      "learned_steps 451: 0.0\n",
      "learned_steps 452: 0.0\n",
      "learned_steps 453: 0.0\n",
      "learned_steps 454: 0.0\n",
      "learned_steps 455: 0.0\n",
      "learned_steps 456: 0.0\n",
      "learned_steps 457: 0.0\n",
      "learned_steps 458: 0.0\n",
      "learned_steps 459: 0.0\n",
      "learned_steps 460: 0.0\n",
      "learned_steps 461: 0.0\n",
      "learned_steps 462: 0.0\n",
      "learned_steps 463: 0.0\n",
      "learned_steps 464: 0.0\n",
      "learned_steps 465: 0.0\n",
      "learned_steps 466: 0.0\n",
      "learned_steps 467: 0.0\n",
      "learned_steps 468: 0.0\n",
      "learned_steps 469: 0.0\n",
      "learned_steps 470: 0.0\n",
      "learned_steps 471: 0.0\n",
      "learned_steps 472: 0.0\n",
      "learned_steps 473: 0.0\n",
      "learned_steps 474: 0.0\n",
      "learned_steps 475: 0.0\n",
      "learned_steps 476: 0.0\n",
      "learned_steps 477: 0.0\n",
      "learned_steps 478: 0.0\n",
      "learned_steps 479: 0.0\n",
      "learned_steps 480: 0.0\n",
      "learned_steps 481: 0.0\n",
      "learned_steps 482: 0.0\n",
      "learned_steps 483: 0.0\n",
      "learned_steps 484: 0.0\n",
      "learned_steps 485: 0.0\n",
      "learned_steps 486: 0.0\n",
      "learned_steps 487: 0.0\n",
      "learned_steps 488: 0.0\n",
      "learned_steps 489: 0.0\n",
      "learned_steps 490: 0.0\n",
      "learned_steps 491: 0.0\n",
      "learned_steps 492: 0.0\n",
      "learned_steps 493: 0.0\n",
      "learned_steps 494: 0.0\n",
      "learned_steps 495: 0.0\n",
      "learned_steps 496: 0.0\n",
      "learned_steps 497: 0.0\n",
      "learned_steps 498: 0.0\n",
      "learned_steps 499: 0.0\n",
      "learned_steps 500: 0.0\n",
      "learned_steps 501: 0.0\n",
      "learned_steps 502: 0.10000000149011612\n",
      "learned_steps 503: 0.0\n",
      "learned_steps 504: 0.0\n",
      "learned_steps 505: 0.0\n",
      "learned_steps 506: 0.0\n",
      "learned_steps 507: 0.0\n",
      "learned_steps 508: 0.0\n",
      "learned_steps 509: 0.0\n",
      "learned_steps 510: 0.0\n",
      "learned_steps 511: 0.0\n",
      "learned_steps 512: 0.0\n",
      "learned_steps 513: 0.0\n",
      "learned_steps 514: 0.0\n",
      "learned_steps 515: 0.0\n",
      "learned_steps 516: 0.0\n",
      "learned_steps 517: 0.0\n",
      "learned_steps 518: 0.0\n",
      "learned_steps 519: 0.0\n",
      "learned_steps 520: 0.0\n",
      "learned_steps 521: 0.0\n",
      "learned_steps 522: 0.0\n",
      "learned_steps 523: 0.0\n",
      "learned_steps 524: 0.0\n",
      "learned_steps 525: 0.0\n",
      "learned_steps 526: 0.0\n",
      "learned_steps 527: 0.0\n",
      "learned_steps 528: 0.0\n",
      "learned_steps 529: 0.0\n",
      "learned_steps 530: 0.0\n",
      "learned_steps 531: 0.10000000149011612\n",
      "learned_steps 532: 0.0\n",
      "learned_steps 533: 0.0\n",
      "learned_steps 534: 0.0\n",
      "learned_steps 535: 0.0\n",
      "learned_steps 536: 0.0\n",
      "learned_steps 537: 0.0\n",
      "learned_steps 538: 0.0\n",
      "learned_steps 539: 0.0\n",
      "learned_steps 540: 0.0\n",
      "learned_steps 541: 0.0\n",
      "learned_steps 542: 0.0\n",
      "learned_steps 543: 0.0\n",
      "learned_steps 544: 0.0\n",
      "learned_steps 545: 0.0\n",
      "learned_steps 546: 0.0\n",
      "learned_steps 547: 0.0\n",
      "learned_steps 548: 0.0\n",
      "learned_steps 549: 0.0\n",
      "learned_steps 550: 0.0\n",
      "learned_steps 551: 0.0\n",
      "learned_steps 552: 0.0\n",
      "learned_steps 553: 0.0\n",
      "learned_steps 554: 0.0\n",
      "learned_steps 555: 0.0\n",
      "learned_steps 556: 0.0\n",
      "learned_steps 557: 0.0\n",
      "learned_steps 558: 0.0\n",
      "learned_steps 559: 0.0\n",
      "learned_steps 560: 0.0\n",
      "learned_steps 561: 0.0\n",
      "learned_steps 562: 0.0\n",
      "learned_steps 563: 0.0\n",
      "learned_steps 564: 0.0\n",
      "learned_steps 565: 0.0\n",
      "learned_steps 566: 0.0\n",
      "learned_steps 567: 0.0\n",
      "learned_steps 568: 0.0\n",
      "learned_steps 569: 0.0\n",
      "learned_steps 570: 0.0\n",
      "learned_steps 571: 0.0\n",
      "learned_steps 572: 0.0\n",
      "learned_steps 573: 0.0\n",
      "learned_steps 574: 0.0\n",
      "learned_steps 575: 0.0\n",
      "learned_steps 576: 0.0\n",
      "learned_steps 577: 0.0\n",
      "learned_steps 578: 0.0\n",
      "learned_steps 579: 0.0\n",
      "learned_steps 580: 0.0\n",
      "learned_steps 581: 0.0\n",
      "learned_steps 582: 0.0\n",
      "learned_steps 583: 0.0\n",
      "learned_steps 584: 0.0\n",
      "learned_steps 585: 0.0\n",
      "learned_steps 586: 0.0\n",
      "learned_steps 587: 0.0\n",
      "learned_steps 588: 0.0\n",
      "learned_steps 589: 0.0\n",
      "learned_steps 590: 0.0\n",
      "learned_steps 591: 0.0\n",
      "learned_steps 592: 0.0\n",
      "learned_steps 593: 0.0\n",
      "learned_steps 594: 0.0\n",
      "learned_steps 595: 0.0\n",
      "learned_steps 596: 0.0\n",
      "learned_steps 597: 0.0\n",
      "learned_steps 598: 0.0\n",
      "learned_steps 599: 0.0\n",
      "learned_steps 600: 0.0\n",
      "learned_steps 601: 0.0\n",
      "learned_steps 602: 0.0\n",
      "learned_steps 603: 0.0\n",
      "learned_steps 604: 0.0\n",
      "learned_steps 605: 0.0\n",
      "learned_steps 606: 0.0\n",
      "learned_steps 607: 0.0\n",
      "learned_steps 608: 0.0\n",
      "learned_steps 609: 0.0\n",
      "learned_steps 610: 0.0\n",
      "learned_steps 611: 0.0\n",
      "learned_steps 612: 0.0\n",
      "learned_steps 613: 0.0\n",
      "learned_steps 614: 0.0\n",
      "learned_steps 615: 0.0\n",
      "learned_steps 616: 0.0\n",
      "learned_steps 617: 0.0\n",
      "learned_steps 618: 0.0\n",
      "learned_steps 619: 0.0\n",
      "learned_steps 620: 0.0\n",
      "learned_steps 621: 0.0\n",
      "learned_steps 622: 0.0\n",
      "learned_steps 623: 0.0\n",
      "learned_steps 624: 0.0\n",
      "learned_steps 625: 0.0\n",
      "learned_steps 626: 0.0\n",
      "learned_steps 627: 0.0\n",
      "learned_steps 628: 0.0\n",
      "learned_steps 629: 0.0\n",
      "learned_steps 630: 0.0\n",
      "learned_steps 631: 0.0\n",
      "learned_steps 632: 0.0\n",
      "learned_steps 633: 0.0\n",
      "learned_steps 634: 0.0\n",
      "learned_steps 635: 0.0\n",
      "learned_steps 636: 0.0\n",
      "learned_steps 637: 0.0\n",
      "learned_steps 638: 0.0\n",
      "learned_steps 639: 0.0\n",
      "learned_steps 640: 0.0\n",
      "learned_steps 641: 0.0\n",
      "learned_steps 642: 0.0\n",
      "learned_steps 643: 0.0\n",
      "learned_steps 644: 0.0\n",
      "learned_steps 645: 0.0\n",
      "learned_steps 646: 0.0\n",
      "learned_steps 647: 0.0\n",
      "learned_steps 648: 0.0\n",
      "learned_steps 649: 0.0\n",
      "learned_steps 650: 0.0\n",
      "learned_steps 651: 0.0\n",
      "learned_steps 652: 0.0\n",
      "learned_steps 653: 0.0\n",
      "learned_steps 654: 0.0\n",
      "learned_steps 655: 0.0\n",
      "learned_steps 656: 0.0\n",
      "learned_steps 657: 0.0\n",
      "learned_steps 658: 0.0\n",
      "learned_steps 659: 0.0\n",
      "learned_steps 660: 0.0\n",
      "learned_steps 661: 0.0\n",
      "learned_steps 662: 0.0\n",
      "learned_steps 663: 0.0\n",
      "learned_steps 664: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "def get_extra_obs(states, actions):\n",
    "    ''' \n",
    "        return a list contains other agents states and actions, len = num_agents\n",
    "        states: list of states by each agent\n",
    "        actions: List of action by each agent\n",
    "    '''\n",
    "    extra_obs = []\n",
    "    # print(f\"actions : {actions}\")\n",
    "    for i in range(states.shape[0]):\n",
    "        list_ = []\n",
    "        # states\n",
    "        list_.extend(states[i])\n",
    "        # agent's action\n",
    "        list_.extend(actions[i])\n",
    "        # other agent's actions\n",
    "        list_.extend(actions[np.arange(len(actions))!= i][0])\n",
    "        extra_obs.append(list_)\n",
    "    return extra_obs\n",
    "\n",
    "\n",
    "class Actor_critic_model(nn.Module):\n",
    "    def __init__(self, params_dir, input_dim, act_size, num_agents):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.act_size = act_size\n",
    "        self.num_agents = num_agents\n",
    "        self.params = parse_params(params_dir)\n",
    "        self.mu = self.create_module_list(self.create_actor)\n",
    "        self.std =  self.create_std()\n",
    "        self.val = self.create_module_list(self.create_critic)\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_in')\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def create_module_list(self, func):\n",
    "        module_list = nn.ModuleList()\n",
    "        for _ in range(self.num_agents):\n",
    "            module_list.append(func())\n",
    "        return module_list\n",
    "    \n",
    "    def create_std(self):\n",
    "        param_list = nn.ParameterList([nn.Parameter(\n",
    "            torch.ones(1, self.act_size)) for _ in range(self.num_agents)]\n",
    "                        )\n",
    "        return param_list\n",
    "\n",
    "    def create_actor(self):\n",
    "        module_list = nn.ModuleList()\n",
    "        layer = nn.Sequential()\n",
    "        fc = nn.Linear(self.input_dim, self.params['hidden_dim'])\n",
    "        layer.add_module(f\"fc_layer_1\", fc)\n",
    "        # layer.add_module(f\"bn_layer_1\",\n",
    "                        # nn.BatchNorm1d(self.params['hidden_dim']))\n",
    "        # layer.add_module(f\"RELU_layer_1\", nn.LeakyReLU())\n",
    "        layer.add_module(f\"RELU_layer_1\", nn.ReLU())\n",
    "        module_list.append(layer)\n",
    "        # module_list.apply(self._init_weights)\n",
    "        self.add_hidden_layer(module_list, self.params['actor_h_num'],\n",
    "                         self.params['hidden_dim'], self.params['hidden_dim'])\n",
    "        module_list.append(nn.Sequential(nn.Linear(self.params['hidden_dim'],\n",
    "                                          self.act_size)))\n",
    "        # module_list.apply(self._init_weights)\n",
    "        return module_list\n",
    "    \n",
    "    def create_critic(self):\n",
    "        module_list = nn.ModuleList()\n",
    "        layer = nn.Sequential()\n",
    "        fc = nn.Linear(self.input_dim + self.act_size * 2, self.params['hidden_dim'])\n",
    "        layer.add_module(f\"fc_layer_1\", fc)\n",
    "        # layer.add_module(f\"bn_layer_1\",\n",
    "                        # nn.BatchNorm1d(self.params['hidden_dim']))\n",
    "        # layer.add_module(f\"RELU_layer_1\", nn.LeakyReLU())\n",
    "        layer.add_module(f\"RELU_layer_1\", nn.ReLU())\n",
    "        module_list.append(layer)\n",
    "        self.add_hidden_layer(module_list, self.params['critic_h_num'],\n",
    "                         self.params['hidden_dim'], self.params['hidden_dim'])\n",
    "        module_list.append(nn.Sequential(nn.Linear(self.params['hidden_dim'], 1)))\n",
    "        # module_list.apply(self._init_weights)\n",
    "        return module_list\n",
    "    \n",
    "    def add_hidden_layer(self, module_list, num_hidden_layer,\n",
    "                         input_dim, output_dim):\n",
    "        if num_hidden_layer == 0:\n",
    "            return\n",
    "        for i in range(1, num_hidden_layer+1):\n",
    "            layer = nn.Sequential()\n",
    "            fc = nn.Linear(input_dim, output_dim)\n",
    "            layer.add_module(f\"fc_layer_{i}\", fc)\n",
    "            # layer.add_module(f\"bn_layer_{i}\",\n",
    "                          #    nn.BatchNorm1d(output_dim))\n",
    "            # layer.add_module(f\"RELU_layer_{i}\", nn.LeakyReLU())\n",
    "            layer.add_module(f\"RELU_layer_{i}\", nn.ReLU())\n",
    "            module_list.append(layer)\n",
    "            \n",
    "    def forward(self, states, actor=True, train=True, index=None):\n",
    "        '''\n",
    "            If actor is True, output actions and log probabilities FloatTensor\n",
    "            If Critic (actor = False), output state value FloatTensor\n",
    "        '''\n",
    "        x_ = torch.FloatTensor(states).to(self.device)\n",
    "        if actor:\n",
    "            # forward in actor path\n",
    "            mus = torch.zeros(self.num_agents, self.act_size, dtype=torch.float)\n",
    "            dists = []\n",
    "            acts = torch.zeros(self.num_agents, self.act_size, dtype=torch.float)\n",
    "            lps = torch.zeros(self.num_agents, self.act_size, dtype=torch.float)\n",
    "            for i in range(self.num_agents):\n",
    "                \n",
    "                mu_ = x_[i]\n",
    "                for m in self.mu[i]:\n",
    "                    mu_ = m(mu_)\n",
    "                mus[i] = (mu_)\n",
    "                dists.append(torch.distributions.Normal(mus[i], self.std[i]))\n",
    "                act_ = dists[i].sample()\n",
    "                if train:\n",
    "                    # only return log probabilities in training phases\n",
    "                    lps[i] = dists[i].log_prob(act_)\n",
    "                    acts[i] = torch.clamp(act_, -1, 1)\n",
    "                    return acts, lps\n",
    "                # return only actions in executation phases\n",
    "                return torch.clamp(act_, -1, 1)\n",
    "        # forward in value path\n",
    "        for v in self.val[index]:\n",
    "            x_ = v(x_)\n",
    "        return x_\n",
    "\n",
    "def parse_params(params_dir):\n",
    "    with open(params_dir) as fp:\n",
    "        params = json.load(fp)\n",
    "    return params\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, device, num_agents, params_dir, state_size, action_size):\n",
    "        self.model = Actor_critic_model(params_dir, state_size, action_size, num_agents).to(device)\n",
    "        \n",
    "        # I should try a version without target, just like A2C\n",
    "        self.target = Actor_critic_model(params_dir, state_size, action_size, num_agents).to(device)\n",
    "        self.device = device\n",
    "        self.num_agents = num_agents\n",
    "        self.params = self.model.params\n",
    "        self.optimizer = [optim.Adam(self.model.parameters(),\n",
    "                                    lr=self.params['lr']) for _ in range(self.num_agents)]\n",
    "                                    # lr=0.0001)\n",
    "\n",
    "    def __call__(self, states):\n",
    "        # mu, std, val, etp = self.model(states)\n",
    "        actions, log_prob = self.model(states)\n",
    "        return actions, log_prob\n",
    "\n",
    "    def step(self, memories):\n",
    "        '''\n",
    "        second edition\n",
    "            experiences:\n",
    "                list with n_steps_taken * [actions, rewards, log_probs,\n",
    "                                           not_dones, state_values]:\n",
    "                    actions (tensor: num agents * num actions)\n",
    "                    rewards (list: size = num agents)\n",
    "                    log_probs (tensor: num agents * num actions)\n",
    "                    not_dones (np array: size = num agents)\n",
    "                    state_values (list: size = num agents)\n",
    "        '''\n",
    "        loss = [0.0] * self.num_agents\n",
    "        for idx in range(self.num_agents):\n",
    "            actions, rewards, log_probs, not_dones, states, next_states= memories[idx].spit()\n",
    "            # print(f\"state_values : {state_values}\")\n",
    "            rewards = torch.FloatTensor(rewards).view(-1, 1)\n",
    "            #print(f\"len(rewards[0]) - 1 : {len(rewards) - 1}\")\n",
    "            not_dones = torch.FloatTensor(not_dones).to(device).unsqueeze(1)\n",
    "            # print(rewards)\n",
    "            state_values = self.model(states, actor=False, index=idx)\n",
    "            next_values = self.target(next_states, actor=False, index=idx)\n",
    "            returns = rewards + self.params['gamma'] * not_dones * next_values.detach()\n",
    "            advantage_  = rewards + self.params['gamma'] * not_dones * next_values.detach() - state_values.detach()\n",
    "            # print(f\"log_probs.shape : {log_probs.shape}\")\n",
    "            # print(f\"advantage_.shape : {advantage_.shape}\")\n",
    "            # print(f\"state_values[i].shape : {state_values[i].shape}\")\n",
    "            # print(f\"return_.shape : {return_.shape}\")\n",
    "            # print(f\"processed_experience : {processed_experience}\")\n",
    "            log_probs = torch.stack(log_probs)\n",
    "            policy_loss = -(log_probs) * advantage_\n",
    "            value_loss = (0.5 * (returns - state_values).pow(2))\n",
    "            self.optimizer[idx].zero_grad()\n",
    "            loss[idx] = ((policy_loss + value_loss.unsqueeze(1)).mean())\n",
    "            # print(f\"loss[idx] : {loss[idx]}\")\n",
    "            if torch.isnan(loss[idx]).any():\n",
    "                print('Nan in loss function')\n",
    "                pass\n",
    "            # loss[idx].backward()\n",
    "            if idx == (self.num_agents - 1):\n",
    "                loss[idx].backward()\n",
    "            else:\n",
    "                loss[idx].backward(retain_graph=True)\n",
    "            nn.utils.clip_grad_norm_(self.model.parameters(), self.model.params['grad_clip'])\n",
    "            self.optimizer[idx].step()\n",
    "        self.soft_udpate()\n",
    "        \n",
    "    def soft_udpate(self):\n",
    "        for tp, lp in zip(self.target.parameters(),\n",
    "                          self.model.parameters()):\n",
    "            tp.data.copy_(self.params['TAU']*lp.data +\n",
    "                          (1.0-self.params['TAU'])*tp.data)\n",
    "            \n",
    "    def hard_udpate(self):\n",
    "        for tp, lp in zip(self.target.parameters(),\n",
    "                          self.model.parameters()):\n",
    "            tp.data.copy_(lp.data)\n",
    "\n",
    "\n",
    "class Experience():\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        #self.extra_into = []\n",
    "        self.log_probs = []\n",
    "        self.not_dones = []\n",
    "        self.states = []\n",
    "        self.next_states = []\n",
    "        # self.etp = []\n",
    "\n",
    "    def add(self, actions, rewards, log_probs, not_dones,\n",
    "            states, next_states):\n",
    "        self.actions.append(actions)\n",
    "        self.rewards.append(rewards)\n",
    "        #self.extra_into.append(extra_into)\n",
    "        self.log_probs.append(log_probs)\n",
    "        self.not_dones.append(not_dones)\n",
    "        self.states.append(states)\n",
    "        self.next_states.append(next_states)\n",
    "        # self.etp.append(etp)\n",
    "\n",
    "    def spit(self):\n",
    "        return (self.actions[1:], self.rewards[1:], self.log_probs[1:],\n",
    "                self.not_dones[1:],\n",
    "                self.states[1:], self.next_states[1:])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rewards)\n",
    "    \n",
    "params_dir = f\"./params.txt\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = Agent(device, num_agents, params_dir, state_size, action_size)\n",
    "\n",
    "def plot_scores(scores):\n",
    "    fig, ax = plt.subplots(1, figsize=(8, 8))\n",
    "    plt.plot(np.arange(len(scores)), scores)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "scores_window = deque(maxlen=100)\n",
    "memories = [Experience() for _ in range(num_agents)]\n",
    "learned_steps = 0\n",
    "while learned_steps < 1000:\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    states_ = env_info.vector_observations\n",
    "    scores = np.zeros(num_agents)\n",
    "    done = [False] * num_agents\n",
    "    actions_ = np.random.random([num_agents, action_size])\n",
    "    log_prob_ = torch.rand(num_agents, action_size)\n",
    "    rewards_ = [0] * num_agents\n",
    "    states_plus = get_extra_obs(states, actions)\n",
    "    steps = 0\n",
    "    # while not np.any(done):\n",
    "    while steps < 20:\n",
    "        actions_next, log_prob_next = agent(states)\n",
    "        next_states_plus = get_extra_obs(states, actions_next.cpu().numpy())\n",
    "        env_info = env.step(actions_next.detach().cpu().numpy())[brain_name]\n",
    "        done = env_info.local_done\n",
    "        not_done_ = (1 - np.array(done))\n",
    "        for idx in range(num_agents):\n",
    "            memories[idx].add(actions_[idx], rewards_[idx], log_prob_[idx],\n",
    "                              not_done_[idx], states_plus[idx], next_states_plus[idx])\n",
    "        steps += 1\n",
    "\n",
    "        rewards_ = env_info.rewards\n",
    "        states = env_info.vector_observations\n",
    "        actions_ = actions_next\n",
    "        log_prob_ = log_prob_next\n",
    "        states_plus = next_states_plus\n",
    "        scores += rewards_\n",
    "        if (len(memories[0].actions) % 1000 == 0) and (len(memories[0].actions) >1):\n",
    "            agent.step(memories)\n",
    "            learned_steps += 1\n",
    "            memories = [Experience() for _ in range(num_agents)]\n",
    "            print(f\"learned_steps {learned_steps}: {np.max(scores)}\")\n",
    "        if learned_steps % 1000 == 0:\n",
    "            agent.hard_udpate()\n",
    "        if np.any(done):\n",
    "            # memories = Experience()\n",
    "            # print(f\" steps : {steps}\")\n",
    "            break\n",
    "        # print(scores)\n",
    "\n",
    "    scores_window.append(np.max(scores))\n",
    "    if (len(scores_window)) == 100 and ((sum(scores_window) / len(scores_window)) > 0.5):\n",
    "        torch.save(agent.model.state_dict(), agent.params['working_dir'])\n",
    "        print(f\"Envinroment solved in episode{steps}!\")\n",
    "        print(f\"Score: {scores_window}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.76130287,  0.2669243 ],\n",
       "       [-1.        , -0.35200716]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "def get_extra_obs(states, actions):\n",
    "    ''' \n",
    "        return a list contains other agents states and actions, len = num_agents\n",
    "        states: list of states by each agent\n",
    "        actions: List of action by each agent\n",
    "    '''\n",
    "    extra_obs = []\n",
    "    # print(f\"actions : {actions}\")\n",
    "    for i in range(states.shape[0]):\n",
    "        list_ = []\n",
    "        # states\n",
    "        list_.extend(states[i])\n",
    "        # agent's action\n",
    "        list_.extend(actions[i])\n",
    "        # other agent's actions\n",
    "        list_.extend(actions[np.arange(len(actions))!= i][0])\n",
    "        extra_obs.append(list_)\n",
    "    return extra_obs\n",
    "\n",
    "\n",
    "class Actor_critic_model(nn.Module):\n",
    "    def __init__(self, params_dir, input_dim, act_size, num_agents):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.act_size = act_size\n",
    "        self.num_agents = num_agents\n",
    "        self.params = parse_params(params_dir)\n",
    "        self.mu = self.create_module_list(self.create_actor)\n",
    "        self.std =  self.create_std()\n",
    "        self.val = self.create_module_list(self.create_critic)\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_in')\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def create_module_list(self, func):\n",
    "        module_list = nn.ModuleList()\n",
    "        for _ in range(self.num_agents):\n",
    "            module_list.append(func())\n",
    "        return module_list\n",
    "    \n",
    "    def create_std(self):\n",
    "        param_list = nn.ParameterList([nn.Parameter(\n",
    "            torch.ones(1, self.act_size)) for _ in range(self.num_agents)]\n",
    "                        )\n",
    "        return param_list\n",
    "\n",
    "    def create_actor(self):\n",
    "        module_list = nn.ModuleList()\n",
    "        layer = nn.Sequential()\n",
    "        fc = nn.Linear(self.input_dim, self.params['hidden_dim'])\n",
    "        layer.add_module(f\"fc_layer_1\", fc)\n",
    "        # layer.add_module(f\"bn_layer_1\",\n",
    "                        # nn.BatchNorm1d(self.params['hidden_dim']))\n",
    "        # layer.add_module(f\"RELU_layer_1\", nn.LeakyReLU())\n",
    "        layer.add_module(f\"RELU_layer_1\", nn.ReLU())\n",
    "        module_list.append(layer)\n",
    "        # module_list.apply(self._init_weights)\n",
    "        self.add_hidden_layer(module_list, self.params['actor_h_num'],\n",
    "                         self.params['hidden_dim'], self.params['hidden_dim'])\n",
    "        module_list.append(nn.Sequential(nn.Linear(self.params['hidden_dim'],\n",
    "                                          self.act_size)))\n",
    "        # module_list.apply(self._init_weights)\n",
    "        return module_list\n",
    "    \n",
    "    def create_critic(self):\n",
    "        module_list = nn.ModuleList()\n",
    "        layer = nn.Sequential()\n",
    "        fc = nn.Linear(self.input_dim + self.act_size * 2, self.params['hidden_dim'])\n",
    "        layer.add_module(f\"fc_layer_1\", fc)\n",
    "        # layer.add_module(f\"bn_layer_1\",\n",
    "                        # nn.BatchNorm1d(self.params['hidden_dim']))\n",
    "        # layer.add_module(f\"RELU_layer_1\", nn.LeakyReLU())\n",
    "        layer.add_module(f\"RELU_layer_1\", nn.ReLU())\n",
    "        module_list.append(layer)\n",
    "        self.add_hidden_layer(module_list, self.params['critic_h_num'],\n",
    "                         self.params['hidden_dim'], self.params['hidden_dim'])\n",
    "        module_list.append(nn.Sequential(nn.Linear(self.params['hidden_dim'], 1)))\n",
    "        # module_list.apply(self._init_weights)\n",
    "        return module_list\n",
    "    \n",
    "    def add_hidden_layer(self, module_list, num_hidden_layer,\n",
    "                         input_dim, output_dim):\n",
    "        if num_hidden_layer == 0:\n",
    "            return\n",
    "        for i in range(1, num_hidden_layer+1):\n",
    "            layer = nn.Sequential()\n",
    "            fc = nn.Linear(input_dim, output_dim)\n",
    "            layer.add_module(f\"fc_layer_{i}\", fc)\n",
    "            # layer.add_module(f\"bn_layer_{i}\",\n",
    "                          #    nn.BatchNorm1d(output_dim))\n",
    "            # layer.add_module(f\"RELU_layer_{i}\", nn.LeakyReLU())\n",
    "            layer.add_module(f\"RELU_layer_{i}\", nn.ReLU())\n",
    "            module_list.append(layer)\n",
    "            \n",
    "    def forward(self, states, actor=True, train=True, index=None):\n",
    "        '''\n",
    "            If actor is True, output actions and log probabilities FloatTensor\n",
    "            If Critic (actor = False), output state value FloatTensor\n",
    "        '''\n",
    "        x_ = torch.FloatTensor(states).to(self.device)\n",
    "        if actor:\n",
    "            # forward in actor path\n",
    "            mus = torch.zeros(self.num_agents, self.act_size, dtype=torch.float)\n",
    "            dists = []\n",
    "            acts = torch.zeros(self.num_agents, self.act_size, dtype=torch.float)\n",
    "            lps = torch.zeros(self.num_agents, self.act_size, dtype=torch.float)\n",
    "            for i in range(self.num_agents):\n",
    "                \n",
    "                mu_ = x_[i]\n",
    "                for m in self.mu[i]:\n",
    "                    mu_ = m(mu_)\n",
    "                mus[i] = (mu_)\n",
    "                dists.append(torch.distributions.Normal(mus[i], self.std[i]))\n",
    "                act_ = dists[i].sample()\n",
    "                if train:\n",
    "                    # only return log probabilities in training phases\n",
    "                    lps[i] = dists[i].log_prob(act_)\n",
    "                    acts[i] = torch.clamp(act_, -1, 1)\n",
    "                    return acts, lps\n",
    "                # return only actions in executation phases\n",
    "                return torch.clamp(act_, -1, 1)\n",
    "        # forward in value path\n",
    "        for v in self.val[index]:\n",
    "            x_ = v(x_)\n",
    "        return x_\n",
    "\n",
    "def parse_params(params_dir):\n",
    "    with open(params_dir) as fp:\n",
    "        params = json.load(fp)\n",
    "    return params\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, device, num_agents, params_dir, state_size, action_size):\n",
    "        self.model = Actor_critic_model(params_dir, state_size, action_size, num_agents).to(device)\n",
    "        \n",
    "        # I should try a version without target, just like A2C\n",
    "        self.target = Actor_critic_model(params_dir, state_size, action_size, num_agents).to(device)\n",
    "        self.device = device\n",
    "        self.num_agents = num_agents\n",
    "        self.params = self.model.params\n",
    "        self.optimizer = [optim.Adam(self.model.parameters(),\n",
    "                                    lr=self.params['lr']) for _ in range(self.num_agents)]\n",
    "                                    # lr=0.0001)\n",
    "\n",
    "    def __call__(self, states):\n",
    "        # mu, std, val, etp = self.model(states)\n",
    "        actions, log_prob = self.model(states)\n",
    "        return actions, log_prob\n",
    "\n",
    "    def step(self, memories):\n",
    "        '''\n",
    "        second edition\n",
    "            experiences:\n",
    "                list with n_steps_taken * [actions, rewards, log_probs,\n",
    "                                           not_dones, state_values]:\n",
    "                    actions (tensor: num agents * num actions)\n",
    "                    rewards (list: size = num agents)\n",
    "                    log_probs (tensor: num agents * num actions)\n",
    "                    not_dones (np array: size = num agents)\n",
    "                    state_values (list: size = num agents)\n",
    "        '''\n",
    "        loss = [0.0] * self.num_agents\n",
    "        for idx in range(self.num_agents):\n",
    "            actions, rewards, log_probs, not_dones, states, next_states= memories[idx].spit()\n",
    "            # print(f\"state_values : {state_values}\")\n",
    "            rewards = torch.FloatTensor(rewards).view(-1, 1)\n",
    "            #print(f\"len(rewards[0]) - 1 : {len(rewards) - 1}\")\n",
    "            not_dones = torch.FloatTensor(not_dones).to(device).unsqueeze(1)\n",
    "            # print(rewards)\n",
    "            state_values = self.model(states, actor=False, index=idx)\n",
    "            next_values = self.target(next_states, actor=False, index=idx)\n",
    "            returns = rewards + self.params['gamma'] * not_dones * next_values.detach()\n",
    "            advantage_  = rewards + self.params['gamma'] * not_dones * next_values.detach() - state_values.detach()\n",
    "            # print(f\"log_probs.shape : {log_probs.shape}\")\n",
    "            # print(f\"advantage_.shape : {advantage_.shape}\")\n",
    "            # print(f\"state_values[i].shape : {state_values[i].shape}\")\n",
    "            # print(f\"return_.shape : {return_.shape}\")\n",
    "            # print(f\"processed_experience : {processed_experience}\")\n",
    "            log_probs = torch.stack(log_probs)\n",
    "            policy_loss = -(log_probs) * advantage_\n",
    "            value_loss = (0.5 * (returns - state_values).pow(2))\n",
    "            self.optimizer[idx].zero_grad()\n",
    "            loss[idx] = ((policy_loss + value_loss.unsqueeze(1)).mean())\n",
    "            # print(f\"loss[idx] : {loss[idx]}\")\n",
    "            if torch.isnan(loss[idx]).any():\n",
    "                print('Nan in loss function')\n",
    "                pass\n",
    "            # loss[idx].backward()\n",
    "            if idx == (self.num_agents - 1):\n",
    "                loss[idx].backward()\n",
    "            else:\n",
    "                loss[idx].backward(retain_graph=True)\n",
    "            nn.utils.clip_grad_norm_(self.model.parameters(), self.model.params['grad_clip'])\n",
    "            self.optimizer[idx].step()\n",
    "        self.soft_udpate()\n",
    "        \n",
    "    def soft_udpate(self):\n",
    "        for tp, lp in zip(self.target.parameters(),\n",
    "                          self.model.parameters()):\n",
    "            tp.data.copy_(self.params['TAU']*lp.data +\n",
    "                          (1.0-self.params['TAU'])*tp.data)\n",
    "            \n",
    "    def hard_udpate(self):\n",
    "        for tp, lp in zip(self.target.parameters(),\n",
    "                          self.model.parameters()):\n",
    "            tp.data.copy_(lp.data)\n",
    "\n",
    "\n",
    "class Experience():\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        #self.extra_into = []\n",
    "        self.log_probs = []\n",
    "        self.not_dones = []\n",
    "        self.states = []\n",
    "        self.next_states = []\n",
    "        # self.etp = []\n",
    "\n",
    "    def add(self, actions, rewards, log_probs, not_dones,\n",
    "            states, next_states):\n",
    "        self.actions.append(actions)\n",
    "        self.rewards.append(rewards)\n",
    "        #self.extra_into.append(extra_into)\n",
    "        self.log_probs.append(log_probs)\n",
    "        self.not_dones.append(not_dones)\n",
    "        self.states.append(states)\n",
    "        self.next_states.append(next_states)\n",
    "        # self.etp.append(etp)\n",
    "\n",
    "    def spit(self):\n",
    "        return (self.actions[1:], self.rewards[1:], self.log_probs[1:],\n",
    "                self.not_dones[1:],\n",
    "                self.states[1:], self.next_states[1:])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rewards)\n",
    "    \n",
    "params_dir = f\"./params.txt\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = Agent(device, num_agents, params_dir, state_size, action_size)\n",
    "\n",
    "def plot_scores(scores):\n",
    "    fig, ax = plt.subplots(1, figsize=(8, 8))\n",
    "    plt.plot(np.arange(len(scores)), scores)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "scores_window = deque(maxlen=100)\n",
    "memories = [Experience() for _ in range(num_agents)]\n",
    "learned_steps = 0\n",
    "while learned_steps < 1000:\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    states_ = env_info.vector_observations\n",
    "    scores = np.zeros(num_agents)\n",
    "    done = [False] * num_agents\n",
    "    actions_ = np.random.random([num_agents, action_size])\n",
    "    log_prob_ = torch.rand(num_agents, action_size)\n",
    "    rewards_ = [0] * num_agents\n",
    "    states_plus = get_extra_obs(states, actions)\n",
    "    steps = 0\n",
    "    # while not np.any(done):\n",
    "    while steps < 20:\n",
    "        actions_next, log_prob_next = agent(states)\n",
    "        next_states_plus = get_extra_obs(states, actions_next.cpu().numpy())\n",
    "        env_info = env.step(actions_next.detach().cpu().numpy())[brain_name]\n",
    "        done = env_info.local_done\n",
    "        not_done_ = (1 - np.array(done))\n",
    "        for idx in range(num_agents):\n",
    "            memories[idx].add(actions_[idx], rewards_[idx], log_prob_[idx],\n",
    "                              not_done_[idx], states_plus[idx], next_states_plus[idx])\n",
    "        steps += 1\n",
    "\n",
    "        rewards_ = env_info.rewards\n",
    "        states = env_info.vector_observations\n",
    "        actions_ = actions_next\n",
    "        log_prob_ = log_prob_next\n",
    "        states_plus = next_states_plus\n",
    "        scores += rewards_\n",
    "        if (len(memories[0].actions) % 1000 == 0) and (len(memories[0].actions) >1):\n",
    "            agent.step(memories)\n",
    "            learned_steps += 1\n",
    "            memories = [Experience() for _ in range(num_agents)]\n",
    "            print(f\"learned_steps {learned_steps}: {np.max(scores)}\")\n",
    "        if learned_steps % 1000 == 0:\n",
    "            agent.hard_udpate()\n",
    "        if np.any(done):\n",
    "            # memories = Experience()\n",
    "            # print(f\" steps : {steps}\")\n",
    "            break\n",
    "        # print(scores)\n",
    "\n",
    "    scores_window.append(np.max(scores))\n",
    "    if (len(scores_window)) == 100 and ((sum(scores_window) / len(scores_window)) > 0.5):\n",
    "        torch.save(agent.model.state_dict(), agent.params['working_dir'])\n",
    "        print(f\"Envinroment solved in episode{steps}!\")\n",
    "        print(f\"Score: {scores_window}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True], dtype=bool)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(len(actions))!= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extra_obs(states, actions):\n",
    "    ''' \n",
    "        return a list contains other agents states and actions, len = num_agents\n",
    "        states: list of states by each agent\n",
    "        actions: List of action by each agent\n",
    "    '''\n",
    "    extra_obs = []\n",
    "    # print(f\"actions : {actions}\")\n",
    "    for i in range(states.shape[0]):\n",
    "        list_ = []\n",
    "        # states\n",
    "        list_.extend(states[i])\n",
    "        # agent's action\n",
    "        list_.extend(actions[i])\n",
    "        # other agent's actions\n",
    "        list_.extend(actions[np.arange(len(actions))!= i][0])\n",
    "        extra_obs.append(list_)\n",
    "    return extra_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_extra_obs(states, actions)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = get_extra_obs(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000, -7.4364, -1.5000, -0.0000,  0.0000,  6.6949,\n",
       "         5.9608, -0.0000,  0.0000, -1.0000, -0.8501,  0.9570,  0.6319])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor(a [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, -0.009999999776482582]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.91444326,  0.80984923,  0.56989909],\n",
       "       [ 0.18852114,  0.22988881,  0.49607161]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random([2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned_steps 1: 0.0\n",
      "learned_steps 2: 0.0\n",
      "learned_steps 3: 0.0\n",
      "learned_steps 4: 0.0\n",
      "learned_steps 5: 0.0\n",
      "learned_steps 6: 0.0\n",
      "learned_steps 7: 0.0\n",
      "learned_steps 8: 0.0\n",
      "learned_steps 9: 0.0\n",
      "learned_steps 10: 0.0\n",
      "learned_steps 11: 0.0\n",
      "learned_steps 12: 0.0\n",
      "learned_steps 13: 0.0\n",
      "learned_steps 14: 0.0\n",
      "learned_steps 15: 0.0\n",
      "learned_steps 16: 0.0\n",
      "learned_steps 17: 0.10000000149011612\n",
      "learned_steps 18: 0.0\n",
      "learned_steps 19: 0.10000000149011612\n",
      "learned_steps 20: 0.0\n",
      "learned_steps 21: 0.0\n",
      "learned_steps 22: 0.0\n",
      "learned_steps 23: 0.0\n",
      "learned_steps 24: 0.0\n",
      "learned_steps 25: 0.0\n",
      "learned_steps 26: 0.0\n",
      "learned_steps 27: 0.0\n",
      "learned_steps 28: 0.0\n",
      "learned_steps 29: 0.0\n",
      "learned_steps 30: 0.0\n",
      "learned_steps 31: 0.0\n",
      "learned_steps 32: 0.0\n",
      "learned_steps 33: 0.0\n",
      "learned_steps 34: 0.0\n",
      "learned_steps 35: 0.0\n",
      "learned_steps 36: 0.0\n",
      "learned_steps 37: 0.0\n",
      "learned_steps 38: 0.0\n",
      "learned_steps 39: 0.0\n",
      "learned_steps 40: 0.0\n",
      "learned_steps 41: 0.0\n",
      "learned_steps 42: 0.0\n",
      "learned_steps 43: 0.0\n",
      "learned_steps 44: 0.0\n",
      "learned_steps 45: 0.0\n",
      "learned_steps 46: 0.0\n",
      "learned_steps 47: 0.0\n",
      "learned_steps 48: 0.0\n",
      "learned_steps 49: 0.0\n",
      "learned_steps 50: 0.0\n",
      "learned_steps 51: 0.0\n",
      "learned_steps 52: 0.0\n",
      "learned_steps 53: 0.0\n",
      "learned_steps 54: 0.0\n",
      "learned_steps 55: 0.0\n",
      "learned_steps 56: 0.0\n",
      "learned_steps 57: 0.0\n",
      "learned_steps 58: 0.0\n",
      "learned_steps 59: 0.0\n",
      "learned_steps 60: 0.0\n",
      "learned_steps 61: 0.0\n",
      "learned_steps 62: 0.0\n",
      "learned_steps 63: 0.0\n",
      "learned_steps 64: 0.0\n",
      "learned_steps 65: 0.0\n",
      "learned_steps 66: 0.10000000149011612\n",
      "learned_steps 67: 0.0\n",
      "learned_steps 68: 0.0\n",
      "learned_steps 69: 0.0\n",
      "learned_steps 70: 0.0\n",
      "learned_steps 71: 0.0\n",
      "learned_steps 72: 0.0\n",
      "learned_steps 73: 0.10000000149011612\n",
      "learned_steps 74: 0.0\n",
      "learned_steps 75: 0.0\n",
      "learned_steps 76: 0.0\n",
      "learned_steps 77: 0.0\n",
      "learned_steps 78: 0.0\n",
      "learned_steps 79: 0.0\n",
      "learned_steps 80: 0.0\n",
      "learned_steps 81: 0.0\n",
      "learned_steps 82: 0.0\n",
      "learned_steps 83: 0.0\n",
      "learned_steps 84: 0.0\n",
      "learned_steps 85: 0.0\n",
      "learned_steps 86: 0.0\n",
      "learned_steps 87: 0.0\n",
      "learned_steps 88: 0.0\n",
      "learned_steps 89: 0.0\n",
      "learned_steps 90: 0.0\n",
      "learned_steps 91: 0.0\n",
      "learned_steps 92: 0.0\n",
      "learned_steps 93: 0.0\n",
      "learned_steps 94: 0.0\n",
      "learned_steps 95: 0.0\n",
      "learned_steps 96: 0.0\n",
      "learned_steps 97: 0.0\n",
      "learned_steps 98: 0.0\n",
      "learned_steps 99: 0.0\n",
      "learned_steps 100: 0.0\n",
      "learned_steps 101: 0.0\n",
      "learned_steps 102: 0.0\n",
      "learned_steps 103: 0.0\n",
      "learned_steps 104: 0.0\n",
      "learned_steps 105: 0.0\n",
      "learned_steps 106: 0.0\n",
      "learned_steps 107: 0.0\n",
      "learned_steps 108: 0.10000000149011612\n",
      "learned_steps 109: 0.0\n",
      "learned_steps 110: 0.0\n",
      "learned_steps 111: 0.10000000149011612\n",
      "learned_steps 112: 0.0\n",
      "learned_steps 113: 0.0\n",
      "learned_steps 114: 0.0\n",
      "learned_steps 115: 0.0\n",
      "learned_steps 116: 0.0\n",
      "learned_steps 117: 0.0\n",
      "learned_steps 118: 0.0\n",
      "learned_steps 119: 0.0\n",
      "learned_steps 120: 0.0\n",
      "learned_steps 121: 0.0\n",
      "learned_steps 122: 0.0\n",
      "learned_steps 123: 0.0\n",
      "learned_steps 124: 0.0\n",
      "learned_steps 125: 0.0\n",
      "learned_steps 126: 0.0\n",
      "learned_steps 127: 0.0\n",
      "learned_steps 128: 0.0\n",
      "learned_steps 129: 0.0\n",
      "learned_steps 130: 0.0\n",
      "learned_steps 131: 0.0\n",
      "learned_steps 132: 0.0\n",
      "learned_steps 133: 0.0\n",
      "learned_steps 134: 0.0\n",
      "learned_steps 135: 0.0\n",
      "learned_steps 136: 0.0\n",
      "learned_steps 137: 0.0\n",
      "learned_steps 138: 0.0\n",
      "learned_steps 139: 0.0\n",
      "learned_steps 140: 0.10000000149011612\n",
      "learned_steps 141: 0.0\n",
      "learned_steps 142: 0.0\n",
      "learned_steps 143: 0.0\n",
      "learned_steps 144: 0.0\n",
      "learned_steps 145: 0.10000000149011612\n",
      "learned_steps 146: 0.0\n",
      "learned_steps 147: 0.0\n",
      "learned_steps 148: 0.0\n",
      "learned_steps 149: 0.0\n",
      "learned_steps 150: 0.0\n",
      "learned_steps 151: 0.0\n",
      "learned_steps 152: 0.10000000149011612\n",
      "learned_steps 153: 0.0\n",
      "learned_steps 154: 0.0\n",
      "learned_steps 155: 0.0\n",
      "learned_steps 156: 0.0\n",
      "learned_steps 157: 0.0\n",
      "learned_steps 158: 0.0\n",
      "learned_steps 159: 0.10000000149011612\n",
      "learned_steps 160: 0.0\n",
      "learned_steps 161: 0.0\n",
      "learned_steps 162: 0.0\n",
      "learned_steps 163: 0.0\n",
      "learned_steps 164: 0.0\n",
      "learned_steps 165: 0.0\n",
      "learned_steps 166: 0.0\n",
      "learned_steps 167: 0.0\n",
      "learned_steps 168: 0.0\n",
      "learned_steps 169: 0.0\n",
      "learned_steps 170: 0.0\n",
      "learned_steps 171: 0.0\n",
      "learned_steps 172: 0.0\n",
      "learned_steps 173: 0.0\n",
      "learned_steps 174: 0.0\n",
      "learned_steps 175: 0.0\n",
      "learned_steps 176: 0.0\n",
      "learned_steps 177: 0.0\n",
      "learned_steps 178: 0.0\n",
      "learned_steps 179: 0.0\n",
      "learned_steps 180: 0.0\n",
      "learned_steps 181: 0.0\n",
      "learned_steps 182: 0.0\n",
      "learned_steps 183: 0.0\n",
      "learned_steps 184: 0.0\n",
      "learned_steps 185: 0.0\n",
      "learned_steps 186: 0.0\n",
      "learned_steps 187: 0.0\n",
      "learned_steps 188: 0.0\n",
      "learned_steps 189: 0.0\n",
      "learned_steps 190: 0.0\n",
      "learned_steps 191: 0.0\n",
      "learned_steps 192: 0.10000000149011612\n",
      "learned_steps 193: 0.0\n",
      "learned_steps 194: 0.0\n",
      "learned_steps 195: 0.10000000149011612\n",
      "learned_steps 196: 0.0\n",
      "learned_steps 197: 0.0\n",
      "learned_steps 198: 0.0\n",
      "learned_steps 199: 0.0\n",
      "learned_steps 200: 0.0\n",
      "learned_steps 201: 0.0\n",
      "learned_steps 202: 0.0\n",
      "learned_steps 203: 0.0\n",
      "learned_steps 204: 0.0\n",
      "learned_steps 205: 0.0\n",
      "learned_steps 206: 0.0\n",
      "learned_steps 207: 0.0\n",
      "learned_steps 208: 0.0\n",
      "learned_steps 209: 0.0\n",
      "learned_steps 210: 0.0\n",
      "learned_steps 211: 0.0\n",
      "learned_steps 212: 0.0\n",
      "learned_steps 213: 0.0\n",
      "learned_steps 214: 0.0\n",
      "learned_steps 215: 0.0\n",
      "learned_steps 216: 0.0\n",
      "learned_steps 217: 0.10000000149011612\n",
      "learned_steps 218: 0.0\n",
      "learned_steps 219: 0.0\n",
      "learned_steps 220: 0.0\n",
      "learned_steps 221: 0.0\n",
      "learned_steps 222: 0.0\n",
      "learned_steps 223: 0.0\n",
      "learned_steps 224: 0.0\n",
      "learned_steps 225: 0.0\n",
      "learned_steps 226: 0.0\n",
      "learned_steps 227: 0.0\n",
      "learned_steps 228: 0.0\n",
      "learned_steps 229: 0.0\n",
      "learned_steps 230: 0.0\n",
      "learned_steps 231: 0.0\n",
      "learned_steps 232: 0.0\n",
      "learned_steps 233: 0.0\n",
      "learned_steps 234: 0.0\n",
      "learned_steps 235: 0.0\n",
      "learned_steps 236: 0.0\n",
      "learned_steps 237: 0.0\n",
      "learned_steps 238: 0.0\n",
      "learned_steps 239: 0.0\n",
      "learned_steps 240: 0.0\n",
      "learned_steps 241: 0.0\n",
      "learned_steps 242: 0.0\n",
      "learned_steps 243: 0.0\n",
      "learned_steps 244: 0.0\n",
      "learned_steps 245: 0.0\n",
      "learned_steps 246: 0.0\n",
      "learned_steps 247: 0.0\n",
      "learned_steps 248: 0.0\n",
      "learned_steps 249: 0.0\n",
      "learned_steps 250: 0.0\n",
      "learned_steps 251: 0.0\n",
      "learned_steps 252: 0.0\n",
      "learned_steps 253: 0.0\n",
      "learned_steps 254: 0.0\n",
      "learned_steps 255: 0.0\n",
      "learned_steps 256: 0.0\n",
      "learned_steps 257: 0.0\n",
      "learned_steps 258: 0.0\n",
      "learned_steps 259: 0.0\n",
      "learned_steps 260: 0.0\n",
      "learned_steps 261: 0.0\n",
      "learned_steps 262: 0.0\n",
      "learned_steps 263: 0.0\n",
      "learned_steps 264: 0.0\n",
      "learned_steps 265: 0.0\n",
      "learned_steps 266: 0.0\n",
      "learned_steps 267: 0.0\n",
      "learned_steps 268: 0.0\n",
      "learned_steps 269: 0.0\n",
      "learned_steps 270: 0.0\n",
      "learned_steps 271: 0.0\n",
      "learned_steps 272: 0.0\n",
      "learned_steps 273: 0.0\n",
      "learned_steps 274: 0.10000000149011612\n",
      "learned_steps 275: 0.0\n",
      "learned_steps 276: 0.0\n",
      "learned_steps 277: 0.0\n",
      "learned_steps 278: 0.0\n",
      "learned_steps 279: 0.0\n",
      "learned_steps 280: 0.0\n",
      "learned_steps 281: 0.0\n",
      "learned_steps 282: 0.0\n",
      "learned_steps 283: 0.0\n",
      "learned_steps 284: 0.0\n",
      "learned_steps 285: 0.0\n",
      "learned_steps 286: 0.0\n",
      "learned_steps 287: 0.0\n",
      "learned_steps 288: 0.0\n",
      "learned_steps 289: 0.0\n",
      "learned_steps 290: 0.0\n",
      "learned_steps 291: 0.0\n",
      "learned_steps 292: 0.0\n",
      "learned_steps 293: 0.0\n",
      "learned_steps 294: 0.0\n",
      "learned_steps 295: 0.10000000149011612\n",
      "learned_steps 296: 0.0\n",
      "learned_steps 297: 0.0\n",
      "learned_steps 298: 0.0\n",
      "learned_steps 299: 0.0\n",
      "learned_steps 300: 0.0\n",
      "learned_steps 301: 0.0\n",
      "learned_steps 302: 0.0\n",
      "learned_steps 303: 0.0\n",
      "learned_steps 304: 0.0\n",
      "learned_steps 305: 0.0\n",
      "learned_steps 306: 0.0\n",
      "learned_steps 307: 0.0\n",
      "learned_steps 308: 0.10000000149011612\n",
      "learned_steps 309: 0.0\n",
      "learned_steps 310: 0.0\n",
      "learned_steps 311: 0.0\n",
      "learned_steps 312: 0.0\n",
      "learned_steps 313: 0.0\n",
      "learned_steps 314: 0.0\n",
      "learned_steps 315: 0.0\n",
      "learned_steps 316: 0.0\n",
      "learned_steps 317: 0.0\n",
      "learned_steps 318: 0.0\n",
      "learned_steps 319: 0.0\n",
      "learned_steps 320: 0.0\n",
      "learned_steps 321: 0.0\n",
      "learned_steps 322: 0.0\n",
      "learned_steps 323: 0.0\n",
      "learned_steps 324: 0.0\n",
      "learned_steps 325: 0.0\n",
      "learned_steps 326: 0.0\n",
      "learned_steps 327: 0.0\n",
      "learned_steps 328: 0.0\n",
      "learned_steps 329: 0.0\n",
      "learned_steps 330: 0.0\n",
      "learned_steps 331: 0.0\n",
      "learned_steps 332: 0.0\n",
      "learned_steps 333: 0.0\n",
      "learned_steps 334: 0.0\n",
      "learned_steps 335: 0.0\n",
      "learned_steps 336: 0.0\n",
      "learned_steps 337: 0.0\n",
      "learned_steps 338: 0.0\n",
      "learned_steps 339: 0.0\n",
      "learned_steps 340: 0.0\n",
      "learned_steps 341: 0.0\n",
      "learned_steps 342: 0.0\n",
      "learned_steps 343: 0.0\n",
      "learned_steps 344: 0.0\n",
      "learned_steps 345: 0.0\n",
      "learned_steps 346: 0.0\n",
      "learned_steps 347: 0.0\n",
      "learned_steps 348: 0.0\n",
      "learned_steps 349: 0.0\n",
      "learned_steps 350: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned_steps 351: 0.0\n",
      "learned_steps 352: 0.0\n",
      "learned_steps 353: 0.0\n",
      "learned_steps 354: 0.0\n",
      "learned_steps 355: 0.0\n",
      "learned_steps 356: 0.0\n",
      "learned_steps 357: 0.0\n",
      "learned_steps 358: 0.0\n",
      "learned_steps 359: 0.0\n",
      "learned_steps 360: 0.0\n",
      "learned_steps 361: 0.0\n",
      "learned_steps 362: 0.10000000149011612\n",
      "learned_steps 363: 0.0\n",
      "learned_steps 364: 0.0\n",
      "learned_steps 365: 0.0\n",
      "learned_steps 366: 0.10000000149011612\n",
      "learned_steps 367: 0.0\n",
      "learned_steps 368: 0.0\n",
      "learned_steps 369: 0.0\n",
      "learned_steps 370: 0.0\n",
      "learned_steps 371: 0.0\n",
      "learned_steps 372: 0.0\n",
      "learned_steps 373: 0.0\n",
      "learned_steps 374: 0.0\n",
      "learned_steps 375: 0.0\n",
      "learned_steps 376: 0.0\n",
      "learned_steps 377: 0.0\n",
      "learned_steps 378: 0.0\n",
      "learned_steps 379: 0.0\n",
      "learned_steps 380: 0.0\n",
      "learned_steps 381: 0.0\n",
      "learned_steps 382: 0.0\n",
      "learned_steps 383: 0.0\n",
      "learned_steps 384: 0.0\n",
      "learned_steps 385: 0.0\n",
      "learned_steps 386: 0.0\n",
      "learned_steps 387: 0.0\n",
      "learned_steps 388: 0.0\n",
      "learned_steps 389: 0.0\n",
      "learned_steps 390: 0.0\n",
      "learned_steps 391: 0.0\n",
      "learned_steps 392: 0.0\n",
      "learned_steps 393: 0.0\n",
      "learned_steps 394: 0.0\n",
      "learned_steps 395: 0.0\n",
      "learned_steps 396: 0.0\n",
      "learned_steps 397: 0.0\n",
      "learned_steps 398: 0.0\n",
      "learned_steps 399: 0.0\n",
      "learned_steps 400: 0.0\n",
      "learned_steps 401: 0.0\n",
      "learned_steps 402: 0.0\n",
      "learned_steps 403: 0.0\n",
      "learned_steps 404: 0.0\n",
      "learned_steps 405: 0.0\n",
      "learned_steps 406: 0.0\n",
      "learned_steps 407: 0.0\n",
      "learned_steps 408: 0.0\n",
      "learned_steps 409: 0.0\n",
      "learned_steps 410: 0.0\n",
      "learned_steps 411: 0.0\n",
      "learned_steps 412: 0.0\n",
      "learned_steps 413: 0.0\n",
      "learned_steps 414: 0.0\n",
      "learned_steps 415: 0.0\n",
      "learned_steps 416: 0.0\n",
      "learned_steps 417: 0.09000000357627869\n",
      "learned_steps 418: 0.0\n",
      "learned_steps 419: 0.0\n",
      "learned_steps 420: 0.10000000149011612\n",
      "learned_steps 421: 0.0\n",
      "learned_steps 422: 0.0\n",
      "learned_steps 423: 0.0\n",
      "learned_steps 424: 0.0\n",
      "learned_steps 425: 0.0\n",
      "learned_steps 426: 0.0\n",
      "learned_steps 427: 0.0\n",
      "learned_steps 428: 0.0\n",
      "learned_steps 429: 0.0\n",
      "learned_steps 430: 0.0\n",
      "learned_steps 431: 0.10000000149011612\n",
      "learned_steps 432: 0.0\n",
      "learned_steps 433: 0.0\n",
      "learned_steps 434: 0.0\n",
      "learned_steps 435: 0.0\n",
      "learned_steps 436: 0.0\n",
      "learned_steps 437: 0.0\n",
      "learned_steps 438: 0.0\n",
      "learned_steps 439: 0.0\n",
      "learned_steps 440: 0.10000000149011612\n",
      "learned_steps 441: 0.0\n",
      "learned_steps 442: 0.0\n",
      "learned_steps 443: 0.0\n",
      "learned_steps 444: 0.0\n",
      "learned_steps 445: 0.0\n",
      "learned_steps 446: 0.0\n",
      "learned_steps 447: 0.0\n",
      "learned_steps 448: 0.0\n",
      "learned_steps 449: 0.0\n",
      "learned_steps 450: 0.0\n",
      "learned_steps 451: 0.0\n",
      "learned_steps 452: 0.0\n",
      "learned_steps 453: 0.0\n",
      "learned_steps 454: 0.0\n",
      "learned_steps 455: 0.0\n",
      "learned_steps 456: 0.0\n",
      "learned_steps 457: 0.0\n",
      "learned_steps 458: 0.0\n",
      "learned_steps 459: 0.0\n",
      "learned_steps 460: 0.10000000149011612\n",
      "learned_steps 461: 0.0\n",
      "learned_steps 462: 0.0\n",
      "learned_steps 463: 0.0\n",
      "learned_steps 464: 0.0\n",
      "learned_steps 465: 0.10000000149011612\n",
      "learned_steps 466: 0.0\n",
      "learned_steps 467: 0.0\n",
      "learned_steps 468: 0.0\n",
      "learned_steps 469: 0.0\n",
      "learned_steps 470: 0.0\n",
      "learned_steps 471: 0.0\n",
      "learned_steps 472: 0.0\n",
      "learned_steps 473: 0.0\n",
      "learned_steps 474: 0.0\n",
      "learned_steps 475: 0.0\n",
      "learned_steps 476: 0.0\n",
      "learned_steps 477: 0.0\n",
      "learned_steps 478: 0.0\n",
      "learned_steps 479: 0.0\n",
      "learned_steps 480: 0.0\n",
      "learned_steps 481: 0.0\n",
      "learned_steps 482: 0.0\n",
      "learned_steps 483: 0.0\n",
      "learned_steps 484: 0.0\n",
      "learned_steps 485: 0.0\n",
      "learned_steps 486: 0.0\n",
      "learned_steps 487: 0.0\n",
      "learned_steps 488: 0.0\n",
      "learned_steps 489: 0.0\n",
      "learned_steps 490: 0.0\n",
      "learned_steps 491: 0.0\n",
      "learned_steps 492: 0.0\n",
      "learned_steps 493: 0.0\n",
      "learned_steps 494: 0.0\n",
      "learned_steps 495: 0.0\n",
      "learned_steps 496: 0.0\n",
      "learned_steps 497: 0.0\n",
      "learned_steps 498: 0.0\n",
      "learned_steps 499: 0.0\n",
      "learned_steps 500: 0.0\n",
      "learned_steps 501: 0.0\n",
      "learned_steps 502: 0.0\n",
      "learned_steps 503: 0.10000000149011612\n",
      "learned_steps 504: 0.0\n",
      "learned_steps 505: 0.0\n",
      "learned_steps 506: 0.0\n",
      "learned_steps 507: 0.0\n",
      "learned_steps 508: 0.0\n",
      "learned_steps 509: 0.0\n",
      "learned_steps 510: 0.0\n",
      "learned_steps 511: 0.0\n",
      "learned_steps 512: 0.0\n",
      "learned_steps 513: 0.0\n",
      "learned_steps 514: 0.0\n",
      "learned_steps 515: 0.0\n",
      "learned_steps 516: 0.0\n",
      "learned_steps 517: 0.0\n",
      "learned_steps 518: 0.0\n",
      "learned_steps 519: 0.0\n",
      "learned_steps 520: 0.0\n",
      "learned_steps 521: 0.0\n",
      "learned_steps 522: 0.0\n",
      "learned_steps 523: 0.0\n",
      "learned_steps 524: 0.0\n",
      "learned_steps 525: 0.0\n",
      "learned_steps 526: 0.0\n",
      "learned_steps 527: 0.0\n",
      "learned_steps 528: 0.0\n",
      "learned_steps 529: 0.0\n",
      "learned_steps 530: 0.0\n",
      "learned_steps 531: 0.0\n",
      "learned_steps 532: 0.0\n",
      "learned_steps 533: 0.0\n",
      "learned_steps 534: 0.0\n",
      "learned_steps 535: 0.10000000149011612\n",
      "learned_steps 536: 0.0\n",
      "learned_steps 537: 0.0\n",
      "learned_steps 538: 0.0\n",
      "learned_steps 539: 0.0\n",
      "learned_steps 540: 0.10000000149011612\n",
      "learned_steps 541: 0.0\n",
      "learned_steps 542: 0.0\n",
      "learned_steps 543: 0.0\n",
      "learned_steps 544: 0.0\n",
      "learned_steps 545: 0.0\n",
      "learned_steps 546: 0.0\n",
      "learned_steps 547: 0.0\n",
      "learned_steps 548: 0.0\n",
      "learned_steps 549: 0.0\n",
      "learned_steps 550: 0.0\n",
      "learned_steps 551: 0.0\n",
      "learned_steps 552: 0.0\n",
      "learned_steps 553: 0.0\n",
      "learned_steps 554: 0.0\n",
      "learned_steps 555: 0.0\n",
      "learned_steps 556: 0.0\n",
      "learned_steps 557: 0.0\n",
      "learned_steps 558: 0.0\n",
      "learned_steps 559: 0.0\n",
      "learned_steps 560: 0.0\n",
      "learned_steps 561: 0.0\n",
      "learned_steps 562: 0.0\n",
      "learned_steps 563: 0.0\n",
      "learned_steps 564: 0.0\n",
      "learned_steps 565: 0.0\n",
      "learned_steps 566: 0.0\n",
      "learned_steps 567: 0.0\n",
      "learned_steps 568: 0.0\n",
      "learned_steps 569: 0.0\n",
      "learned_steps 570: 0.0\n",
      "learned_steps 571: 0.0\n",
      "learned_steps 572: 0.0\n",
      "learned_steps 573: 0.0\n",
      "learned_steps 574: 0.0\n",
      "learned_steps 575: 0.0\n",
      "learned_steps 576: 0.0\n",
      "learned_steps 577: 0.0\n",
      "learned_steps 578: 0.10000000149011612\n",
      "learned_steps 579: 0.0\n",
      "learned_steps 580: 0.0\n",
      "learned_steps 581: 0.0\n",
      "learned_steps 582: 0.0\n",
      "learned_steps 583: 0.0\n",
      "learned_steps 584: 0.0\n",
      "learned_steps 585: 0.0\n",
      "learned_steps 586: 0.10000000149011612\n",
      "learned_steps 587: 0.0\n",
      "learned_steps 588: 0.10000000149011612\n",
      "learned_steps 589: 0.0\n",
      "learned_steps 590: 0.0\n",
      "learned_steps 591: 0.0\n",
      "learned_steps 592: 0.0\n",
      "learned_steps 593: 0.0\n",
      "learned_steps 594: 0.0\n",
      "learned_steps 595: 0.0\n",
      "learned_steps 596: 0.0\n",
      "learned_steps 597: 0.0\n",
      "learned_steps 598: 0.0\n",
      "learned_steps 599: 0.0\n",
      "learned_steps 600: 0.0\n",
      "learned_steps 601: 0.0\n",
      "learned_steps 602: 0.0\n",
      "learned_steps 603: 0.0\n",
      "learned_steps 604: 0.0\n",
      "learned_steps 605: 0.0\n",
      "learned_steps 606: 0.0\n",
      "learned_steps 607: 0.10000000149011612\n",
      "learned_steps 608: 0.0\n",
      "learned_steps 609: 0.0\n",
      "learned_steps 610: 0.0\n",
      "learned_steps 611: 0.0\n",
      "learned_steps 612: 0.0\n",
      "learned_steps 613: 0.0\n",
      "learned_steps 614: 0.0\n",
      "learned_steps 615: 0.0\n",
      "learned_steps 616: 0.0\n",
      "learned_steps 617: 0.0\n",
      "learned_steps 618: 0.10000000149011612\n",
      "learned_steps 619: 0.0\n",
      "learned_steps 620: 0.10000000149011612\n",
      "learned_steps 621: 0.0\n",
      "learned_steps 622: 0.10000000149011612\n",
      "learned_steps 623: 0.0\n",
      "learned_steps 624: 0.0\n",
      "learned_steps 625: 0.0\n",
      "learned_steps 626: 0.0\n",
      "learned_steps 627: 0.0\n",
      "learned_steps 628: 0.0\n",
      "learned_steps 629: 0.0\n",
      "learned_steps 630: 0.0\n",
      "learned_steps 631: 0.0\n",
      "learned_steps 632: 0.0\n",
      "learned_steps 633: 0.0\n",
      "learned_steps 634: 0.0\n",
      "learned_steps 635: 0.0\n",
      "learned_steps 636: 0.0\n",
      "learned_steps 637: 0.0\n",
      "learned_steps 638: 0.0\n",
      "learned_steps 639: 0.0\n",
      "learned_steps 640: 0.0\n",
      "learned_steps 641: 0.0\n",
      "learned_steps 642: 0.0\n",
      "learned_steps 643: 0.0\n",
      "learned_steps 644: 0.0\n",
      "learned_steps 645: 0.0\n",
      "learned_steps 646: 0.0\n",
      "learned_steps 647: 0.0\n",
      "learned_steps 648: 0.0\n",
      "learned_steps 649: 0.0\n",
      "learned_steps 650: 0.0\n",
      "learned_steps 651: 0.0\n",
      "learned_steps 652: 0.0\n",
      "learned_steps 653: 0.0\n",
      "learned_steps 654: 0.0\n",
      "learned_steps 655: 0.0\n",
      "learned_steps 656: 0.0\n",
      "learned_steps 657: 0.0\n",
      "learned_steps 658: 0.0\n",
      "learned_steps 659: 0.0\n",
      "learned_steps 660: 0.0\n",
      "learned_steps 661: 0.0\n",
      "learned_steps 662: 0.10000000149011612\n",
      "learned_steps 663: 0.0\n",
      "learned_steps 664: 0.0\n",
      "learned_steps 665: 0.0\n",
      "learned_steps 666: 0.0\n",
      "learned_steps 667: 0.0\n",
      "learned_steps 668: 0.0\n",
      "learned_steps 669: 0.0\n",
      "learned_steps 670: 0.0\n",
      "learned_steps 671: 0.0\n",
      "learned_steps 672: 0.0\n",
      "learned_steps 673: 0.0\n",
      "learned_steps 674: 0.0\n",
      "learned_steps 675: 0.0\n",
      "learned_steps 676: 0.0\n",
      "learned_steps 677: 0.0\n",
      "learned_steps 678: 0.0\n",
      "learned_steps 679: 0.0\n",
      "learned_steps 680: 0.0\n",
      "learned_steps 681: 0.0\n",
      "learned_steps 682: 0.0\n",
      "learned_steps 683: 0.0\n",
      "learned_steps 684: 0.0\n",
      "learned_steps 685: 0.0\n",
      "learned_steps 686: 0.0\n",
      "learned_steps 687: 0.0\n",
      "learned_steps 688: 0.0\n",
      "learned_steps 689: 0.0\n",
      "learned_steps 690: 0.0\n",
      "learned_steps 691: 0.0\n",
      "learned_steps 692: 0.0\n",
      "learned_steps 693: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned_steps 694: 0.0\n",
      "learned_steps 695: 0.0\n",
      "learned_steps 696: 0.0\n",
      "learned_steps 697: 0.10000000149011612\n",
      "learned_steps 698: 0.0\n",
      "learned_steps 699: 0.0\n",
      "learned_steps 700: 0.0\n",
      "learned_steps 701: 0.0\n",
      "learned_steps 702: 0.0\n",
      "learned_steps 703: 0.0\n",
      "learned_steps 704: 0.10000000149011612\n",
      "learned_steps 705: 0.0\n",
      "learned_steps 706: 0.0\n",
      "learned_steps 707: 0.0\n",
      "learned_steps 708: 0.0\n",
      "learned_steps 709: 0.0\n",
      "learned_steps 710: 0.0\n",
      "learned_steps 711: 0.0\n",
      "learned_steps 712: 0.0\n",
      "learned_steps 713: 0.0\n",
      "learned_steps 714: 0.0\n",
      "learned_steps 715: 0.0\n",
      "learned_steps 716: 0.0\n",
      "learned_steps 717: 0.0\n",
      "learned_steps 718: 0.10000000149011612\n",
      "learned_steps 719: 0.0\n",
      "learned_steps 720: 0.0\n",
      "learned_steps 721: 0.10000000149011612\n",
      "learned_steps 722: 0.0\n",
      "learned_steps 723: 0.0\n",
      "learned_steps 724: 0.0\n",
      "learned_steps 725: 0.0\n",
      "learned_steps 726: 0.0\n",
      "learned_steps 727: 0.0\n",
      "learned_steps 728: 0.0\n",
      "learned_steps 729: 0.0\n",
      "learned_steps 730: 0.0\n",
      "learned_steps 731: 0.0\n",
      "learned_steps 732: 0.0\n",
      "learned_steps 733: 0.0\n",
      "learned_steps 734: 0.0\n",
      "learned_steps 735: 0.0\n",
      "learned_steps 736: 0.0\n",
      "learned_steps 737: 0.0\n",
      "learned_steps 738: 0.0\n",
      "learned_steps 739: 0.0\n",
      "learned_steps 740: 0.0\n",
      "learned_steps 741: 0.0\n",
      "learned_steps 742: 0.0\n",
      "learned_steps 743: 0.0\n",
      "learned_steps 744: 0.0\n",
      "learned_steps 745: 0.0\n",
      "learned_steps 746: 0.0\n",
      "learned_steps 747: 0.0\n",
      "learned_steps 748: 0.0\n",
      "learned_steps 749: 0.0\n",
      "learned_steps 750: 0.0\n",
      "learned_steps 751: 0.0\n",
      "learned_steps 752: 0.0\n",
      "learned_steps 753: 0.0\n",
      "learned_steps 754: 0.0\n",
      "learned_steps 755: 0.0\n",
      "learned_steps 756: 0.0\n",
      "learned_steps 757: 0.0\n",
      "learned_steps 758: 0.0\n",
      "learned_steps 759: 0.0\n",
      "learned_steps 760: 0.0\n",
      "learned_steps 761: 0.0\n",
      "learned_steps 762: 0.0\n",
      "learned_steps 763: 0.0\n",
      "learned_steps 764: 0.0\n",
      "learned_steps 765: 0.0\n",
      "learned_steps 766: 0.0\n",
      "learned_steps 767: 0.0\n",
      "learned_steps 768: 0.0\n",
      "learned_steps 769: 0.0\n",
      "learned_steps 770: 0.0\n",
      "learned_steps 771: 0.0\n",
      "learned_steps 772: 0.0\n",
      "learned_steps 773: 0.0\n",
      "learned_steps 774: 0.0\n",
      "learned_steps 775: 0.0\n",
      "learned_steps 776: 0.0\n",
      "learned_steps 777: 0.0\n",
      "learned_steps 778: 0.0\n",
      "learned_steps 779: 0.0\n",
      "learned_steps 780: 0.0\n",
      "learned_steps 781: 0.0\n",
      "learned_steps 782: 0.0\n",
      "learned_steps 783: 0.0\n",
      "learned_steps 784: 0.0\n",
      "learned_steps 785: 0.0\n",
      "learned_steps 786: 0.0\n",
      "learned_steps 787: 0.10000000149011612\n",
      "learned_steps 788: 0.0\n",
      "learned_steps 789: 0.0\n",
      "learned_steps 790: 0.0\n",
      "learned_steps 791: 0.0\n",
      "learned_steps 792: 0.0\n",
      "learned_steps 793: 0.0\n",
      "learned_steps 794: 0.0\n",
      "learned_steps 795: 0.0\n",
      "learned_steps 796: 0.0\n",
      "learned_steps 797: 0.0\n",
      "learned_steps 798: 0.0\n",
      "learned_steps 799: 0.0\n",
      "learned_steps 800: 0.0\n",
      "learned_steps 801: 0.0\n",
      "learned_steps 802: 0.0\n",
      "learned_steps 803: 0.10000000149011612\n",
      "learned_steps 804: 0.0\n",
      "learned_steps 805: 0.10000000149011612\n",
      "learned_steps 806: 0.10000000149011612\n",
      "learned_steps 807: 0.0\n",
      "learned_steps 808: 0.0\n",
      "learned_steps 809: 0.0\n",
      "learned_steps 810: 0.0\n",
      "learned_steps 811: 0.0\n",
      "learned_steps 812: 0.0\n",
      "learned_steps 813: 0.0\n",
      "learned_steps 814: 0.0\n",
      "learned_steps 815: 0.0\n",
      "learned_steps 816: 0.0\n",
      "learned_steps 817: 0.0\n",
      "learned_steps 818: 0.0\n",
      "learned_steps 819: 0.0\n",
      "learned_steps 820: 0.0\n",
      "learned_steps 821: 0.0\n",
      "learned_steps 822: 0.0\n",
      "learned_steps 823: 0.10000000149011612\n",
      "learned_steps 824: 0.0\n",
      "learned_steps 825: 0.0\n",
      "learned_steps 826: 0.0\n",
      "learned_steps 827: 0.0\n",
      "learned_steps 828: 0.0\n",
      "learned_steps 829: 0.10000000149011612\n",
      "learned_steps 830: 0.0\n",
      "learned_steps 831: 0.0\n",
      "learned_steps 832: 0.0\n",
      "learned_steps 833: 0.0\n",
      "learned_steps 834: 0.0\n",
      "learned_steps 835: 0.0\n",
      "learned_steps 836: 0.0\n",
      "learned_steps 837: 0.0\n",
      "learned_steps 838: 0.0\n",
      "learned_steps 839: 0.10000000149011612\n",
      "learned_steps 840: 0.0\n",
      "learned_steps 841: 0.0\n",
      "learned_steps 842: 0.0\n",
      "learned_steps 843: 0.0\n",
      "learned_steps 844: 0.0\n",
      "learned_steps 845: 0.10000000149011612\n",
      "learned_steps 846: 0.0\n",
      "learned_steps 847: 0.0\n",
      "learned_steps 848: 0.0\n",
      "learned_steps 849: 0.0\n",
      "learned_steps 850: 0.0\n",
      "learned_steps 851: 0.0\n",
      "learned_steps 852: 0.10000000149011612\n",
      "learned_steps 853: 0.0\n",
      "learned_steps 854: 0.10000000149011612\n",
      "learned_steps 855: 0.0\n",
      "learned_steps 856: 0.0\n",
      "learned_steps 857: 0.0\n",
      "learned_steps 858: 0.0\n",
      "learned_steps 859: 0.0\n",
      "learned_steps 860: 0.0\n",
      "learned_steps 861: 0.0\n",
      "learned_steps 862: 0.0\n",
      "learned_steps 863: 0.0\n",
      "learned_steps 864: 0.0\n",
      "learned_steps 865: 0.0\n",
      "learned_steps 866: 0.0\n",
      "learned_steps 867: 0.0\n",
      "learned_steps 868: 0.0\n",
      "learned_steps 869: 0.0\n",
      "learned_steps 870: 0.0\n",
      "learned_steps 871: 0.10000000149011612\n",
      "learned_steps 872: 0.0\n",
      "learned_steps 873: 0.0\n",
      "learned_steps 874: 0.0\n",
      "learned_steps 875: 0.0\n",
      "learned_steps 876: 0.0\n",
      "learned_steps 877: 0.0\n",
      "learned_steps 878: 0.0\n",
      "learned_steps 879: 0.0\n",
      "learned_steps 880: 0.0\n",
      "learned_steps 881: 0.0\n",
      "learned_steps 882: 0.0\n",
      "learned_steps 883: 0.0\n",
      "learned_steps 884: 0.0\n",
      "learned_steps 885: 0.0\n",
      "learned_steps 886: 0.0\n",
      "learned_steps 887: 0.0\n",
      "learned_steps 888: 0.0\n",
      "learned_steps 889: 0.0\n",
      "learned_steps 890: 0.0\n",
      "learned_steps 891: 0.0\n",
      "learned_steps 892: 0.0\n",
      "learned_steps 893: 0.0\n",
      "learned_steps 894: 0.0\n",
      "learned_steps 895: 0.0\n",
      "learned_steps 896: 0.0\n",
      "learned_steps 897: 0.0\n",
      "learned_steps 898: 0.0\n",
      "learned_steps 899: 0.0\n",
      "learned_steps 900: 0.0\n",
      "learned_steps 901: 0.0\n",
      "learned_steps 902: 0.0\n",
      "learned_steps 903: 0.0\n",
      "learned_steps 904: 0.0\n",
      "learned_steps 905: 0.0\n",
      "learned_steps 906: 0.0\n",
      "learned_steps 907: 0.0\n",
      "learned_steps 908: 0.0\n",
      "learned_steps 909: 0.0\n",
      "learned_steps 910: 0.0\n",
      "learned_steps 911: 0.0\n",
      "learned_steps 912: 0.0\n",
      "learned_steps 913: 0.0\n",
      "learned_steps 914: 0.0\n",
      "learned_steps 915: 0.0\n",
      "learned_steps 916: 0.0\n",
      "learned_steps 917: 0.0\n",
      "learned_steps 918: 0.0\n",
      "learned_steps 919: 0.10000000149011612\n",
      "learned_steps 920: 0.0\n",
      "learned_steps 921: 0.0\n",
      "learned_steps 922: 0.0\n",
      "learned_steps 923: 0.0\n",
      "learned_steps 924: 0.0\n",
      "learned_steps 925: 0.0\n",
      "learned_steps 926: 0.0\n",
      "learned_steps 927: 0.0\n",
      "learned_steps 928: 0.0\n",
      "learned_steps 929: 0.0\n",
      "learned_steps 930: 0.0\n",
      "learned_steps 931: 0.0\n",
      "learned_steps 932: 0.0\n",
      "learned_steps 933: 0.0\n",
      "learned_steps 934: 0.0\n",
      "learned_steps 935: 0.0\n",
      "learned_steps 936: 0.0\n",
      "learned_steps 937: 0.0\n",
      "learned_steps 938: 0.0\n",
      "learned_steps 939: 0.0\n",
      "learned_steps 940: 0.0\n",
      "learned_steps 941: 0.0\n",
      "learned_steps 942: 0.0\n",
      "learned_steps 943: 0.0\n",
      "learned_steps 944: 0.0\n",
      "learned_steps 945: 0.0\n",
      "learned_steps 946: 0.0\n",
      "learned_steps 947: 0.0\n",
      "learned_steps 948: 0.0\n",
      "learned_steps 949: 0.0\n",
      "learned_steps 950: 0.0\n",
      "learned_steps 951: 0.0\n",
      "learned_steps 952: 0.0\n",
      "learned_steps 953: 0.0\n",
      "learned_steps 954: 0.0\n",
      "learned_steps 955: 0.0\n",
      "learned_steps 956: 0.0\n",
      "learned_steps 957: 0.0\n",
      "learned_steps 958: 0.0\n",
      "learned_steps 959: 0.0\n",
      "learned_steps 960: 0.0\n",
      "learned_steps 961: 0.0\n",
      "learned_steps 962: 0.0\n",
      "learned_steps 963: 0.0\n",
      "learned_steps 964: 0.0\n",
      "learned_steps 965: 0.0\n",
      "learned_steps 966: 0.0\n",
      "learned_steps 967: 0.10000000149011612\n",
      "learned_steps 968: 0.0\n",
      "learned_steps 969: 0.0\n",
      "learned_steps 970: 0.0\n",
      "learned_steps 971: 0.10000000149011612\n",
      "learned_steps 972: 0.0\n",
      "learned_steps 973: 0.0\n",
      "learned_steps 974: 0.0\n",
      "learned_steps 975: 0.0\n",
      "learned_steps 976: 0.0\n",
      "learned_steps 977: 0.10000000149011612\n",
      "learned_steps 978: 0.10000000149011612\n",
      "learned_steps 979: 0.0\n",
      "learned_steps 980: 0.0\n",
      "learned_steps 981: 0.0\n",
      "learned_steps 982: 0.0\n",
      "learned_steps 983: 0.0\n",
      "learned_steps 984: 0.0\n",
      "learned_steps 985: 0.0\n",
      "learned_steps 986: 0.0\n",
      "learned_steps 987: 0.0\n",
      "learned_steps 988: 0.0\n",
      "learned_steps 989: 0.0\n",
      "learned_steps 990: 0.0\n",
      "learned_steps 991: 0.0\n",
      "learned_steps 992: 0.0\n",
      "learned_steps 993: 0.0\n",
      "learned_steps 994: 0.0\n",
      "learned_steps 995: 0.0\n",
      "learned_steps 996: 0.0\n",
      "learned_steps 997: 0.0\n",
      "learned_steps 998: 0.0\n",
      "learned_steps 999: 0.10000000149011612\n",
      "learned_steps 1000: 0.0\n",
      "learned_steps 1001: 0.10000000149011612\n",
      "learned_steps 1002: 0.0\n",
      "learned_steps 1003: 0.0\n",
      "learned_steps 1004: 0.0\n",
      "learned_steps 1005: 0.0\n",
      "learned_steps 1006: 0.0\n",
      "learned_steps 1007: 0.0\n",
      "learned_steps 1008: 0.0\n",
      "learned_steps 1009: 0.0\n",
      "learned_steps 1010: 0.0\n",
      "learned_steps 1011: 0.0\n",
      "learned_steps 1012: 0.0\n",
      "learned_steps 1013: 0.0\n",
      "learned_steps 1014: 0.0\n",
      "learned_steps 1015: 0.0\n",
      "learned_steps 1016: 0.0\n",
      "learned_steps 1017: 0.0\n",
      "learned_steps 1018: 0.10000000149011612\n",
      "learned_steps 1019: 0.0\n",
      "learned_steps 1020: 0.10000000149011612\n",
      "learned_steps 1021: 0.0\n",
      "learned_steps 1022: 0.0\n",
      "learned_steps 1023: 0.0\n",
      "learned_steps 1024: 0.0\n",
      "learned_steps 1025: 0.0\n",
      "learned_steps 1026: 0.0\n",
      "learned_steps 1027: 0.10000000149011612\n",
      "learned_steps 1028: 0.0\n",
      "learned_steps 1029: 0.0\n",
      "learned_steps 1030: 0.0\n",
      "learned_steps 1031: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned_steps 1032: 0.0\n",
      "learned_steps 1033: 0.0\n",
      "learned_steps 1034: 0.0\n",
      "learned_steps 1035: 0.0\n",
      "learned_steps 1036: 0.0\n",
      "learned_steps 1037: 0.0\n",
      "learned_steps 1038: 0.0\n",
      "learned_steps 1039: 0.0\n",
      "learned_steps 1040: 0.0\n",
      "learned_steps 1041: 0.0\n",
      "learned_steps 1042: 0.0\n",
      "learned_steps 1043: 0.0\n",
      "learned_steps 1044: 0.0\n",
      "learned_steps 1045: 0.0\n",
      "learned_steps 1046: 0.0\n",
      "learned_steps 1047: 0.0\n",
      "learned_steps 1048: 0.0\n",
      "learned_steps 1049: 0.0\n",
      "learned_steps 1050: 0.0\n",
      "learned_steps 1051: 0.0\n",
      "learned_steps 1052: 0.0\n",
      "learned_steps 1053: 0.0\n",
      "learned_steps 1054: 0.0\n",
      "learned_steps 1055: 0.10000000149011612\n",
      "learned_steps 1056: 0.0\n",
      "learned_steps 1057: 0.0\n",
      "learned_steps 1058: 0.0\n",
      "learned_steps 1059: 0.0\n",
      "learned_steps 1060: 0.0\n",
      "learned_steps 1061: 0.0\n",
      "learned_steps 1062: 0.0\n",
      "learned_steps 1063: 0.0\n",
      "learned_steps 1064: 0.10000000149011612\n",
      "learned_steps 1065: 0.0\n",
      "learned_steps 1066: 0.09000000357627869\n",
      "learned_steps 1067: 0.0\n",
      "learned_steps 1068: 0.0\n",
      "learned_steps 1069: 0.0\n",
      "learned_steps 1070: 0.0\n",
      "learned_steps 1071: 0.0\n",
      "learned_steps 1072: 0.0\n",
      "learned_steps 1073: 0.0\n",
      "learned_steps 1074: 0.0\n",
      "learned_steps 1075: 0.0\n",
      "learned_steps 1076: 0.0\n",
      "learned_steps 1077: 0.0\n",
      "learned_steps 1078: 0.0\n",
      "learned_steps 1079: 0.0\n",
      "learned_steps 1080: 0.0\n",
      "learned_steps 1081: 0.0\n",
      "learned_steps 1082: 0.0\n",
      "learned_steps 1083: 0.0\n",
      "learned_steps 1084: 0.0\n",
      "learned_steps 1085: 0.0\n",
      "learned_steps 1086: 0.0\n",
      "learned_steps 1087: 0.0\n",
      "learned_steps 1088: 0.0\n",
      "learned_steps 1089: 0.0\n",
      "learned_steps 1090: 0.0\n",
      "learned_steps 1091: 0.0\n",
      "learned_steps 1092: 0.0\n",
      "learned_steps 1093: 0.0\n",
      "learned_steps 1094: 0.0\n",
      "learned_steps 1095: 0.0\n",
      "learned_steps 1096: 0.0\n",
      "learned_steps 1097: 0.0\n",
      "learned_steps 1098: 0.0\n",
      "learned_steps 1099: 0.0\n",
      "learned_steps 1100: 0.0\n",
      "learned_steps 1101: 0.0\n",
      "learned_steps 1102: 0.0\n",
      "learned_steps 1103: 0.0\n",
      "learned_steps 1104: 0.0\n",
      "learned_steps 1105: 0.0\n",
      "learned_steps 1106: 0.0\n",
      "learned_steps 1107: 0.0\n",
      "learned_steps 1108: 0.0\n",
      "learned_steps 1109: 0.0\n",
      "learned_steps 1110: 0.0\n",
      "learned_steps 1111: 0.0\n",
      "learned_steps 1112: 0.0\n",
      "learned_steps 1113: 0.0\n",
      "learned_steps 1114: 0.0\n",
      "learned_steps 1115: 0.0\n",
      "learned_steps 1116: 0.0\n",
      "learned_steps 1117: 0.0\n",
      "learned_steps 1118: 0.0\n",
      "learned_steps 1119: 0.0\n",
      "learned_steps 1120: 0.0\n",
      "learned_steps 1121: 0.0\n",
      "learned_steps 1122: 0.0\n",
      "learned_steps 1123: 0.0\n",
      "learned_steps 1124: 0.0\n",
      "learned_steps 1125: 0.0\n",
      "learned_steps 1126: 0.0\n",
      "learned_steps 1127: 0.0\n",
      "learned_steps 1128: 0.0\n",
      "learned_steps 1129: 0.0\n",
      "learned_steps 1130: 0.0\n",
      "learned_steps 1131: 0.0\n",
      "learned_steps 1132: 0.0\n",
      "learned_steps 1133: 0.0\n",
      "learned_steps 1134: 0.0\n",
      "learned_steps 1135: 0.0\n",
      "learned_steps 1136: 0.0\n",
      "learned_steps 1137: 0.0\n",
      "learned_steps 1138: 0.0\n",
      "learned_steps 1139: 0.0\n",
      "learned_steps 1140: 0.0\n",
      "learned_steps 1141: 0.0\n",
      "learned_steps 1142: 0.0\n",
      "learned_steps 1143: 0.0\n",
      "learned_steps 1144: 0.10000000149011612\n",
      "learned_steps 1145: 0.0\n",
      "learned_steps 1146: 0.0\n",
      "learned_steps 1147: 0.0\n",
      "learned_steps 1148: 0.0\n",
      "learned_steps 1149: 0.0\n",
      "learned_steps 1150: 0.0\n",
      "learned_steps 1151: 0.0\n",
      "learned_steps 1152: 0.0\n",
      "learned_steps 1153: 0.0\n",
      "learned_steps 1154: 0.0\n",
      "learned_steps 1155: 0.0\n",
      "learned_steps 1156: 0.0\n",
      "learned_steps 1157: 0.0\n",
      "learned_steps 1158: 0.0\n",
      "learned_steps 1159: 0.0\n",
      "learned_steps 1160: 0.0\n",
      "learned_steps 1161: 0.0\n",
      "learned_steps 1162: 0.0\n",
      "learned_steps 1163: 0.0\n",
      "learned_steps 1164: 0.0\n",
      "learned_steps 1165: 0.0\n",
      "learned_steps 1166: 0.0\n",
      "learned_steps 1167: 0.0\n",
      "learned_steps 1168: 0.0\n",
      "learned_steps 1169: 0.0\n",
      "learned_steps 1170: 0.0\n",
      "learned_steps 1171: 0.0\n",
      "learned_steps 1172: 0.0\n",
      "learned_steps 1173: 0.0\n",
      "learned_steps 1174: 0.0\n",
      "learned_steps 1175: 0.0\n",
      "learned_steps 1176: 0.0\n",
      "learned_steps 1177: 0.0\n",
      "learned_steps 1178: 0.0\n",
      "learned_steps 1179: 0.0\n",
      "learned_steps 1180: 0.0\n",
      "learned_steps 1181: 0.0\n",
      "learned_steps 1182: 0.0\n",
      "learned_steps 1183: 0.0\n",
      "learned_steps 1184: 0.0\n",
      "learned_steps 1185: 0.0\n",
      "learned_steps 1186: 0.0\n",
      "learned_steps 1187: 0.0\n",
      "learned_steps 1188: 0.0\n",
      "learned_steps 1189: 0.0\n",
      "learned_steps 1190: 0.0\n",
      "learned_steps 1191: 0.0\n",
      "learned_steps 1192: 0.0\n",
      "learned_steps 1193: 0.0\n",
      "learned_steps 1194: 0.0\n",
      "learned_steps 1195: 0.0\n",
      "learned_steps 1196: 0.10000000149011612\n",
      "learned_steps 1197: 0.0\n",
      "learned_steps 1198: 0.0\n",
      "learned_steps 1199: 0.0\n",
      "learned_steps 1200: 0.0\n",
      "learned_steps 1201: 0.0\n",
      "learned_steps 1202: 0.0\n",
      "learned_steps 1203: 0.0\n",
      "learned_steps 1204: 0.0\n",
      "learned_steps 1205: 0.0\n",
      "learned_steps 1206: 0.0\n",
      "learned_steps 1207: 0.0\n",
      "learned_steps 1208: 0.0\n",
      "learned_steps 1209: 0.0\n",
      "learned_steps 1210: 0.0\n",
      "learned_steps 1211: 0.0\n",
      "learned_steps 1212: 0.0\n",
      "learned_steps 1213: 0.0\n",
      "learned_steps 1214: 0.0\n",
      "learned_steps 1215: 0.0\n",
      "learned_steps 1216: 0.0\n",
      "learned_steps 1217: 0.0\n",
      "learned_steps 1218: 0.0\n",
      "learned_steps 1219: 0.0\n",
      "learned_steps 1220: 0.0\n",
      "learned_steps 1221: 0.0\n",
      "learned_steps 1222: 0.0\n",
      "learned_steps 1223: 0.0\n",
      "learned_steps 1224: 0.10000000149011612\n",
      "learned_steps 1225: 0.0\n",
      "learned_steps 1226: 0.0\n",
      "learned_steps 1227: 0.0\n",
      "learned_steps 1228: 0.0\n",
      "learned_steps 1229: 0.0\n",
      "learned_steps 1230: 0.0\n",
      "learned_steps 1231: 0.0\n",
      "learned_steps 1232: 0.0\n",
      "learned_steps 1233: 0.0\n",
      "learned_steps 1234: 0.0\n",
      "learned_steps 1235: 0.0\n",
      "learned_steps 1236: 0.0\n",
      "learned_steps 1237: 0.0\n",
      "learned_steps 1238: 0.0\n",
      "learned_steps 1239: 0.0\n",
      "learned_steps 1240: 0.0\n",
      "learned_steps 1241: 0.0\n",
      "learned_steps 1242: 0.0\n",
      "learned_steps 1243: 0.0\n",
      "learned_steps 1244: 0.0\n",
      "learned_steps 1245: 0.0\n",
      "learned_steps 1246: 0.0\n",
      "learned_steps 1247: 0.0\n",
      "learned_steps 1248: 0.0\n",
      "learned_steps 1249: 0.0\n",
      "learned_steps 1250: 0.0\n",
      "learned_steps 1251: 0.0\n",
      "learned_steps 1252: 0.0\n",
      "learned_steps 1253: 0.0\n",
      "learned_steps 1254: 0.0\n",
      "learned_steps 1255: 0.0\n",
      "learned_steps 1256: 0.0\n",
      "learned_steps 1257: 0.0\n",
      "learned_steps 1258: 0.0\n",
      "learned_steps 1259: 0.0\n",
      "learned_steps 1260: 0.0\n",
      "learned_steps 1261: 0.0\n",
      "learned_steps 1262: 0.0\n",
      "learned_steps 1263: 0.0\n",
      "learned_steps 1264: 0.0\n",
      "learned_steps 1265: 0.0\n",
      "learned_steps 1266: 0.0\n",
      "learned_steps 1267: 0.0\n",
      "learned_steps 1268: 0.0\n",
      "learned_steps 1269: 0.0\n",
      "learned_steps 1270: 0.0\n",
      "learned_steps 1271: 0.0\n",
      "learned_steps 1272: 0.0\n",
      "learned_steps 1273: 0.0\n",
      "learned_steps 1274: 0.0\n",
      "learned_steps 1275: 0.0\n",
      "learned_steps 1276: 0.0\n",
      "learned_steps 1277: 0.0\n",
      "learned_steps 1278: 0.0\n",
      "learned_steps 1279: 0.0\n",
      "learned_steps 1280: 0.0\n",
      "learned_steps 1281: 0.0\n",
      "learned_steps 1282: 0.0\n",
      "learned_steps 1283: 0.0\n",
      "learned_steps 1284: 0.0\n",
      "learned_steps 1285: 0.0\n",
      "learned_steps 1286: 0.0\n",
      "learned_steps 1287: 0.10000000149011612\n",
      "learned_steps 1288: 0.0\n",
      "learned_steps 1289: 0.0\n",
      "learned_steps 1290: 0.0\n",
      "learned_steps 1291: 0.0\n",
      "learned_steps 1292: 0.0\n",
      "learned_steps 1293: 0.0\n",
      "learned_steps 1294: 0.0\n",
      "learned_steps 1295: 0.0\n",
      "learned_steps 1296: 0.0\n",
      "learned_steps 1297: 0.0\n",
      "learned_steps 1298: 0.0\n",
      "learned_steps 1299: 0.0\n",
      "learned_steps 1300: 0.0\n",
      "learned_steps 1301: 0.0\n",
      "learned_steps 1302: 0.0\n",
      "learned_steps 1303: 0.0\n",
      "learned_steps 1304: 0.0\n",
      "learned_steps 1305: 0.0\n",
      "learned_steps 1306: 0.0\n",
      "learned_steps 1307: 0.0\n",
      "learned_steps 1308: 0.0\n",
      "learned_steps 1309: 0.0\n",
      "learned_steps 1310: 0.0\n",
      "learned_steps 1311: 0.0\n",
      "learned_steps 1312: 0.0\n",
      "learned_steps 1313: 0.0\n",
      "learned_steps 1314: 0.0\n",
      "learned_steps 1315: 0.0\n",
      "learned_steps 1316: 0.0\n",
      "learned_steps 1317: 0.0\n",
      "learned_steps 1318: 0.0\n",
      "learned_steps 1319: 0.0\n",
      "learned_steps 1320: 0.0\n",
      "learned_steps 1321: 0.10000000149011612\n",
      "learned_steps 1322: 0.0\n",
      "learned_steps 1323: 0.0\n",
      "learned_steps 1324: 0.0\n",
      "learned_steps 1325: 0.0\n",
      "learned_steps 1326: 0.0\n",
      "learned_steps 1327: 0.0\n",
      "learned_steps 1328: 0.0\n",
      "learned_steps 1329: 0.0\n",
      "learned_steps 1330: 0.0\n",
      "learned_steps 1331: 0.0\n",
      "learned_steps 1332: 0.0\n",
      "learned_steps 1333: 0.0\n",
      "learned_steps 1334: 0.0\n",
      "learned_steps 1335: 0.0\n",
      "learned_steps 1336: 0.0\n",
      "learned_steps 1337: 0.0\n",
      "learned_steps 1338: 0.0\n",
      "learned_steps 1339: 0.0\n",
      "learned_steps 1340: 0.0\n",
      "learned_steps 1341: 0.0\n",
      "learned_steps 1342: 0.0\n",
      "learned_steps 1343: 0.0\n",
      "learned_steps 1344: 0.0\n",
      "learned_steps 1345: 0.0\n",
      "learned_steps 1346: 0.0\n",
      "learned_steps 1347: 0.0\n",
      "learned_steps 1348: 0.0\n",
      "learned_steps 1349: 0.0\n",
      "learned_steps 1350: 0.0\n",
      "learned_steps 1351: 0.0\n",
      "learned_steps 1352: 0.0\n",
      "learned_steps 1353: 0.10000000149011612\n",
      "learned_steps 1354: 0.0\n",
      "learned_steps 1355: 0.0\n",
      "learned_steps 1356: 0.10000000149011612\n",
      "learned_steps 1357: 0.10000000149011612\n",
      "learned_steps 1358: 0.0\n",
      "learned_steps 1359: 0.0\n",
      "learned_steps 1360: 0.0\n",
      "learned_steps 1361: 0.0\n",
      "learned_steps 1362: 0.0\n",
      "learned_steps 1363: 0.0\n",
      "learned_steps 1364: 0.0\n",
      "learned_steps 1365: 0.0\n",
      "learned_steps 1366: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned_steps 1367: 0.0\n",
      "learned_steps 1368: 0.0\n",
      "learned_steps 1369: 0.0\n",
      "learned_steps 1370: 0.0\n",
      "learned_steps 1371: 0.0\n",
      "learned_steps 1372: 0.0\n",
      "learned_steps 1373: 0.0\n",
      "learned_steps 1374: 0.0\n",
      "learned_steps 1375: 0.10000000149011612\n",
      "learned_steps 1376: 0.0\n",
      "learned_steps 1377: 0.0\n",
      "learned_steps 1378: 0.10000000149011612\n",
      "learned_steps 1379: 0.0\n",
      "learned_steps 1380: 0.0\n",
      "learned_steps 1381: 0.0\n",
      "learned_steps 1382: 0.10000000149011612\n",
      "learned_steps 1383: 0.0\n",
      "learned_steps 1384: 0.0\n",
      "learned_steps 1385: 0.0\n",
      "learned_steps 1386: 0.0\n",
      "learned_steps 1387: 0.0\n",
      "learned_steps 1388: 0.0\n",
      "learned_steps 1389: 0.0\n",
      "learned_steps 1390: 0.0\n",
      "learned_steps 1391: 0.0\n",
      "learned_steps 1392: 0.0\n",
      "learned_steps 1393: 0.0\n",
      "learned_steps 1394: 0.0\n",
      "learned_steps 1395: 0.0\n",
      "learned_steps 1396: 0.0\n",
      "learned_steps 1397: 0.0\n",
      "learned_steps 1398: 0.0\n",
      "learned_steps 1399: 0.0\n",
      "learned_steps 1400: 0.0\n",
      "learned_steps 1401: 0.0\n",
      "learned_steps 1402: 0.0\n",
      "learned_steps 1403: 0.0\n",
      "learned_steps 1404: 0.0\n",
      "learned_steps 1405: 0.0\n",
      "learned_steps 1406: 0.0\n",
      "learned_steps 1407: 0.0\n",
      "learned_steps 1408: 0.0\n",
      "learned_steps 1409: 0.0\n",
      "learned_steps 1410: 0.0\n",
      "learned_steps 1411: 0.0\n",
      "learned_steps 1412: 0.0\n",
      "learned_steps 1413: 0.0\n",
      "learned_steps 1414: 0.0\n",
      "learned_steps 1415: 0.0\n",
      "learned_steps 1416: 0.0\n",
      "learned_steps 1417: 0.0\n",
      "learned_steps 1418: 0.0\n",
      "learned_steps 1419: 0.0\n",
      "learned_steps 1420: 0.0\n",
      "learned_steps 1421: 0.0\n",
      "learned_steps 1422: 0.0\n",
      "learned_steps 1423: 0.0\n",
      "learned_steps 1424: 0.0\n",
      "learned_steps 1425: 0.0\n",
      "learned_steps 1426: 0.0\n",
      "learned_steps 1427: 0.0\n",
      "learned_steps 1428: 0.0\n",
      "learned_steps 1429: 0.0\n",
      "learned_steps 1430: 0.0\n",
      "learned_steps 1431: 0.0\n",
      "learned_steps 1432: 0.0\n",
      "learned_steps 1433: 0.0\n",
      "learned_steps 1434: 0.0\n",
      "learned_steps 1435: 0.0\n",
      "learned_steps 1436: 0.0\n",
      "learned_steps 1437: 0.0\n",
      "learned_steps 1438: 0.0\n",
      "learned_steps 1439: 0.0\n",
      "learned_steps 1440: 0.0\n",
      "learned_steps 1441: 0.0\n",
      "learned_steps 1442: 0.0\n",
      "learned_steps 1443: 0.0\n",
      "learned_steps 1444: 0.0\n",
      "learned_steps 1445: 0.0\n",
      "learned_steps 1446: 0.0\n",
      "learned_steps 1447: 0.0\n",
      "learned_steps 1448: 0.0\n",
      "learned_steps 1449: 0.0\n",
      "learned_steps 1450: 0.0\n",
      "learned_steps 1451: 0.0\n",
      "learned_steps 1452: 0.0\n",
      "learned_steps 1453: 0.0\n",
      "learned_steps 1454: 0.0\n",
      "learned_steps 1455: 0.0\n",
      "learned_steps 1456: 0.0\n",
      "learned_steps 1457: 0.0\n",
      "learned_steps 1458: 0.0\n",
      "learned_steps 1459: 0.0\n",
      "learned_steps 1460: 0.0\n",
      "learned_steps 1461: 0.0\n",
      "learned_steps 1462: 0.0\n",
      "learned_steps 1463: 0.0\n",
      "learned_steps 1464: 0.0\n",
      "learned_steps 1465: 0.0\n",
      "learned_steps 1466: 0.0\n",
      "learned_steps 1467: 0.0\n",
      "learned_steps 1468: 0.0\n",
      "learned_steps 1469: 0.0\n",
      "learned_steps 1470: 0.0\n",
      "learned_steps 1471: 0.0\n",
      "learned_steps 1472: 0.0\n",
      "learned_steps 1473: 0.0\n",
      "learned_steps 1474: 0.0\n",
      "learned_steps 1475: 0.0\n",
      "learned_steps 1476: 0.0\n",
      "learned_steps 1477: 0.0\n",
      "learned_steps 1478: 0.0\n",
      "learned_steps 1479: 0.0\n",
      "learned_steps 1480: 0.0\n",
      "learned_steps 1481: 0.0\n",
      "learned_steps 1482: 0.0\n",
      "learned_steps 1483: 0.0\n",
      "learned_steps 1484: 0.10000000149011612\n",
      "learned_steps 1485: 0.0\n",
      "learned_steps 1486: 0.0\n",
      "learned_steps 1487: 0.0\n",
      "learned_steps 1488: 0.10000000149011612\n",
      "learned_steps 1489: 0.0\n",
      "learned_steps 1490: 0.0\n",
      "learned_steps 1491: 0.0\n",
      "learned_steps 1492: 0.0\n",
      "learned_steps 1493: 0.0\n",
      "learned_steps 1494: 0.0\n",
      "learned_steps 1495: 0.0\n",
      "learned_steps 1496: 0.0\n",
      "learned_steps 1497: 0.0\n",
      "learned_steps 1498: 0.0\n",
      "learned_steps 1499: 0.0\n",
      "learned_steps 1500: 0.0\n",
      "learned_steps 1501: 0.0\n",
      "learned_steps 1502: 0.0\n",
      "learned_steps 1503: 0.0\n",
      "learned_steps 1504: 0.0\n",
      "learned_steps 1505: 0.0\n",
      "learned_steps 1506: 0.0\n",
      "learned_steps 1507: 0.0\n",
      "learned_steps 1508: 0.0\n",
      "learned_steps 1509: 0.0\n",
      "learned_steps 1510: 0.0\n",
      "learned_steps 1511: 0.0\n",
      "learned_steps 1512: 0.0\n",
      "learned_steps 1513: 0.0\n",
      "learned_steps 1514: 0.0\n",
      "learned_steps 1515: 0.10000000149011612\n",
      "learned_steps 1516: 0.0\n",
      "learned_steps 1517: 0.0\n",
      "learned_steps 1518: 0.0\n",
      "learned_steps 1519: 0.0\n",
      "learned_steps 1520: 0.0\n",
      "learned_steps 1521: 0.0\n",
      "learned_steps 1522: 0.0\n",
      "learned_steps 1523: 0.0\n",
      "learned_steps 1524: 0.0\n",
      "learned_steps 1525: 0.0\n",
      "learned_steps 1526: 0.10000000149011612\n",
      "learned_steps 1527: 0.0\n",
      "learned_steps 1528: 0.0\n",
      "learned_steps 1529: 0.0\n",
      "learned_steps 1530: 0.0\n",
      "learned_steps 1531: 0.0\n",
      "learned_steps 1532: 0.0\n",
      "learned_steps 1533: 0.0\n",
      "learned_steps 1534: 0.0\n",
      "learned_steps 1535: 0.10000000149011612\n",
      "learned_steps 1536: 0.0\n",
      "learned_steps 1537: 0.0\n",
      "learned_steps 1538: 0.0\n",
      "learned_steps 1539: 0.0\n",
      "learned_steps 1540: 0.0\n",
      "learned_steps 1541: 0.0\n",
      "learned_steps 1542: 0.0\n",
      "learned_steps 1543: 0.0\n",
      "learned_steps 1544: 0.0\n",
      "learned_steps 1545: 0.0\n",
      "learned_steps 1546: 0.0\n",
      "learned_steps 1547: 0.10000000149011612\n",
      "learned_steps 1548: 0.10000000149011612\n",
      "learned_steps 1549: 0.0\n",
      "learned_steps 1550: 0.0\n",
      "learned_steps 1551: 0.0\n",
      "learned_steps 1552: 0.0\n",
      "learned_steps 1553: 0.0\n",
      "learned_steps 1554: 0.0\n",
      "learned_steps 1555: 0.0\n",
      "learned_steps 1556: 0.0\n",
      "learned_steps 1557: 0.0\n",
      "learned_steps 1558: 0.0\n",
      "learned_steps 1559: 0.0\n",
      "learned_steps 1560: 0.0\n",
      "learned_steps 1561: 0.0\n",
      "learned_steps 1562: 0.0\n",
      "learned_steps 1563: 0.0\n",
      "learned_steps 1564: 0.0\n",
      "learned_steps 1565: 0.0\n",
      "learned_steps 1566: 0.0\n",
      "learned_steps 1567: 0.0\n",
      "learned_steps 1568: 0.10000000149011612\n",
      "learned_steps 1569: 0.0\n",
      "learned_steps 1570: 0.0\n",
      "learned_steps 1571: 0.0\n",
      "learned_steps 1572: 0.0\n",
      "learned_steps 1573: 0.0\n",
      "learned_steps 1574: 0.0\n",
      "learned_steps 1575: 0.0\n",
      "learned_steps 1576: 0.0\n",
      "learned_steps 1577: 0.0\n",
      "learned_steps 1578: 0.0\n",
      "learned_steps 1579: 0.0\n",
      "learned_steps 1580: 0.0\n",
      "learned_steps 1581: 0.0\n",
      "learned_steps 1582: 0.0\n",
      "learned_steps 1583: 0.0\n",
      "learned_steps 1584: 0.0\n",
      "learned_steps 1585: 0.0\n",
      "learned_steps 1586: 0.0\n",
      "learned_steps 1587: 0.0\n",
      "learned_steps 1588: 0.0\n",
      "learned_steps 1589: 0.0\n",
      "learned_steps 1590: 0.0\n",
      "learned_steps 1591: 0.0\n",
      "learned_steps 1592: 0.0\n",
      "learned_steps 1593: 0.0\n",
      "learned_steps 1594: 0.0\n",
      "learned_steps 1595: 0.0\n",
      "learned_steps 1596: 0.0\n",
      "learned_steps 1597: 0.0\n",
      "learned_steps 1598: 0.0\n",
      "learned_steps 1599: 0.0\n",
      "learned_steps 1600: 0.0\n",
      "learned_steps 1601: 0.0\n",
      "learned_steps 1602: 0.0\n",
      "learned_steps 1603: 0.0\n",
      "learned_steps 1604: 0.0\n",
      "learned_steps 1605: 0.0\n",
      "learned_steps 1606: 0.0\n",
      "learned_steps 1607: 0.0\n",
      "learned_steps 1608: 0.10000000149011612\n",
      "learned_steps 1609: 0.0\n",
      "learned_steps 1610: 0.0\n",
      "learned_steps 1611: 0.0\n",
      "learned_steps 1612: 0.0\n",
      "learned_steps 1613: 0.0\n",
      "learned_steps 1614: 0.0\n",
      "learned_steps 1615: 0.0\n",
      "learned_steps 1616: 0.0\n",
      "learned_steps 1617: 0.0\n",
      "learned_steps 1618: 0.0\n",
      "learned_steps 1619: 0.0\n",
      "learned_steps 1620: 0.0\n",
      "learned_steps 1621: 0.0\n",
      "learned_steps 1622: 0.0\n",
      "learned_steps 1623: 0.0\n",
      "learned_steps 1624: 0.0\n",
      "learned_steps 1625: 0.0\n",
      "learned_steps 1626: 0.0\n",
      "learned_steps 1627: 0.0\n",
      "learned_steps 1628: 0.0\n",
      "learned_steps 1629: 0.0\n",
      "learned_steps 1630: 0.0\n",
      "learned_steps 1631: 0.10000000149011612\n",
      "learned_steps 1632: 0.0\n",
      "learned_steps 1633: 0.0\n",
      "learned_steps 1634: 0.0\n",
      "learned_steps 1635: 0.0\n",
      "learned_steps 1636: 0.0\n",
      "learned_steps 1637: 0.0\n",
      "learned_steps 1638: 0.0\n",
      "learned_steps 1639: 0.0\n",
      "learned_steps 1640: 0.0\n",
      "learned_steps 1641: 0.0\n",
      "learned_steps 1642: 0.0\n",
      "learned_steps 1643: 0.0\n",
      "learned_steps 1644: 0.0\n",
      "learned_steps 1645: 0.0\n",
      "learned_steps 1646: 0.0\n",
      "learned_steps 1647: 0.0\n",
      "learned_steps 1648: 0.0\n",
      "learned_steps 1649: 0.0\n",
      "learned_steps 1650: 0.0\n",
      "learned_steps 1651: 0.0\n",
      "learned_steps 1652: 0.0\n",
      "learned_steps 1653: 0.10000000149011612\n",
      "learned_steps 1654: 0.0\n",
      "learned_steps 1655: 0.0\n",
      "learned_steps 1656: 0.0\n",
      "learned_steps 1657: 0.0\n",
      "learned_steps 1658: 0.0\n",
      "learned_steps 1659: 0.0\n",
      "learned_steps 1660: 0.0\n",
      "learned_steps 1661: 0.0\n",
      "learned_steps 1662: 0.0\n",
      "learned_steps 1663: 0.0\n",
      "learned_steps 1664: 0.0\n",
      "learned_steps 1665: 0.0\n",
      "learned_steps 1666: 0.0\n",
      "learned_steps 1667: 0.0\n",
      "learned_steps 1668: 0.0\n",
      "learned_steps 1669: 0.0\n",
      "learned_steps 1670: 0.0\n",
      "learned_steps 1671: 0.0\n",
      "learned_steps 1672: 0.0\n",
      "learned_steps 1673: 0.0\n",
      "learned_steps 1674: 0.0\n",
      "learned_steps 1675: 0.0\n",
      "learned_steps 1676: 0.0\n",
      "learned_steps 1677: 0.0\n",
      "learned_steps 1678: 0.0\n",
      "learned_steps 1679: 0.0\n",
      "learned_steps 1680: 0.0\n",
      "learned_steps 1681: 0.0\n",
      "learned_steps 1682: 0.0\n",
      "learned_steps 1683: 0.0\n",
      "learned_steps 1684: 0.0\n",
      "learned_steps 1685: 0.0\n",
      "learned_steps 1686: 0.0\n",
      "learned_steps 1687: 0.0\n",
      "learned_steps 1688: 0.0\n",
      "learned_steps 1689: 0.0\n",
      "learned_steps 1690: 0.0\n",
      "learned_steps 1691: 0.0\n",
      "learned_steps 1692: 0.0\n",
      "learned_steps 1693: 0.0\n",
      "learned_steps 1694: 0.0\n",
      "learned_steps 1695: 0.0\n",
      "learned_steps 1696: 0.0\n",
      "learned_steps 1697: 0.0\n",
      "learned_steps 1698: 0.0\n",
      "learned_steps 1699: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned_steps 1700: 0.0\n",
      "learned_steps 1701: 0.0\n",
      "learned_steps 1702: 0.0\n",
      "learned_steps 1703: 0.0\n",
      "learned_steps 1704: 0.0\n",
      "learned_steps 1705: 0.0\n",
      "learned_steps 1706: 0.0\n",
      "learned_steps 1707: 0.0\n",
      "learned_steps 1708: 0.0\n",
      "learned_steps 1709: 0.0\n",
      "learned_steps 1710: 0.0\n",
      "learned_steps 1711: 0.0\n",
      "learned_steps 1712: 0.0\n",
      "learned_steps 1713: 0.0\n",
      "learned_steps 1714: 0.0\n",
      "learned_steps 1715: 0.10000000149011612\n",
      "learned_steps 1716: 0.0\n",
      "learned_steps 1717: 0.0\n",
      "learned_steps 1718: 0.10000000149011612\n",
      "learned_steps 1719: 0.0\n",
      "learned_steps 1720: 0.0\n",
      "learned_steps 1721: 0.0\n",
      "learned_steps 1722: 0.0\n",
      "learned_steps 1723: 0.0\n",
      "learned_steps 1724: 0.0\n",
      "learned_steps 1725: 0.0\n",
      "learned_steps 1726: 0.0\n",
      "learned_steps 1727: 0.0\n",
      "learned_steps 1728: 0.0\n",
      "learned_steps 1729: 0.0\n",
      "learned_steps 1730: 0.0\n",
      "learned_steps 1731: 0.0\n",
      "learned_steps 1732: 0.0\n",
      "learned_steps 1733: 0.0\n",
      "learned_steps 1734: 0.0\n",
      "learned_steps 1735: 0.0\n",
      "learned_steps 1736: 0.0\n",
      "learned_steps 1737: 0.0\n",
      "learned_steps 1738: 0.0\n",
      "learned_steps 1739: 0.0\n",
      "learned_steps 1740: 0.0\n",
      "learned_steps 1741: 0.0\n",
      "learned_steps 1742: 0.0\n",
      "learned_steps 1743: 0.0\n",
      "learned_steps 1744: 0.0\n",
      "learned_steps 1745: 0.0\n",
      "learned_steps 1746: 0.0\n",
      "learned_steps 1747: 0.0\n",
      "learned_steps 1748: 0.0\n",
      "learned_steps 1749: 0.0\n",
      "learned_steps 1750: 0.0\n",
      "learned_steps 1751: 0.0\n",
      "learned_steps 1752: 0.10000000149011612\n",
      "learned_steps 1753: 0.0\n",
      "learned_steps 1754: 0.0\n",
      "learned_steps 1755: 0.0\n",
      "learned_steps 1756: 0.0\n",
      "learned_steps 1757: 0.0\n",
      "learned_steps 1758: 0.0\n",
      "learned_steps 1759: 0.10000000149011612\n",
      "learned_steps 1760: 0.0\n",
      "learned_steps 1761: 0.0\n",
      "learned_steps 1762: 0.0\n",
      "learned_steps 1763: 0.0\n",
      "learned_steps 1764: 0.0\n",
      "learned_steps 1765: 0.0\n",
      "learned_steps 1766: 0.0\n",
      "learned_steps 1767: 0.0\n",
      "learned_steps 1768: 0.0\n",
      "learned_steps 1769: 0.0\n",
      "learned_steps 1770: 0.0\n",
      "learned_steps 1771: 0.0\n",
      "learned_steps 1772: 0.0\n",
      "learned_steps 1773: 0.0\n",
      "learned_steps 1774: 0.0\n",
      "learned_steps 1775: 0.0\n",
      "learned_steps 1776: 0.0\n",
      "learned_steps 1777: 0.0\n",
      "learned_steps 1778: 0.0\n",
      "learned_steps 1779: 0.0\n",
      "learned_steps 1780: 0.0\n",
      "learned_steps 1781: 0.0\n",
      "learned_steps 1782: 0.0\n",
      "learned_steps 1783: 0.0\n",
      "learned_steps 1784: 0.0\n",
      "learned_steps 1785: 0.10000000149011612\n",
      "learned_steps 1786: 0.0\n",
      "learned_steps 1787: 0.0\n",
      "learned_steps 1788: 0.0\n",
      "learned_steps 1789: 0.0\n",
      "learned_steps 1790: 0.0\n",
      "learned_steps 1791: 0.0\n",
      "learned_steps 1792: 0.0\n",
      "learned_steps 1793: 0.0\n",
      "learned_steps 1794: 0.0\n",
      "learned_steps 1795: 0.0\n",
      "learned_steps 1796: 0.0\n",
      "learned_steps 1797: 0.0\n",
      "learned_steps 1798: 0.0\n",
      "learned_steps 1799: 0.0\n",
      "learned_steps 1800: 0.0\n",
      "learned_steps 1801: 0.0\n",
      "learned_steps 1802: 0.0\n",
      "learned_steps 1803: 0.0\n",
      "learned_steps 1804: 0.0\n",
      "learned_steps 1805: 0.0\n",
      "learned_steps 1806: 0.0\n",
      "learned_steps 1807: 0.0\n",
      "learned_steps 1808: 0.0\n",
      "learned_steps 1809: 0.0\n",
      "learned_steps 1810: 0.0\n",
      "learned_steps 1811: 0.0\n",
      "learned_steps 1812: 0.0\n",
      "learned_steps 1813: 0.0\n",
      "learned_steps 1814: 0.10000000149011612\n",
      "learned_steps 1815: 0.0\n",
      "learned_steps 1816: 0.0\n",
      "learned_steps 1817: 0.0\n",
      "learned_steps 1818: 0.0\n",
      "learned_steps 1819: 0.10000000149011612\n",
      "learned_steps 1820: 0.10000000149011612\n",
      "learned_steps 1821: 0.0\n",
      "learned_steps 1822: 0.0\n",
      "learned_steps 1823: 0.0\n",
      "learned_steps 1824: 0.10000000149011612\n",
      "learned_steps 1825: 0.0\n",
      "learned_steps 1826: 0.0\n",
      "learned_steps 1827: 0.0\n",
      "learned_steps 1828: 0.0\n",
      "learned_steps 1829: 0.0\n",
      "learned_steps 1830: 0.0\n",
      "learned_steps 1831: 0.0\n",
      "learned_steps 1832: 0.0\n",
      "learned_steps 1833: 0.0\n",
      "learned_steps 1834: 0.0\n",
      "learned_steps 1835: 0.0\n",
      "learned_steps 1836: 0.0\n",
      "learned_steps 1837: 0.0\n",
      "learned_steps 1838: 0.0\n",
      "learned_steps 1839: 0.0\n",
      "learned_steps 1840: 0.0\n",
      "learned_steps 1841: 0.0\n",
      "learned_steps 1842: 0.0\n",
      "learned_steps 1843: 0.0\n",
      "learned_steps 1844: 0.0\n",
      "learned_steps 1845: 0.0\n",
      "learned_steps 1846: 0.0\n",
      "learned_steps 1847: 0.0\n",
      "learned_steps 1848: 0.0\n",
      "learned_steps 1849: 0.0\n",
      "learned_steps 1850: 0.0\n",
      "learned_steps 1851: 0.0\n",
      "learned_steps 1852: 0.0\n",
      "learned_steps 1853: 0.0\n",
      "learned_steps 1854: 0.0\n",
      "learned_steps 1855: 0.0\n",
      "learned_steps 1856: 0.0\n",
      "learned_steps 1857: 0.0\n",
      "learned_steps 1858: 0.0\n",
      "learned_steps 1859: 0.0\n",
      "learned_steps 1860: 0.0\n",
      "learned_steps 1861: 0.0\n",
      "learned_steps 1862: 0.0\n",
      "learned_steps 1863: 0.0\n",
      "learned_steps 1864: 0.0\n",
      "learned_steps 1865: 0.0\n",
      "learned_steps 1866: 0.0\n",
      "learned_steps 1867: 0.0\n",
      "learned_steps 1868: 0.0\n",
      "learned_steps 1869: 0.0\n",
      "learned_steps 1870: 0.0\n",
      "learned_steps 1871: 0.0\n",
      "learned_steps 1872: 0.0\n",
      "learned_steps 1873: 0.0\n",
      "learned_steps 1874: 0.0\n",
      "learned_steps 1875: 0.0\n",
      "learned_steps 1876: 0.0\n",
      "learned_steps 1877: 0.0\n",
      "learned_steps 1878: 0.0\n",
      "learned_steps 1879: 0.0\n",
      "learned_steps 1880: 0.0\n",
      "learned_steps 1881: 0.10000000149011612\n",
      "learned_steps 1882: 0.0\n",
      "learned_steps 1883: 0.0\n",
      "learned_steps 1884: 0.0\n",
      "learned_steps 1885: 0.0\n",
      "learned_steps 1886: 0.0\n",
      "learned_steps 1887: 0.10000000149011612\n",
      "learned_steps 1888: 0.0\n",
      "learned_steps 1889: 0.0\n",
      "learned_steps 1890: 0.0\n",
      "learned_steps 1891: 0.0\n",
      "learned_steps 1892: 0.0\n",
      "learned_steps 1893: 0.0\n",
      "learned_steps 1894: 0.0\n",
      "learned_steps 1895: 0.0\n",
      "learned_steps 1896: 0.0\n",
      "learned_steps 1897: 0.0\n",
      "learned_steps 1898: 0.0\n",
      "learned_steps 1899: 0.0\n",
      "learned_steps 1900: 0.0\n",
      "learned_steps 1901: 0.0\n",
      "learned_steps 1902: 0.0\n",
      "learned_steps 1903: 0.0\n",
      "learned_steps 1904: 0.0\n",
      "learned_steps 1905: 0.0\n",
      "learned_steps 1906: 0.0\n",
      "learned_steps 1907: 0.0\n",
      "learned_steps 1908: 0.0\n",
      "learned_steps 1909: 0.0\n",
      "learned_steps 1910: 0.0\n",
      "learned_steps 1911: 0.0\n",
      "learned_steps 1912: 0.0\n",
      "learned_steps 1913: 0.0\n",
      "learned_steps 1914: 0.0\n",
      "learned_steps 1915: 0.0\n",
      "learned_steps 1916: 0.0\n",
      "learned_steps 1917: 0.0\n",
      "learned_steps 1918: 0.0\n",
      "learned_steps 1919: 0.0\n",
      "learned_steps 1920: 0.0\n",
      "learned_steps 1921: 0.0\n",
      "learned_steps 1922: 0.0\n",
      "learned_steps 1923: 0.0\n",
      "learned_steps 1924: 0.10000000149011612\n",
      "learned_steps 1925: 0.0\n",
      "learned_steps 1926: 0.0\n",
      "learned_steps 1927: 0.0\n",
      "learned_steps 1928: 0.0\n",
      "learned_steps 1929: 0.0\n",
      "learned_steps 1930: 0.0\n",
      "learned_steps 1931: 0.0\n",
      "learned_steps 1932: 0.0\n",
      "learned_steps 1933: 0.0\n",
      "learned_steps 1934: 0.0\n",
      "learned_steps 1935: 0.0\n",
      "learned_steps 1936: 0.0\n",
      "learned_steps 1937: 0.0\n",
      "learned_steps 1938: 0.10000000149011612\n",
      "learned_steps 1939: 0.0\n",
      "learned_steps 1940: 0.0\n",
      "learned_steps 1941: 0.0\n",
      "learned_steps 1942: 0.0\n",
      "learned_steps 1943: 0.0\n",
      "learned_steps 1944: 0.0\n",
      "learned_steps 1945: 0.0\n",
      "learned_steps 1946: 0.0\n",
      "learned_steps 1947: 0.0\n",
      "learned_steps 1948: 0.0\n",
      "learned_steps 1949: 0.0\n",
      "learned_steps 1950: 0.10000000149011612\n",
      "learned_steps 1951: 0.0\n",
      "learned_steps 1952: 0.0\n",
      "learned_steps 1953: 0.0\n",
      "learned_steps 1954: 0.0\n",
      "learned_steps 1955: 0.0\n",
      "learned_steps 1956: 0.0\n",
      "learned_steps 1957: 0.0\n",
      "learned_steps 1958: 0.0\n",
      "learned_steps 1959: 0.0\n",
      "learned_steps 1960: 0.0\n",
      "learned_steps 1961: 0.0\n",
      "learned_steps 1962: 0.0\n",
      "learned_steps 1963: 0.0\n",
      "learned_steps 1964: 0.0\n",
      "learned_steps 1965: 0.0\n",
      "learned_steps 1966: 0.0\n",
      "learned_steps 1967: 0.0\n",
      "learned_steps 1968: 0.0\n",
      "learned_steps 1969: 0.0\n",
      "learned_steps 1970: 0.0\n",
      "learned_steps 1971: 0.0\n",
      "learned_steps 1972: 0.0\n",
      "learned_steps 1973: 0.0\n",
      "learned_steps 1974: 0.0\n",
      "learned_steps 1975: 0.0\n",
      "learned_steps 1976: 0.0\n",
      "learned_steps 1977: 0.0\n",
      "learned_steps 1978: 0.0\n",
      "learned_steps 1979: 0.10000000149011612\n",
      "learned_steps 1980: 0.0\n",
      "learned_steps 1981: 0.0\n",
      "learned_steps 1982: 0.0\n",
      "learned_steps 1983: 0.0\n",
      "learned_steps 1984: 0.0\n",
      "learned_steps 1985: 0.0\n",
      "learned_steps 1986: 0.0\n",
      "learned_steps 1987: 0.0\n",
      "learned_steps 1988: 0.0\n",
      "learned_steps 1989: 0.0\n",
      "learned_steps 1990: 0.0\n",
      "learned_steps 1991: 0.0\n",
      "learned_steps 1992: 0.0\n",
      "learned_steps 1993: 0.0\n",
      "learned_steps 1994: 0.0\n",
      "learned_steps 1995: 0.10000000149011612\n",
      "learned_steps 1996: 0.0\n",
      "learned_steps 1997: 0.0\n",
      "learned_steps 1998: 0.0\n",
      "learned_steps 1999: 0.0\n",
      "learned_steps 2000: 0.0\n",
      "learned_steps 2001: 0.0\n",
      "learned_steps 2002: 0.0\n",
      "learned_steps 2003: 0.0\n",
      "learned_steps 2004: 0.0\n",
      "learned_steps 2005: 0.0\n",
      "learned_steps 2006: 0.0\n",
      "learned_steps 2007: 0.0\n",
      "learned_steps 2008: 0.0\n",
      "learned_steps 2009: 0.0\n",
      "learned_steps 2010: 0.0\n",
      "learned_steps 2011: 0.0\n",
      "learned_steps 2012: 0.0\n",
      "learned_steps 2013: 0.0\n",
      "learned_steps 2014: 0.0\n",
      "learned_steps 2015: 0.0\n",
      "learned_steps 2016: 0.0\n",
      "learned_steps 2017: 0.0\n",
      "learned_steps 2018: 0.0\n",
      "learned_steps 2019: 0.0\n",
      "learned_steps 2020: 0.0\n",
      "learned_steps 2021: 0.0\n",
      "learned_steps 2022: 0.0\n",
      "learned_steps 2023: 0.0\n",
      "learned_steps 2024: 0.0\n",
      "learned_steps 2025: 0.0\n",
      "learned_steps 2026: 0.0\n",
      "learned_steps 2027: 0.0\n",
      "learned_steps 2028: 0.0\n",
      "learned_steps 2029: 0.0\n",
      "learned_steps 2030: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned_steps 2031: 0.0\n",
      "learned_steps 2032: 0.0\n",
      "learned_steps 2033: 0.0\n",
      "learned_steps 2034: 0.0\n",
      "learned_steps 2035: 0.0\n",
      "learned_steps 2036: 0.0\n",
      "learned_steps 2037: 0.0\n",
      "learned_steps 2038: 0.0\n",
      "learned_steps 2039: 0.0\n",
      "learned_steps 2040: 0.0\n",
      "learned_steps 2041: 0.0\n",
      "learned_steps 2042: 0.0\n",
      "learned_steps 2043: 0.0\n",
      "learned_steps 2044: 0.0\n",
      "learned_steps 2045: 0.10000000149011612\n",
      "learned_steps 2046: 0.0\n",
      "learned_steps 2047: 0.0\n",
      "learned_steps 2048: 0.0\n",
      "learned_steps 2049: 0.0\n",
      "learned_steps 2050: 0.0\n",
      "learned_steps 2051: 0.0\n",
      "learned_steps 2052: 0.0\n",
      "learned_steps 2053: 0.0\n",
      "learned_steps 2054: 0.0\n",
      "learned_steps 2055: 0.0\n",
      "learned_steps 2056: 0.0\n",
      "learned_steps 2057: 0.0\n",
      "learned_steps 2058: 0.10000000149011612\n",
      "learned_steps 2059: 0.0\n",
      "learned_steps 2060: 0.0\n",
      "learned_steps 2061: 0.0\n",
      "learned_steps 2062: 0.0\n",
      "learned_steps 2063: 0.0\n",
      "learned_steps 2064: 0.0\n",
      "learned_steps 2065: 0.0\n",
      "learned_steps 2066: 0.0\n",
      "learned_steps 2067: 0.0\n",
      "learned_steps 2068: 0.0\n",
      "learned_steps 2069: 0.0\n",
      "learned_steps 2070: 0.0\n",
      "learned_steps 2071: 0.0\n",
      "learned_steps 2072: 0.0\n",
      "learned_steps 2073: 0.0\n",
      "learned_steps 2074: 0.10000000149011612\n",
      "learned_steps 2075: 0.0\n",
      "learned_steps 2076: 0.0\n",
      "learned_steps 2077: 0.0\n",
      "learned_steps 2078: 0.0\n",
      "learned_steps 2079: 0.0\n",
      "learned_steps 2080: 0.0\n",
      "learned_steps 2081: 0.0\n",
      "learned_steps 2082: 0.0\n",
      "learned_steps 2083: 0.0\n",
      "learned_steps 2084: 0.0\n",
      "learned_steps 2085: 0.0\n",
      "learned_steps 2086: 0.0\n",
      "learned_steps 2087: 0.0\n",
      "learned_steps 2088: 0.0\n",
      "learned_steps 2089: 0.0\n",
      "learned_steps 2090: 0.0\n",
      "learned_steps 2091: 0.0\n",
      "learned_steps 2092: 0.0\n",
      "learned_steps 2093: 0.0\n",
      "learned_steps 2094: 0.0\n",
      "learned_steps 2095: 0.0\n",
      "learned_steps 2096: 0.0\n",
      "learned_steps 2097: 0.0\n",
      "learned_steps 2098: 0.0\n",
      "learned_steps 2099: 0.0\n",
      "learned_steps 2100: 0.0\n",
      "learned_steps 2101: 0.10000000149011612\n",
      "learned_steps 2102: 0.0\n",
      "learned_steps 2103: 0.0\n",
      "learned_steps 2104: 0.0\n",
      "learned_steps 2105: 0.0\n",
      "learned_steps 2106: 0.10000000149011612\n",
      "learned_steps 2107: 0.0\n",
      "learned_steps 2108: 0.0\n",
      "learned_steps 2109: 0.0\n",
      "learned_steps 2110: 0.0\n",
      "learned_steps 2111: 0.0\n",
      "learned_steps 2112: 0.0\n",
      "learned_steps 2113: 0.0\n",
      "learned_steps 2114: 0.0\n",
      "learned_steps 2115: 0.0\n",
      "learned_steps 2116: 0.0\n",
      "learned_steps 2117: 0.0\n",
      "learned_steps 2118: 0.0\n",
      "learned_steps 2119: 0.0\n",
      "learned_steps 2120: 0.0\n",
      "learned_steps 2121: 0.0\n",
      "learned_steps 2122: 0.0\n",
      "learned_steps 2123: 0.10000000149011612\n",
      "learned_steps 2124: 0.0\n",
      "learned_steps 2125: 0.0\n",
      "learned_steps 2126: 0.0\n",
      "learned_steps 2127: 0.0\n",
      "learned_steps 2128: 0.0\n",
      "learned_steps 2129: 0.0\n",
      "learned_steps 2130: 0.0\n",
      "learned_steps 2131: 0.0\n",
      "learned_steps 2132: 0.0\n",
      "learned_steps 2133: 0.0\n",
      "learned_steps 2134: 0.0\n",
      "learned_steps 2135: 0.0\n",
      "learned_steps 2136: 0.0\n",
      "learned_steps 2137: 0.10000000149011612\n",
      "learned_steps 2138: 0.0\n",
      "learned_steps 2139: 0.0\n",
      "learned_steps 2140: 0.0\n",
      "learned_steps 2141: 0.0\n",
      "learned_steps 2142: 0.0\n",
      "learned_steps 2143: 0.0\n",
      "learned_steps 2144: 0.10000000149011612\n",
      "learned_steps 2145: 0.0\n",
      "learned_steps 2146: 0.0\n",
      "learned_steps 2147: 0.0\n",
      "learned_steps 2148: 0.0\n",
      "learned_steps 2149: 0.0\n",
      "learned_steps 2150: 0.0\n",
      "learned_steps 2151: 0.0\n",
      "learned_steps 2152: 0.0\n",
      "learned_steps 2153: 0.0\n",
      "learned_steps 2154: 0.0\n",
      "learned_steps 2155: 0.0\n",
      "learned_steps 2156: 0.0\n",
      "learned_steps 2157: 0.0\n",
      "learned_steps 2158: 0.0\n",
      "learned_steps 2159: 0.10000000149011612\n",
      "learned_steps 2160: 0.0\n",
      "learned_steps 2161: 0.0\n",
      "learned_steps 2162: 0.0\n",
      "learned_steps 2163: 0.0\n",
      "learned_steps 2164: 0.0\n",
      "learned_steps 2165: 0.0\n",
      "learned_steps 2166: 0.0\n",
      "learned_steps 2167: 0.0\n",
      "learned_steps 2168: 0.0\n",
      "learned_steps 2169: 0.0\n",
      "learned_steps 2170: 0.0\n",
      "learned_steps 2171: 0.0\n",
      "learned_steps 2172: 0.0\n",
      "learned_steps 2173: 0.0\n",
      "learned_steps 2174: 0.0\n",
      "learned_steps 2175: 0.0\n",
      "learned_steps 2176: 0.0\n",
      "learned_steps 2177: 0.0\n",
      "learned_steps 2178: 0.0\n",
      "learned_steps 2179: 0.0\n",
      "learned_steps 2180: 0.0\n",
      "learned_steps 2181: 0.0\n",
      "learned_steps 2182: 0.0\n",
      "learned_steps 2183: 0.0\n",
      "learned_steps 2184: 0.0\n",
      "learned_steps 2185: 0.0\n",
      "learned_steps 2186: 0.0\n",
      "learned_steps 2187: 0.0\n",
      "learned_steps 2188: 0.0\n",
      "learned_steps 2189: 0.0\n",
      "learned_steps 2190: 0.0\n",
      "learned_steps 2191: 0.0\n",
      "learned_steps 2192: 0.0\n",
      "learned_steps 2193: 0.0\n",
      "learned_steps 2194: 0.0\n",
      "learned_steps 2195: 0.0\n",
      "learned_steps 2196: 0.0\n",
      "learned_steps 2197: 0.0\n",
      "learned_steps 2198: 0.0\n",
      "learned_steps 2199: 0.0\n",
      "learned_steps 2200: 0.0\n",
      "learned_steps 2201: 0.0\n",
      "learned_steps 2202: 0.0\n",
      "learned_steps 2203: 0.0\n",
      "learned_steps 2204: 0.0\n",
      "learned_steps 2205: 0.0\n",
      "learned_steps 2206: 0.10000000149011612\n",
      "learned_steps 2207: 0.0\n",
      "learned_steps 2208: 0.0\n",
      "learned_steps 2209: 0.0\n",
      "learned_steps 2210: 0.0\n",
      "learned_steps 2211: 0.0\n",
      "learned_steps 2212: 0.0\n",
      "learned_steps 2213: 0.0\n",
      "learned_steps 2214: 0.0\n",
      "learned_steps 2215: 0.0\n",
      "learned_steps 2216: 0.0\n",
      "learned_steps 2217: 0.0\n",
      "learned_steps 2218: 0.0\n",
      "learned_steps 2219: 0.0\n",
      "learned_steps 2220: 0.0\n",
      "learned_steps 2221: 0.0\n",
      "learned_steps 2222: 0.0\n",
      "learned_steps 2223: 0.0\n",
      "learned_steps 2224: 0.0\n",
      "learned_steps 2225: 0.0\n",
      "learned_steps 2226: 0.0\n",
      "learned_steps 2227: 0.0\n",
      "learned_steps 2228: 0.10000000149011612\n",
      "learned_steps 2229: 0.0\n",
      "learned_steps 2230: 0.0\n",
      "learned_steps 2231: 0.0\n",
      "learned_steps 2232: 0.10000000149011612\n",
      "learned_steps 2233: 0.0\n",
      "learned_steps 2234: 0.0\n",
      "learned_steps 2235: 0.0\n",
      "learned_steps 2236: 0.0\n",
      "learned_steps 2237: 0.0\n",
      "learned_steps 2238: 0.0\n",
      "learned_steps 2239: 0.0\n",
      "learned_steps 2240: 0.0\n",
      "learned_steps 2241: 0.0\n",
      "learned_steps 2242: 0.0\n",
      "learned_steps 2243: 0.0\n",
      "learned_steps 2244: 0.0\n",
      "learned_steps 2245: 0.0\n",
      "learned_steps 2246: 0.0\n",
      "learned_steps 2247: 0.0\n",
      "learned_steps 2248: 0.0\n",
      "learned_steps 2249: 0.0\n",
      "learned_steps 2250: 0.0\n",
      "learned_steps 2251: 0.0\n",
      "learned_steps 2252: 0.0\n",
      "learned_steps 2253: 0.0\n",
      "learned_steps 2254: 0.0\n",
      "learned_steps 2255: 0.0\n",
      "learned_steps 2256: 0.0\n",
      "learned_steps 2257: 0.0\n",
      "learned_steps 2258: 0.0\n",
      "learned_steps 2259: 0.0\n",
      "learned_steps 2260: 0.0\n",
      "learned_steps 2261: 0.0\n",
      "learned_steps 2262: 0.0\n",
      "learned_steps 2263: 0.0\n",
      "learned_steps 2264: 0.0\n",
      "learned_steps 2265: 0.0\n",
      "learned_steps 2266: 0.0\n",
      "learned_steps 2267: 0.0\n",
      "learned_steps 2268: 0.0\n",
      "learned_steps 2269: 0.0\n",
      "learned_steps 2270: 0.0\n",
      "learned_steps 2271: 0.0\n",
      "learned_steps 2272: 0.0\n",
      "learned_steps 2273: 0.0\n",
      "learned_steps 2274: 0.0\n",
      "learned_steps 2275: 0.0\n",
      "learned_steps 2276: 0.0\n",
      "learned_steps 2277: 0.0\n",
      "learned_steps 2278: 0.0\n",
      "learned_steps 2279: 0.0\n",
      "learned_steps 2280: 0.10000000149011612\n",
      "learned_steps 2281: 0.0\n",
      "learned_steps 2282: 0.0\n",
      "learned_steps 2283: 0.0\n",
      "learned_steps 2284: 0.0\n",
      "learned_steps 2285: 0.0\n",
      "learned_steps 2286: 0.0\n",
      "learned_steps 2287: 0.0\n",
      "learned_steps 2288: 0.0\n",
      "learned_steps 2289: 0.0\n",
      "learned_steps 2290: 0.0\n",
      "learned_steps 2291: 0.0\n",
      "learned_steps 2292: 0.0\n",
      "learned_steps 2293: 0.0\n",
      "learned_steps 2294: 0.0\n",
      "learned_steps 2295: 0.0\n",
      "learned_steps 2296: 0.0\n",
      "learned_steps 2297: 0.0\n",
      "learned_steps 2298: 0.0\n",
      "learned_steps 2299: 0.0\n",
      "learned_steps 2300: 0.0\n",
      "learned_steps 2301: 0.0\n",
      "learned_steps 2302: 0.0\n",
      "learned_steps 2303: 0.0\n",
      "learned_steps 2304: 0.0\n",
      "learned_steps 2305: 0.0\n",
      "learned_steps 2306: 0.0\n",
      "learned_steps 2307: 0.0\n",
      "learned_steps 2308: 0.0\n",
      "learned_steps 2309: 0.0\n",
      "learned_steps 2310: 0.0\n",
      "learned_steps 2311: 0.0\n",
      "learned_steps 2312: 0.0\n",
      "learned_steps 2313: 0.0\n",
      "learned_steps 2314: 0.0\n",
      "learned_steps 2315: 0.10000000149011612\n",
      "learned_steps 2316: 0.0\n",
      "learned_steps 2317: 0.0\n",
      "learned_steps 2318: 0.10000000149011612\n",
      "learned_steps 2319: 0.0\n",
      "learned_steps 2320: 0.0\n",
      "learned_steps 2321: 0.0\n",
      "learned_steps 2322: 0.0\n",
      "learned_steps 2323: 0.0\n",
      "learned_steps 2324: 0.0\n",
      "learned_steps 2325: 0.10000000149011612\n",
      "learned_steps 2326: 0.0\n",
      "learned_steps 2327: 0.0\n",
      "learned_steps 2328: 0.0\n",
      "learned_steps 2329: 0.10000000149011612\n",
      "learned_steps 2330: 0.0\n",
      "learned_steps 2331: 0.0\n",
      "learned_steps 2332: 0.0\n",
      "learned_steps 2333: 0.0\n",
      "learned_steps 2334: 0.0\n",
      "learned_steps 2335: 0.10000000149011612\n",
      "learned_steps 2336: 0.0\n",
      "learned_steps 2337: 0.0\n",
      "learned_steps 2338: 0.0\n",
      "learned_steps 2339: 0.0\n",
      "learned_steps 2340: 0.0\n",
      "learned_steps 2341: 0.0\n",
      "learned_steps 2342: 0.0\n",
      "learned_steps 2343: 0.0\n",
      "learned_steps 2344: 0.0\n",
      "learned_steps 2345: 0.0\n",
      "learned_steps 2346: 0.0\n",
      "learned_steps 2347: 0.0\n",
      "learned_steps 2348: 0.0\n",
      "learned_steps 2349: 0.0\n",
      "learned_steps 2350: 0.0\n",
      "learned_steps 2351: 0.0\n",
      "learned_steps 2352: 0.0\n",
      "learned_steps 2353: 0.0\n",
      "learned_steps 2354: 0.0\n",
      "learned_steps 2355: 0.0\n",
      "learned_steps 2356: 0.0\n",
      "learned_steps 2357: 0.0\n",
      "learned_steps 2358: 0.0\n",
      "learned_steps 2359: 0.0\n",
      "learned_steps 2360: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned_steps 2361: 0.0\n",
      "learned_steps 2362: 0.0\n",
      "learned_steps 2363: 0.0\n",
      "learned_steps 2364: 0.0\n",
      "learned_steps 2365: 0.0\n",
      "learned_steps 2366: 0.0\n",
      "learned_steps 2367: 0.0\n",
      "learned_steps 2368: 0.0\n",
      "learned_steps 2369: 0.0\n",
      "learned_steps 2370: 0.0\n",
      "learned_steps 2371: 0.10000000149011612\n",
      "learned_steps 2372: 0.0\n",
      "learned_steps 2373: 0.0\n",
      "learned_steps 2374: 0.0\n",
      "learned_steps 2375: 0.0\n",
      "learned_steps 2376: 0.0\n",
      "learned_steps 2377: 0.0\n",
      "learned_steps 2378: 0.0\n",
      "learned_steps 2379: 0.10000000149011612\n",
      "learned_steps 2380: 0.0\n",
      "learned_steps 2381: 0.0\n",
      "learned_steps 2382: 0.0\n",
      "learned_steps 2383: 0.0\n",
      "learned_steps 2384: 0.0\n",
      "learned_steps 2385: 0.0\n",
      "learned_steps 2386: 0.0\n",
      "learned_steps 2387: 0.0\n",
      "learned_steps 2388: 0.0\n",
      "learned_steps 2389: 0.0\n",
      "learned_steps 2390: 0.10000000149011612\n",
      "learned_steps 2391: 0.0\n",
      "learned_steps 2392: 0.0\n",
      "learned_steps 2393: 0.0\n",
      "learned_steps 2394: 0.0\n",
      "learned_steps 2395: 0.0\n",
      "learned_steps 2396: 0.0\n",
      "learned_steps 2397: 0.0\n",
      "learned_steps 2398: 0.0\n",
      "learned_steps 2399: 0.0\n",
      "learned_steps 2400: 0.0\n",
      "learned_steps 2401: 0.0\n",
      "learned_steps 2402: 0.0\n",
      "learned_steps 2403: 0.0\n",
      "learned_steps 2404: 0.0\n",
      "learned_steps 2405: 0.0\n",
      "learned_steps 2406: 0.0\n",
      "learned_steps 2407: 0.0\n",
      "learned_steps 2408: 0.0\n",
      "learned_steps 2409: 0.0\n",
      "learned_steps 2410: 0.0\n",
      "learned_steps 2411: 0.0\n",
      "learned_steps 2412: 0.0\n",
      "learned_steps 2413: 0.0\n",
      "learned_steps 2414: 0.0\n",
      "learned_steps 2415: 0.0\n",
      "learned_steps 2416: 0.0\n",
      "learned_steps 2417: 0.0\n",
      "learned_steps 2418: 0.0\n",
      "learned_steps 2419: 0.0\n",
      "learned_steps 2420: 0.0\n",
      "learned_steps 2421: 0.0\n",
      "learned_steps 2422: 0.0\n",
      "learned_steps 2423: 0.0\n",
      "learned_steps 2424: 0.0\n",
      "learned_steps 2425: 0.0\n",
      "learned_steps 2426: 0.0\n",
      "learned_steps 2427: 0.0\n",
      "learned_steps 2428: 0.10000000149011612\n",
      "learned_steps 2429: 0.0\n",
      "learned_steps 2430: 0.0\n",
      "learned_steps 2431: 0.0\n",
      "learned_steps 2432: 0.0\n",
      "learned_steps 2433: 0.0\n",
      "learned_steps 2434: 0.0\n",
      "learned_steps 2435: 0.0\n",
      "learned_steps 2436: 0.0\n",
      "learned_steps 2437: 0.0\n",
      "learned_steps 2438: 0.0\n",
      "learned_steps 2439: 0.10000000149011612\n",
      "learned_steps 2440: 0.0\n",
      "learned_steps 2441: 0.10000000149011612\n",
      "learned_steps 2442: 0.0\n",
      "learned_steps 2443: 0.0\n",
      "learned_steps 2444: 0.0\n",
      "learned_steps 2445: 0.0\n",
      "learned_steps 2446: 0.0\n",
      "learned_steps 2447: 0.0\n",
      "learned_steps 2448: 0.0\n",
      "learned_steps 2449: 0.0\n",
      "learned_steps 2450: 0.0\n",
      "learned_steps 2451: 0.0\n",
      "learned_steps 2452: 0.0\n",
      "learned_steps 2453: 0.0\n",
      "learned_steps 2454: 0.0\n",
      "learned_steps 2455: 0.0\n",
      "learned_steps 2456: 0.0\n",
      "learned_steps 2457: 0.0\n",
      "learned_steps 2458: 0.0\n",
      "learned_steps 2459: 0.10000000149011612\n",
      "learned_steps 2460: 0.0\n",
      "learned_steps 2461: 0.0\n",
      "learned_steps 2462: 0.0\n",
      "learned_steps 2463: 0.0\n",
      "learned_steps 2464: 0.0\n",
      "learned_steps 2465: 0.0\n",
      "learned_steps 2466: 0.0\n",
      "learned_steps 2467: 0.0\n",
      "learned_steps 2468: 0.0\n",
      "learned_steps 2469: 0.0\n",
      "learned_steps 2470: 0.0\n",
      "learned_steps 2471: 0.0\n",
      "learned_steps 2472: 0.0\n",
      "learned_steps 2473: 0.0\n",
      "learned_steps 2474: 0.0\n",
      "learned_steps 2475: 0.0\n",
      "learned_steps 2476: 0.0\n",
      "learned_steps 2477: 0.0\n",
      "learned_steps 2478: 0.0\n",
      "learned_steps 2479: 0.0\n",
      "learned_steps 2480: 0.0\n",
      "learned_steps 2481: 0.0\n",
      "learned_steps 2482: 0.0\n",
      "learned_steps 2483: 0.0\n",
      "learned_steps 2484: 0.0\n",
      "learned_steps 2485: 0.10000000149011612\n",
      "learned_steps 2486: 0.0\n",
      "learned_steps 2487: 0.0\n",
      "learned_steps 2488: 0.0\n",
      "learned_steps 2489: 0.0\n",
      "learned_steps 2490: 0.0\n",
      "learned_steps 2491: 0.0\n",
      "learned_steps 2492: 0.0\n",
      "learned_steps 2493: 0.0\n",
      "learned_steps 2494: 0.0\n",
      "learned_steps 2495: 0.0\n",
      "learned_steps 2496: 0.0\n",
      "learned_steps 2497: 0.0\n",
      "learned_steps 2498: 0.0\n",
      "learned_steps 2499: 0.0\n",
      "learned_steps 2500: 0.0\n",
      "learned_steps 2501: 0.0\n",
      "learned_steps 2502: 0.0\n",
      "learned_steps 2503: 0.0\n",
      "learned_steps 2504: 0.0\n",
      "learned_steps 2505: 0.0\n",
      "learned_steps 2506: 0.10000000149011612\n",
      "learned_steps 2507: 0.0\n",
      "learned_steps 2508: 0.0\n",
      "learned_steps 2509: 0.0\n",
      "learned_steps 2510: 0.0\n",
      "learned_steps 2511: 0.0\n",
      "learned_steps 2512: 0.0\n",
      "learned_steps 2513: 0.0\n",
      "learned_steps 2514: 0.0\n",
      "learned_steps 2515: 0.0\n",
      "learned_steps 2516: 0.0\n",
      "learned_steps 2517: 0.0\n",
      "learned_steps 2518: 0.0\n",
      "learned_steps 2519: 0.0\n",
      "learned_steps 2520: 0.0\n",
      "learned_steps 2521: 0.0\n",
      "learned_steps 2522: 0.0\n",
      "learned_steps 2523: 0.0\n",
      "learned_steps 2524: 0.0\n",
      "learned_steps 2525: 0.0\n",
      "learned_steps 2526: 0.0\n",
      "learned_steps 2527: 0.0\n",
      "learned_steps 2528: 0.0\n",
      "learned_steps 2529: 0.0\n",
      "learned_steps 2530: 0.0\n",
      "learned_steps 2531: 0.10000000149011612\n",
      "learned_steps 2532: 0.0\n",
      "learned_steps 2533: 0.0\n",
      "learned_steps 2534: 0.0\n",
      "learned_steps 2535: 0.0\n",
      "learned_steps 2536: 0.10000000149011612\n",
      "learned_steps 2537: 0.0\n",
      "learned_steps 2538: 0.0\n",
      "learned_steps 2539: 0.0\n",
      "learned_steps 2540: 0.0\n",
      "learned_steps 2541: 0.0\n",
      "learned_steps 2542: 0.0\n",
      "learned_steps 2543: 0.0\n",
      "learned_steps 2544: 0.0\n",
      "learned_steps 2545: 0.0\n",
      "learned_steps 2546: 0.0\n",
      "learned_steps 2547: 0.0\n",
      "learned_steps 2548: 0.0\n",
      "learned_steps 2549: 0.0\n",
      "learned_steps 2550: 0.10000000149011612\n",
      "learned_steps 2551: 0.10000000149011612\n",
      "learned_steps 2552: 0.0\n",
      "learned_steps 2553: 0.0\n",
      "learned_steps 2554: 0.0\n",
      "learned_steps 2555: 0.0\n",
      "learned_steps 2556: 0.0\n",
      "learned_steps 2557: 0.0\n",
      "learned_steps 2558: 0.10000000149011612\n",
      "learned_steps 2559: 0.0\n",
      "learned_steps 2560: 0.0\n",
      "learned_steps 2561: 0.0\n",
      "learned_steps 2562: 0.0\n",
      "learned_steps 2563: 0.0\n",
      "learned_steps 2564: 0.0\n",
      "learned_steps 2565: 0.0\n",
      "learned_steps 2566: 0.0\n",
      "learned_steps 2567: 0.0\n",
      "learned_steps 2568: 0.0\n",
      "learned_steps 2569: 0.0\n",
      "learned_steps 2570: 0.0\n",
      "learned_steps 2571: 0.0\n",
      "learned_steps 2572: 0.0\n",
      "learned_steps 2573: 0.0\n",
      "learned_steps 2574: 0.0\n",
      "learned_steps 2575: 0.0\n",
      "learned_steps 2576: 0.0\n",
      "learned_steps 2577: 0.0\n",
      "learned_steps 2578: 0.0\n",
      "learned_steps 2579: 0.0\n",
      "learned_steps 2580: 0.0\n",
      "learned_steps 2581: 0.0\n",
      "learned_steps 2582: 0.0\n",
      "learned_steps 2583: 0.0\n",
      "learned_steps 2584: 0.0\n",
      "learned_steps 2585: 0.0\n",
      "learned_steps 2586: 0.0\n",
      "learned_steps 2587: 0.10000000149011612\n",
      "learned_steps 2588: 0.10000000149011612\n",
      "learned_steps 2589: 0.0\n",
      "learned_steps 2590: 0.0\n",
      "learned_steps 2591: 0.0\n",
      "learned_steps 2592: 0.0\n",
      "learned_steps 2593: 0.0\n",
      "learned_steps 2594: 0.0\n",
      "learned_steps 2595: 0.0\n",
      "learned_steps 2596: 0.0\n",
      "learned_steps 2597: 0.0\n",
      "learned_steps 2598: 0.0\n",
      "learned_steps 2599: 0.0\n",
      "learned_steps 2600: 0.0\n",
      "learned_steps 2601: 0.0\n",
      "learned_steps 2602: 0.0\n",
      "learned_steps 2603: 0.0\n",
      "learned_steps 2604: 0.0\n",
      "learned_steps 2605: 0.0\n",
      "learned_steps 2606: 0.0\n",
      "learned_steps 2607: 0.0\n",
      "learned_steps 2608: 0.0\n",
      "learned_steps 2609: 0.0\n",
      "learned_steps 2610: 0.0\n",
      "learned_steps 2611: 0.0\n",
      "learned_steps 2612: 0.0\n",
      "learned_steps 2613: 0.0\n",
      "learned_steps 2614: 0.0\n",
      "learned_steps 2615: 0.0\n",
      "learned_steps 2616: 0.10000000149011612\n",
      "learned_steps 2617: 0.10000000149011612\n",
      "learned_steps 2618: 0.0\n",
      "learned_steps 2619: 0.0\n",
      "learned_steps 2620: 0.0\n",
      "learned_steps 2621: 0.0\n",
      "learned_steps 2622: 0.0\n",
      "learned_steps 2623: 0.0\n",
      "learned_steps 2624: 0.0\n",
      "learned_steps 2625: 0.0\n",
      "learned_steps 2626: 0.0\n",
      "learned_steps 2627: 0.0\n",
      "learned_steps 2628: 0.0\n",
      "learned_steps 2629: 0.0\n",
      "learned_steps 2630: 0.0\n",
      "learned_steps 2631: 0.0\n",
      "learned_steps 2632: 0.0\n",
      "learned_steps 2633: 0.0\n",
      "learned_steps 2634: 0.0\n",
      "learned_steps 2635: 0.0\n",
      "learned_steps 2636: 0.0\n",
      "learned_steps 2637: 0.0\n",
      "learned_steps 2638: 0.10000000149011612\n",
      "learned_steps 2639: 0.0\n",
      "learned_steps 2640: 0.0\n",
      "learned_steps 2641: 0.0\n",
      "learned_steps 2642: 0.0\n",
      "learned_steps 2643: 0.0\n",
      "learned_steps 2644: 0.0\n",
      "learned_steps 2645: 0.0\n",
      "learned_steps 2646: 0.0\n",
      "learned_steps 2647: 0.0\n",
      "learned_steps 2648: 0.0\n",
      "learned_steps 2649: 0.0\n",
      "learned_steps 2650: 0.0\n",
      "learned_steps 2651: 0.0\n",
      "learned_steps 2652: 0.0\n",
      "learned_steps 2653: 0.0\n",
      "learned_steps 2654: 0.0\n",
      "learned_steps 2655: 0.0\n",
      "learned_steps 2656: 0.0\n",
      "learned_steps 2657: 0.0\n",
      "learned_steps 2658: 0.0\n",
      "learned_steps 2659: 0.0\n",
      "learned_steps 2660: 0.0\n",
      "learned_steps 2661: 0.0\n",
      "learned_steps 2662: 0.0\n",
      "learned_steps 2663: 0.0\n",
      "learned_steps 2664: 0.10000000149011612\n",
      "learned_steps 2665: 0.0\n",
      "learned_steps 2666: 0.0\n",
      "learned_steps 2667: 0.0\n",
      "learned_steps 2668: 0.0\n",
      "learned_steps 2669: 0.0\n",
      "learned_steps 2670: 0.0\n",
      "learned_steps 2671: 0.0\n",
      "learned_steps 2672: 0.0\n",
      "learned_steps 2673: 0.0\n",
      "learned_steps 2674: 0.0\n",
      "learned_steps 2675: 0.0\n",
      "learned_steps 2676: 0.0\n",
      "learned_steps 2677: 0.0\n",
      "learned_steps 2678: 0.0\n",
      "learned_steps 2679: 0.0\n",
      "learned_steps 2680: 0.0\n",
      "learned_steps 2681: 0.0\n",
      "learned_steps 2682: 0.0\n",
      "learned_steps 2683: 0.10000000149011612\n",
      "learned_steps 2684: 0.0\n",
      "learned_steps 2685: 0.0\n",
      "learned_steps 2686: 0.0\n",
      "learned_steps 2687: 0.0\n",
      "learned_steps 2688: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned_steps 2689: 0.0\n",
      "learned_steps 2690: 0.0\n",
      "learned_steps 2691: 0.0\n",
      "learned_steps 2692: 0.0\n",
      "learned_steps 2693: 0.0\n",
      "learned_steps 2694: 0.0\n",
      "learned_steps 2695: 0.0\n",
      "learned_steps 2696: 0.0\n",
      "learned_steps 2697: 0.0\n",
      "learned_steps 2698: 0.0\n",
      "learned_steps 2699: 0.0\n",
      "learned_steps 2700: 0.0\n",
      "learned_steps 2701: 0.0\n",
      "learned_steps 2702: 0.0\n",
      "learned_steps 2703: 0.0\n",
      "learned_steps 2704: 0.0\n",
      "learned_steps 2705: 0.0\n",
      "learned_steps 2706: 0.0\n",
      "learned_steps 2707: 0.0\n",
      "learned_steps 2708: 0.0\n",
      "learned_steps 2709: 0.0\n",
      "learned_steps 2710: 0.0\n",
      "learned_steps 2711: 0.0\n",
      "learned_steps 2712: 0.0\n",
      "learned_steps 2713: 0.0\n",
      "learned_steps 2714: 0.0\n",
      "learned_steps 2715: 0.0\n",
      "learned_steps 2716: 0.0\n",
      "learned_steps 2717: 0.0\n",
      "learned_steps 2718: 0.0\n",
      "learned_steps 2719: 0.0\n",
      "learned_steps 2720: 0.0\n",
      "learned_steps 2721: 0.0\n",
      "learned_steps 2722: 0.0\n",
      "learned_steps 2723: 0.0\n",
      "learned_steps 2724: 0.0\n",
      "learned_steps 2725: 0.0\n",
      "learned_steps 2726: 0.0\n",
      "learned_steps 2727: 0.0\n",
      "learned_steps 2728: 0.10000000149011612\n",
      "learned_steps 2729: 0.0\n",
      "learned_steps 2730: 0.10000000149011612\n",
      "learned_steps 2731: 0.0\n",
      "learned_steps 2732: 0.0\n",
      "learned_steps 2733: 0.0\n",
      "learned_steps 2734: 0.0\n",
      "learned_steps 2735: 0.0\n",
      "learned_steps 2736: 0.0\n",
      "learned_steps 2737: 0.0\n",
      "learned_steps 2738: 0.0\n",
      "learned_steps 2739: 0.0\n",
      "learned_steps 2740: 0.0\n",
      "learned_steps 2741: 0.0\n",
      "learned_steps 2742: 0.0\n",
      "learned_steps 2743: 0.0\n",
      "learned_steps 2744: 0.0\n",
      "learned_steps 2745: 0.0\n",
      "learned_steps 2746: 0.0\n",
      "learned_steps 2747: 0.0\n",
      "learned_steps 2748: 0.0\n",
      "learned_steps 2749: 0.0\n",
      "learned_steps 2750: 0.0\n",
      "learned_steps 2751: 0.0\n",
      "learned_steps 2752: 0.0\n",
      "learned_steps 2753: 0.0\n",
      "learned_steps 2754: 0.0\n",
      "learned_steps 2755: 0.0\n",
      "learned_steps 2756: 0.0\n",
      "learned_steps 2757: 0.0\n",
      "learned_steps 2758: 0.0\n",
      "learned_steps 2759: 0.0\n",
      "learned_steps 2760: 0.0\n",
      "learned_steps 2761: 0.0\n",
      "learned_steps 2762: 0.0\n",
      "learned_steps 2763: 0.0\n",
      "learned_steps 2764: 0.0\n",
      "learned_steps 2765: 0.0\n",
      "learned_steps 2766: 0.0\n",
      "learned_steps 2767: 0.10000000149011612\n",
      "learned_steps 2768: 0.0\n",
      "learned_steps 2769: 0.0\n",
      "learned_steps 2770: 0.0\n",
      "learned_steps 2771: 0.0\n",
      "learned_steps 2772: 0.0\n",
      "learned_steps 2773: 0.0\n",
      "learned_steps 2774: 0.0\n",
      "learned_steps 2775: 0.0\n",
      "learned_steps 2776: 0.0\n",
      "learned_steps 2777: 0.0\n",
      "learned_steps 2778: 0.0\n",
      "learned_steps 2779: 0.0\n",
      "learned_steps 2780: 0.0\n",
      "learned_steps 2781: 0.0\n",
      "learned_steps 2782: 0.0\n",
      "learned_steps 2783: 0.10000000149011612\n",
      "learned_steps 2784: 0.0\n",
      "learned_steps 2785: 0.0\n",
      "learned_steps 2786: 0.0\n",
      "learned_steps 2787: 0.0\n",
      "learned_steps 2788: 0.0\n",
      "learned_steps 2789: 0.0\n",
      "learned_steps 2790: 0.0\n",
      "learned_steps 2791: 0.0\n",
      "learned_steps 2792: 0.0\n",
      "learned_steps 2793: 0.0\n",
      "learned_steps 2794: 0.0\n",
      "learned_steps 2795: 0.0\n",
      "learned_steps 2796: 0.0\n",
      "learned_steps 2797: 0.0\n",
      "learned_steps 2798: 0.0\n",
      "learned_steps 2799: 0.0\n",
      "learned_steps 2800: 0.0\n",
      "learned_steps 2801: 0.0\n",
      "learned_steps 2802: 0.0\n",
      "learned_steps 2803: 0.0\n",
      "learned_steps 2804: 0.0\n",
      "learned_steps 2805: 0.0\n",
      "learned_steps 2806: 0.0\n",
      "learned_steps 2807: 0.0\n",
      "learned_steps 2808: 0.0\n",
      "learned_steps 2809: 0.0\n",
      "learned_steps 2810: 0.10000000149011612\n",
      "learned_steps 2811: 0.0\n",
      "learned_steps 2812: 0.0\n",
      "learned_steps 2813: 0.10000000149011612\n",
      "learned_steps 2814: 0.0\n",
      "learned_steps 2815: 0.0\n",
      "learned_steps 2816: 0.0\n",
      "learned_steps 2817: 0.0\n",
      "learned_steps 2818: 0.0\n",
      "learned_steps 2819: 0.0\n",
      "learned_steps 2820: 0.0\n",
      "learned_steps 2821: 0.0\n",
      "learned_steps 2822: 0.0\n",
      "learned_steps 2823: 0.0\n",
      "learned_steps 2824: 0.0\n",
      "learned_steps 2825: 0.0\n",
      "learned_steps 2826: 0.0\n",
      "learned_steps 2827: 0.0\n",
      "learned_steps 2828: 0.0\n",
      "learned_steps 2829: 0.0\n",
      "learned_steps 2830: 0.0\n",
      "learned_steps 2831: 0.0\n",
      "learned_steps 2832: 0.0\n",
      "learned_steps 2833: 0.0\n",
      "learned_steps 2834: 0.0\n",
      "learned_steps 2835: 0.0\n",
      "learned_steps 2836: 0.0\n",
      "learned_steps 2837: 0.0\n",
      "learned_steps 2838: 0.0\n",
      "learned_steps 2839: 0.10000000149011612\n",
      "learned_steps 2840: 0.0\n",
      "learned_steps 2841: 0.0\n",
      "learned_steps 2842: 0.0\n",
      "learned_steps 2843: 0.0\n",
      "learned_steps 2844: 0.0\n",
      "learned_steps 2845: 0.0\n",
      "learned_steps 2846: 0.0\n",
      "learned_steps 2847: 0.0\n",
      "learned_steps 2848: 0.0\n",
      "learned_steps 2849: 0.0\n",
      "learned_steps 2850: 0.0\n",
      "learned_steps 2851: 0.0\n",
      "learned_steps 2852: 0.0\n",
      "learned_steps 2853: 0.0\n",
      "learned_steps 2854: 0.0\n",
      "learned_steps 2855: 0.0\n",
      "learned_steps 2856: 0.0\n",
      "learned_steps 2857: 0.10000000149011612\n",
      "learned_steps 2858: 0.0\n",
      "learned_steps 2859: 0.0\n",
      "learned_steps 2860: 0.0\n",
      "learned_steps 2861: 0.0\n",
      "learned_steps 2862: 0.0\n",
      "learned_steps 2863: 0.0\n",
      "learned_steps 2864: 0.0\n",
      "learned_steps 2865: 0.0\n",
      "learned_steps 2866: 0.0\n",
      "learned_steps 2867: 0.0\n",
      "learned_steps 2868: 0.0\n",
      "learned_steps 2869: 0.0\n",
      "learned_steps 2870: 0.0\n",
      "learned_steps 2871: 0.0\n",
      "learned_steps 2872: 0.0\n",
      "learned_steps 2873: 0.0\n",
      "learned_steps 2874: 0.0\n",
      "learned_steps 2875: 0.0\n",
      "learned_steps 2876: 0.0\n",
      "learned_steps 2877: 0.0\n",
      "learned_steps 2878: 0.0\n",
      "learned_steps 2879: 0.0\n",
      "learned_steps 2880: 0.0\n",
      "learned_steps 2881: 0.0\n",
      "learned_steps 2882: 0.0\n",
      "learned_steps 2883: 0.0\n",
      "learned_steps 2884: 0.0\n",
      "learned_steps 2885: 0.0\n",
      "learned_steps 2886: 0.0\n",
      "learned_steps 2887: 0.0\n",
      "learned_steps 2888: 0.0\n",
      "learned_steps 2889: 0.0\n",
      "learned_steps 2890: 0.0\n",
      "learned_steps 2891: 0.0\n",
      "learned_steps 2892: 0.0\n",
      "learned_steps 2893: 0.0\n",
      "learned_steps 2894: 0.0\n",
      "learned_steps 2895: 0.0\n",
      "learned_steps 2896: 0.0\n",
      "learned_steps 2897: 0.0\n",
      "learned_steps 2898: 0.0\n",
      "learned_steps 2899: 0.0\n",
      "learned_steps 2900: 0.0\n",
      "learned_steps 2901: 0.0\n",
      "learned_steps 2902: 0.0\n",
      "learned_steps 2903: 0.0\n",
      "learned_steps 2904: 0.0\n",
      "learned_steps 2905: 0.0\n",
      "learned_steps 2906: 0.0\n",
      "learned_steps 2907: 0.0\n",
      "learned_steps 2908: 0.0\n",
      "learned_steps 2909: 0.0\n",
      "learned_steps 2910: 0.0\n",
      "learned_steps 2911: 0.0\n",
      "learned_steps 2912: 0.0\n",
      "learned_steps 2913: 0.0\n",
      "learned_steps 2914: 0.0\n",
      "learned_steps 2915: 0.0\n",
      "learned_steps 2916: 0.0\n",
      "learned_steps 2917: 0.0\n",
      "learned_steps 2918: 0.0\n",
      "learned_steps 2919: 0.0\n",
      "learned_steps 2920: 0.0\n",
      "learned_steps 2921: 0.0\n",
      "learned_steps 2922: 0.0\n",
      "learned_steps 2923: 0.0\n",
      "learned_steps 2924: 0.0\n",
      "learned_steps 2925: 0.0\n",
      "learned_steps 2926: 0.0\n",
      "learned_steps 2927: 0.0\n",
      "learned_steps 2928: 0.0\n",
      "learned_steps 2929: 0.0\n",
      "learned_steps 2930: 0.0\n",
      "learned_steps 2931: 0.0\n",
      "learned_steps 2932: 0.0\n",
      "learned_steps 2933: 0.0\n",
      "learned_steps 2934: 0.0\n",
      "learned_steps 2935: 0.0\n",
      "learned_steps 2936: 0.0\n",
      "learned_steps 2937: 0.0\n",
      "learned_steps 2938: 0.0\n",
      "learned_steps 2939: 0.0\n",
      "learned_steps 2940: 0.0\n",
      "learned_steps 2941: 0.0\n",
      "learned_steps 2942: 0.0\n",
      "learned_steps 2943: 0.0\n",
      "learned_steps 2944: 0.0\n",
      "learned_steps 2945: 0.0\n",
      "learned_steps 2946: 0.0\n",
      "learned_steps 2947: 0.0\n",
      "learned_steps 2948: 0.0\n",
      "learned_steps 2949: 0.10000000149011612\n",
      "learned_steps 2950: 0.0\n",
      "learned_steps 2951: 0.0\n",
      "learned_steps 2952: 0.0\n",
      "learned_steps 2953: 0.0\n",
      "learned_steps 2954: 0.0\n",
      "learned_steps 2955: 0.0\n",
      "learned_steps 2956: 0.10000000149011612\n",
      "learned_steps 2957: 0.0\n",
      "learned_steps 2958: 0.0\n",
      "learned_steps 2959: 0.0\n",
      "learned_steps 2960: 0.0\n",
      "learned_steps 2961: 0.0\n",
      "learned_steps 2962: 0.0\n",
      "learned_steps 2963: 0.0\n",
      "learned_steps 2964: 0.0\n",
      "learned_steps 2965: 0.10000000149011612\n",
      "learned_steps 2966: 0.0\n",
      "learned_steps 2967: 0.0\n",
      "learned_steps 2968: 0.0\n",
      "learned_steps 2969: 0.10000000149011612\n",
      "learned_steps 2970: 0.0\n",
      "learned_steps 2971: 0.0\n",
      "learned_steps 2972: 0.0\n",
      "learned_steps 2973: 0.0\n",
      "learned_steps 2974: 0.0\n",
      "learned_steps 2975: 0.0\n",
      "learned_steps 2976: 0.0\n",
      "learned_steps 2977: 0.0\n",
      "learned_steps 2978: 0.0\n",
      "learned_steps 2979: 0.0\n",
      "learned_steps 2980: 0.0\n",
      "learned_steps 2981: 0.0\n",
      "learned_steps 2982: 0.0\n",
      "learned_steps 2983: 0.0\n",
      "learned_steps 2984: 0.0\n",
      "learned_steps 2985: 0.0\n",
      "learned_steps 2986: 0.0\n",
      "learned_steps 2987: 0.0\n",
      "learned_steps 2988: 0.0\n",
      "learned_steps 2989: 0.0\n",
      "learned_steps 2990: 0.0\n",
      "learned_steps 2991: 0.0\n",
      "learned_steps 2992: 0.0\n",
      "learned_steps 2993: 0.10000000149011612\n",
      "learned_steps 2994: 0.0\n",
      "learned_steps 2995: 0.0\n",
      "learned_steps 2996: 0.0\n",
      "learned_steps 2997: 0.0\n",
      "learned_steps 2998: 0.0\n",
      "learned_steps 2999: 0.0\n",
      "learned_steps 3000: 0.0\n",
      "learned_steps 3001: 0.0\n",
      "learned_steps 3002: 0.0\n",
      "learned_steps 3003: 0.0\n",
      "learned_steps 3004: 0.0\n",
      "learned_steps 3005: 0.0\n",
      "learned_steps 3006: 0.0\n",
      "learned_steps 3007: 0.0\n",
      "learned_steps 3008: 0.0\n",
      "learned_steps 3009: 0.0\n",
      "learned_steps 3010: 0.0\n",
      "learned_steps 3011: 0.0\n",
      "learned_steps 3012: 0.10000000149011612\n",
      "learned_steps 3013: 0.0\n",
      "learned_steps 3014: 0.0\n",
      "learned_steps 3015: 0.0\n",
      "learned_steps 3016: 0.0\n",
      "learned_steps 3017: 0.0\n",
      "learned_steps 3018: 0.10000000149011612\n",
      "learned_steps 3019: 0.0\n",
      "learned_steps 3020: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned_steps 3021: 0.0\n",
      "learned_steps 3022: 0.0\n",
      "learned_steps 3023: 0.0\n",
      "learned_steps 3024: 0.0\n",
      "learned_steps 3025: 0.0\n",
      "learned_steps 3026: 0.0\n",
      "learned_steps 3027: 0.10000000149011612\n",
      "learned_steps 3028: 0.10000000149011612\n",
      "learned_steps 3029: 0.0\n",
      "learned_steps 3030: 0.10000000149011612\n",
      "learned_steps 3031: 0.0\n",
      "learned_steps 3032: 0.0\n",
      "learned_steps 3033: 0.0\n",
      "learned_steps 3034: 0.10000000149011612\n",
      "learned_steps 3035: 0.0\n",
      "learned_steps 3036: 0.0\n",
      "learned_steps 3037: 0.0\n",
      "learned_steps 3038: 0.0\n",
      "learned_steps 3039: 0.0\n",
      "learned_steps 3040: 0.0\n",
      "learned_steps 3041: 0.10000000149011612\n",
      "learned_steps 3042: 0.0\n",
      "learned_steps 3043: 0.0\n",
      "learned_steps 3044: 0.0\n",
      "learned_steps 3045: 0.0\n",
      "learned_steps 3046: 0.0\n",
      "learned_steps 3047: 0.10000000149011612\n",
      "learned_steps 3048: 0.0\n",
      "learned_steps 3049: 0.0\n",
      "learned_steps 3050: 0.0\n",
      "learned_steps 3051: 0.0\n",
      "learned_steps 3052: 0.0\n",
      "learned_steps 3053: 0.0\n",
      "learned_steps 3054: 0.0\n",
      "learned_steps 3055: 0.0\n",
      "learned_steps 3056: 0.0\n",
      "learned_steps 3057: 0.0\n",
      "learned_steps 3058: 0.0\n",
      "learned_steps 3059: 0.0\n",
      "learned_steps 3060: 0.0\n",
      "learned_steps 3061: 0.0\n",
      "learned_steps 3062: 0.0\n",
      "learned_steps 3063: 0.0\n",
      "learned_steps 3064: 0.0\n",
      "learned_steps 3065: 0.0\n",
      "learned_steps 3066: 0.0\n",
      "learned_steps 3067: 0.0\n",
      "learned_steps 3068: 0.0\n",
      "learned_steps 3069: 0.0\n",
      "learned_steps 3070: 0.0\n",
      "learned_steps 3071: 0.0\n",
      "learned_steps 3072: 0.0\n",
      "learned_steps 3073: 0.0\n",
      "learned_steps 3074: 0.10000000149011612\n",
      "learned_steps 3075: 0.0\n",
      "learned_steps 3076: 0.0\n",
      "learned_steps 3077: 0.0\n",
      "learned_steps 3078: 0.0\n",
      "learned_steps 3079: 0.0\n",
      "learned_steps 3080: 0.10000000149011612\n",
      "learned_steps 3081: 0.0\n",
      "learned_steps 3082: 0.10000000149011612\n",
      "learned_steps 3083: 0.0\n",
      "learned_steps 3084: 0.0\n",
      "learned_steps 3085: 0.0\n",
      "learned_steps 3086: 0.0\n",
      "learned_steps 3087: 0.10000000149011612\n",
      "learned_steps 3088: 0.0\n",
      "learned_steps 3089: 0.10000000149011612\n",
      "learned_steps 3090: 0.0\n",
      "learned_steps 3091: 0.0\n",
      "learned_steps 3092: 0.0\n",
      "learned_steps 3093: 0.0\n",
      "learned_steps 3094: 0.0\n",
      "learned_steps 3095: 0.0\n",
      "learned_steps 3096: 0.0\n",
      "learned_steps 3097: 0.0\n",
      "learned_steps 3098: 0.0\n",
      "learned_steps 3099: 0.0\n",
      "learned_steps 3100: 0.0\n",
      "learned_steps 3101: 0.0\n",
      "learned_steps 3102: 0.0\n",
      "learned_steps 3103: 0.0\n",
      "learned_steps 3104: 0.0\n",
      "learned_steps 3105: 0.0\n",
      "learned_steps 3106: 0.0\n",
      "learned_steps 3107: 0.0\n",
      "learned_steps 3108: 0.0\n",
      "learned_steps 3109: 0.0\n",
      "learned_steps 3110: 0.0\n",
      "learned_steps 3111: 0.0\n",
      "learned_steps 3112: 0.0\n",
      "learned_steps 3113: 0.0\n",
      "learned_steps 3114: 0.0\n",
      "learned_steps 3115: 0.0\n",
      "learned_steps 3116: 0.0\n",
      "learned_steps 3117: 0.0\n",
      "learned_steps 3118: 0.0\n",
      "learned_steps 3119: 0.0\n",
      "learned_steps 3120: 0.10000000149011612\n",
      "learned_steps 3121: 0.0\n",
      "learned_steps 3122: 0.0\n",
      "learned_steps 3123: 0.0\n",
      "learned_steps 3124: 0.0\n",
      "learned_steps 3125: 0.0\n",
      "learned_steps 3126: 0.0\n",
      "learned_steps 3127: 0.0\n",
      "learned_steps 3128: 0.0\n",
      "learned_steps 3129: 0.0\n",
      "learned_steps 3130: 0.0\n",
      "learned_steps 3131: 0.10000000149011612\n",
      "learned_steps 3132: 0.0\n",
      "learned_steps 3133: 0.0\n",
      "learned_steps 3134: 0.10000000149011612\n",
      "learned_steps 3135: 0.0\n",
      "learned_steps 3136: 0.0\n",
      "learned_steps 3137: 0.0\n",
      "learned_steps 3138: 0.0\n",
      "learned_steps 3139: 0.0\n",
      "learned_steps 3140: 0.0\n",
      "learned_steps 3141: 0.0\n",
      "learned_steps 3142: 0.0\n",
      "learned_steps 3143: 0.0\n",
      "learned_steps 3144: 0.0\n",
      "learned_steps 3145: 0.0\n",
      "learned_steps 3146: 0.0\n",
      "learned_steps 3147: 0.0\n",
      "learned_steps 3148: 0.0\n",
      "learned_steps 3149: 0.0\n",
      "learned_steps 3150: 0.0\n",
      "learned_steps 3151: 0.0\n",
      "learned_steps 3152: 0.0\n",
      "learned_steps 3153: 0.0\n",
      "learned_steps 3154: 0.0\n",
      "learned_steps 3155: 0.0\n",
      "learned_steps 3156: 0.10000000149011612\n",
      "learned_steps 3157: 0.10000000149011612\n",
      "learned_steps 3158: 0.0\n",
      "learned_steps 3159: 0.0\n",
      "learned_steps 3160: 0.0\n",
      "learned_steps 3161: 0.0\n",
      "learned_steps 3162: 0.0\n",
      "learned_steps 3163: 0.0\n",
      "learned_steps 3164: 0.0\n",
      "learned_steps 3165: 0.0\n",
      "learned_steps 3166: 0.0\n",
      "learned_steps 3167: 0.10000000149011612\n",
      "learned_steps 3168: 0.0\n",
      "learned_steps 3169: 0.0\n",
      "learned_steps 3170: 0.0\n",
      "learned_steps 3171: 0.0\n",
      "learned_steps 3172: 0.0\n",
      "learned_steps 3173: 0.0\n",
      "learned_steps 3174: 0.10000000149011612\n",
      "learned_steps 3175: 0.0\n",
      "learned_steps 3176: 0.0\n",
      "learned_steps 3177: 0.0\n",
      "learned_steps 3178: 0.0\n",
      "learned_steps 3179: 0.0\n",
      "learned_steps 3180: 0.0\n",
      "learned_steps 3181: 0.0\n",
      "learned_steps 3182: 0.0\n",
      "learned_steps 3183: 0.0\n",
      "learned_steps 3184: 0.0\n",
      "learned_steps 3185: 0.10000000149011612\n",
      "learned_steps 3186: 0.0\n",
      "learned_steps 3187: 0.0\n",
      "learned_steps 3188: 0.0\n",
      "learned_steps 3189: 0.0\n",
      "learned_steps 3190: 0.0\n",
      "learned_steps 3191: 0.0\n",
      "learned_steps 3192: 0.0\n",
      "learned_steps 3193: 0.0\n",
      "learned_steps 3194: 0.0\n",
      "learned_steps 3195: 0.0\n",
      "learned_steps 3196: 0.0\n",
      "learned_steps 3197: 0.0\n",
      "learned_steps 3198: 0.0\n",
      "learned_steps 3199: 0.0\n",
      "learned_steps 3200: 0.0\n",
      "learned_steps 3201: 0.0\n",
      "learned_steps 3202: 0.0\n",
      "learned_steps 3203: 0.0\n",
      "learned_steps 3204: 0.0\n",
      "learned_steps 3205: 0.0\n",
      "learned_steps 3206: 0.0\n",
      "learned_steps 3207: 0.0\n",
      "learned_steps 3208: 0.0\n",
      "learned_steps 3209: 0.0\n",
      "learned_steps 3210: 0.0\n",
      "learned_steps 3211: 0.0\n",
      "learned_steps 3212: 0.0\n",
      "learned_steps 3213: 0.0\n",
      "learned_steps 3214: 0.0\n",
      "learned_steps 3215: 0.0\n",
      "learned_steps 3216: 0.0\n",
      "learned_steps 3217: 0.0\n",
      "learned_steps 3218: 0.0\n",
      "learned_steps 3219: 0.0\n",
      "learned_steps 3220: 0.0\n",
      "learned_steps 3221: 0.0\n",
      "learned_steps 3222: 0.0\n",
      "learned_steps 3223: 0.0\n",
      "learned_steps 3224: 0.0\n",
      "learned_steps 3225: 0.0\n",
      "learned_steps 3226: 0.0\n",
      "learned_steps 3227: 0.0\n",
      "learned_steps 3228: 0.0\n",
      "learned_steps 3229: 0.0\n",
      "learned_steps 3230: 0.0\n",
      "learned_steps 3231: 0.0\n",
      "learned_steps 3232: 0.0\n",
      "learned_steps 3233: 0.0\n",
      "learned_steps 3234: 0.0\n",
      "learned_steps 3235: 0.0\n",
      "learned_steps 3236: 0.0\n",
      "learned_steps 3237: 0.0\n",
      "learned_steps 3238: 0.0\n",
      "learned_steps 3239: 0.0\n",
      "learned_steps 3240: 0.0\n",
      "learned_steps 3241: 0.0\n",
      "learned_steps 3242: 0.0\n",
      "learned_steps 3243: 0.0\n",
      "learned_steps 3244: 0.0\n",
      "learned_steps 3245: 0.0\n",
      "learned_steps 3246: 0.0\n",
      "learned_steps 3247: 0.0\n",
      "learned_steps 3248: 0.0\n",
      "learned_steps 3249: 0.0\n",
      "learned_steps 3250: 0.0\n",
      "learned_steps 3251: 0.0\n",
      "learned_steps 3252: 0.0\n",
      "learned_steps 3253: 0.10000000149011612\n",
      "learned_steps 3254: 0.0\n",
      "learned_steps 3255: 0.0\n",
      "learned_steps 3256: 0.0\n",
      "learned_steps 3257: 0.0\n",
      "learned_steps 3258: 0.0\n",
      "learned_steps 3259: 0.0\n",
      "learned_steps 3260: 0.0\n",
      "learned_steps 3261: 0.0\n",
      "learned_steps 3262: 0.0\n",
      "learned_steps 3263: 0.0\n",
      "learned_steps 3264: 0.0\n",
      "learned_steps 3265: 0.0\n",
      "learned_steps 3266: 0.0\n",
      "learned_steps 3267: 0.0\n",
      "learned_steps 3268: 0.0\n",
      "learned_steps 3269: 0.0\n",
      "learned_steps 3270: 0.0\n",
      "learned_steps 3271: 0.0\n",
      "learned_steps 3272: 0.0\n",
      "learned_steps 3273: 0.0\n",
      "learned_steps 3274: 0.0\n",
      "learned_steps 3275: 0.0\n",
      "learned_steps 3276: 0.0\n",
      "learned_steps 3277: 0.0\n",
      "learned_steps 3278: 0.0\n",
      "learned_steps 3279: 0.0\n",
      "learned_steps 3280: 0.0\n",
      "learned_steps 3281: 0.10000000149011612\n",
      "learned_steps 3282: 0.0\n",
      "learned_steps 3283: 0.0\n",
      "learned_steps 3284: 0.0\n",
      "learned_steps 3285: 0.0\n",
      "learned_steps 3286: 0.0\n",
      "learned_steps 3287: 0.0\n",
      "learned_steps 3288: 0.0\n",
      "learned_steps 3289: 0.0\n",
      "learned_steps 3290: 0.0\n",
      "learned_steps 3291: 0.0\n",
      "learned_steps 3292: 0.0\n",
      "learned_steps 3293: 0.0\n",
      "learned_steps 3294: 0.0\n",
      "learned_steps 3295: 0.0\n",
      "learned_steps 3296: 0.0\n",
      "learned_steps 3297: 0.10000000149011612\n",
      "learned_steps 3298: 0.0\n",
      "learned_steps 3299: 0.0\n",
      "learned_steps 3300: 0.0\n",
      "learned_steps 3301: 0.0\n",
      "learned_steps 3302: 0.10000000149011612\n",
      "learned_steps 3303: 0.0\n",
      "learned_steps 3304: 0.0\n",
      "learned_steps 3305: 0.0\n",
      "learned_steps 3306: 0.0\n",
      "learned_steps 3307: 0.0\n",
      "learned_steps 3308: 0.0\n",
      "learned_steps 3309: 0.0\n",
      "learned_steps 3310: 0.0\n",
      "learned_steps 3311: 0.0\n",
      "learned_steps 3312: 0.0\n",
      "learned_steps 3313: 0.0\n",
      "learned_steps 3314: 0.0\n",
      "learned_steps 3315: 0.0\n",
      "learned_steps 3316: 0.0\n",
      "learned_steps 3317: 0.0\n",
      "learned_steps 3318: 0.0\n",
      "learned_steps 3319: 0.0\n",
      "learned_steps 3320: 0.0\n",
      "learned_steps 3321: 0.0\n",
      "learned_steps 3322: 0.0\n",
      "learned_steps 3323: 0.0\n",
      "learned_steps 3324: 0.0\n",
      "learned_steps 3325: 0.0\n",
      "learned_steps 3326: 0.0\n",
      "learned_steps 3327: 0.0\n",
      "learned_steps 3328: 0.0\n",
      "learned_steps 3329: 0.10000000149011612\n",
      "learned_steps 3330: 0.0\n",
      "learned_steps 3331: 0.0\n",
      "learned_steps 3332: 0.0\n",
      "learned_steps 3333: 0.0\n",
      "learned_steps 3334: 0.0\n",
      "learned_steps 3335: 0.0\n",
      "learned_steps 3336: 0.0\n",
      "learned_steps 3337: 0.0\n",
      "learned_steps 3338: 0.0\n",
      "learned_steps 3339: 0.0\n",
      "learned_steps 3340: 0.10000000149011612\n",
      "learned_steps 3341: 0.10000000149011612\n",
      "learned_steps 3342: 0.0\n",
      "learned_steps 3343: 0.0\n",
      "learned_steps 3344: 0.0\n",
      "learned_steps 3345: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned_steps 3346: 0.0\n",
      "learned_steps 3347: 0.0\n",
      "learned_steps 3348: 0.0\n",
      "learned_steps 3349: 0.0\n",
      "learned_steps 3350: 0.0\n",
      "learned_steps 3351: 0.0\n",
      "learned_steps 3352: 0.0\n",
      "learned_steps 3353: 0.0\n",
      "learned_steps 3354: 0.0\n",
      "learned_steps 3355: 0.0\n",
      "learned_steps 3356: 0.0\n",
      "learned_steps 3357: 0.0\n",
      "learned_steps 3358: 0.0\n",
      "learned_steps 3359: 0.0\n",
      "learned_steps 3360: 0.0\n",
      "learned_steps 3361: 0.0\n",
      "learned_steps 3362: 0.0\n",
      "learned_steps 3363: 0.0\n",
      "learned_steps 3364: 0.0\n",
      "learned_steps 3365: 0.0\n",
      "learned_steps 3366: 0.0\n",
      "learned_steps 3367: 0.0\n",
      "learned_steps 3368: 0.0\n",
      "learned_steps 3369: 0.0\n",
      "learned_steps 3370: 0.0\n",
      "learned_steps 3371: 0.0\n",
      "learned_steps 3372: 0.0\n",
      "learned_steps 3373: 0.0\n",
      "learned_steps 3374: 0.0\n",
      "learned_steps 3375: 0.0\n",
      "learned_steps 3376: 0.0\n",
      "learned_steps 3377: 0.0\n",
      "learned_steps 3378: 0.0\n",
      "learned_steps 3379: 0.0\n",
      "learned_steps 3380: 0.0\n",
      "learned_steps 3381: 0.0\n",
      "learned_steps 3382: 0.0\n",
      "learned_steps 3383: 0.0\n",
      "learned_steps 3384: 0.0\n",
      "learned_steps 3385: 0.0\n",
      "learned_steps 3386: 0.0\n",
      "learned_steps 3387: 0.0\n",
      "learned_steps 3388: 0.0\n",
      "learned_steps 3389: 0.0\n",
      "learned_steps 3390: 0.10000000149011612\n",
      "learned_steps 3391: 0.0\n",
      "learned_steps 3392: 0.0\n",
      "learned_steps 3393: 0.0\n",
      "learned_steps 3394: 0.0\n",
      "learned_steps 3395: 0.0\n",
      "learned_steps 3396: 0.0\n",
      "learned_steps 3397: 0.0\n",
      "learned_steps 3398: 0.0\n",
      "learned_steps 3399: 0.0\n",
      "learned_steps 3400: 0.0\n",
      "learned_steps 3401: 0.10000000149011612\n",
      "learned_steps 3402: 0.0\n",
      "learned_steps 3403: 0.0\n",
      "learned_steps 3404: 0.0\n",
      "learned_steps 3405: 0.0\n",
      "learned_steps 3406: 0.0\n",
      "learned_steps 3407: 0.0\n",
      "learned_steps 3408: 0.0\n",
      "learned_steps 3409: 0.10000000149011612\n",
      "learned_steps 3410: 0.0\n",
      "learned_steps 3411: 0.0\n",
      "learned_steps 3412: 0.0\n",
      "learned_steps 3413: 0.0\n",
      "learned_steps 3414: 0.0\n",
      "learned_steps 3415: 0.0\n",
      "learned_steps 3416: 0.0\n",
      "learned_steps 3417: 0.0\n",
      "learned_steps 3418: 0.0\n",
      "learned_steps 3419: 0.10000000149011612\n",
      "learned_steps 3420: 0.0\n",
      "learned_steps 3421: 0.0\n",
      "learned_steps 3422: 0.0\n",
      "learned_steps 3423: 0.0\n",
      "learned_steps 3424: 0.0\n",
      "learned_steps 3425: 0.0\n",
      "learned_steps 3426: 0.0\n",
      "learned_steps 3427: 0.0\n",
      "learned_steps 3428: 0.0\n",
      "learned_steps 3429: 0.0\n",
      "learned_steps 3430: 0.0\n",
      "learned_steps 3431: 0.0\n",
      "learned_steps 3432: 0.0\n",
      "learned_steps 3433: 0.0\n",
      "learned_steps 3434: 0.0\n",
      "learned_steps 3435: 0.0\n",
      "learned_steps 3436: 0.10000000149011612\n",
      "learned_steps 3437: 0.0\n",
      "learned_steps 3438: 0.0\n",
      "learned_steps 3439: 0.0\n",
      "learned_steps 3440: 0.0\n",
      "learned_steps 3441: 0.0\n",
      "learned_steps 3442: 0.0\n",
      "learned_steps 3443: 0.0\n",
      "learned_steps 3444: 0.0\n",
      "learned_steps 3445: 0.0\n",
      "learned_steps 3446: 0.10000000149011612\n",
      "learned_steps 3447: 0.10000000149011612\n",
      "learned_steps 3448: 0.0\n",
      "learned_steps 3449: 0.0\n",
      "learned_steps 3450: 0.0\n",
      "learned_steps 3451: 0.0\n",
      "learned_steps 3452: 0.0\n",
      "learned_steps 3453: 0.0\n",
      "learned_steps 3454: 0.0\n",
      "learned_steps 3455: 0.0\n",
      "learned_steps 3456: 0.0\n",
      "learned_steps 3457: 0.0\n",
      "learned_steps 3458: 0.0\n",
      "learned_steps 3459: 0.10000000149011612\n",
      "learned_steps 3460: 0.0\n",
      "learned_steps 3461: 0.0\n",
      "learned_steps 3462: 0.0\n",
      "learned_steps 3463: 0.0\n",
      "learned_steps 3464: 0.0\n",
      "learned_steps 3465: 0.0\n",
      "learned_steps 3466: 0.0\n",
      "learned_steps 3467: 0.0\n",
      "learned_steps 3468: 0.0\n",
      "learned_steps 3469: 0.0\n",
      "learned_steps 3470: 0.0\n",
      "learned_steps 3471: 0.0\n",
      "learned_steps 3472: 0.0\n",
      "learned_steps 3473: 0.0\n",
      "learned_steps 3474: 0.0\n",
      "learned_steps 3475: 0.0\n",
      "learned_steps 3476: 0.0\n",
      "learned_steps 3477: 0.0\n",
      "learned_steps 3478: 0.0\n",
      "learned_steps 3479: 0.0\n",
      "learned_steps 3480: 0.0\n",
      "learned_steps 3481: 0.0\n",
      "learned_steps 3482: 0.0\n",
      "learned_steps 3483: 0.0\n",
      "learned_steps 3484: 0.0\n",
      "learned_steps 3485: 0.0\n",
      "learned_steps 3486: 0.0\n",
      "learned_steps 3487: 0.0\n",
      "learned_steps 3488: 0.0\n",
      "learned_steps 3489: 0.0\n",
      "learned_steps 3490: 0.10000000149011612\n",
      "learned_steps 3491: 0.0\n",
      "learned_steps 3492: 0.0\n",
      "learned_steps 3493: 0.0\n",
      "learned_steps 3494: 0.0\n",
      "learned_steps 3495: 0.0\n",
      "learned_steps 3496: 0.0\n",
      "learned_steps 3497: 0.0\n",
      "learned_steps 3498: 0.0\n",
      "learned_steps 3499: 0.0\n",
      "learned_steps 3500: 0.0\n",
      "learned_steps 3501: 0.0\n",
      "learned_steps 3502: 0.0\n",
      "learned_steps 3503: 0.0\n",
      "learned_steps 3504: 0.0\n",
      "learned_steps 3505: 0.0\n",
      "learned_steps 3506: 0.0\n",
      "learned_steps 3507: 0.10000000149011612\n",
      "learned_steps 3508: 0.0\n",
      "learned_steps 3509: 0.0\n",
      "learned_steps 3510: 0.0\n",
      "learned_steps 3511: 0.10000000149011612\n",
      "learned_steps 3512: 0.0\n",
      "learned_steps 3513: 0.0\n",
      "learned_steps 3514: 0.10000000149011612\n",
      "learned_steps 3515: 0.0\n",
      "learned_steps 3516: 0.10000000149011612\n",
      "learned_steps 3517: 0.0\n",
      "learned_steps 3518: 0.0\n",
      "learned_steps 3519: 0.0\n",
      "learned_steps 3520: 0.10000000149011612\n",
      "learned_steps 3521: 0.0\n",
      "learned_steps 3522: 0.0\n",
      "learned_steps 3523: 0.0\n",
      "learned_steps 3524: 0.0\n",
      "learned_steps 3525: 0.0\n",
      "learned_steps 3526: 0.10000000149011612\n",
      "learned_steps 3527: 0.0\n",
      "learned_steps 3528: 0.10000000149011612\n",
      "learned_steps 3529: 0.0\n",
      "learned_steps 3530: 0.0\n",
      "learned_steps 3531: 0.10000000149011612\n",
      "learned_steps 3532: 0.0\n",
      "learned_steps 3533: 0.0\n",
      "learned_steps 3534: 0.0\n",
      "learned_steps 3535: 0.0\n",
      "learned_steps 3536: 0.0\n",
      "learned_steps 3537: 0.0\n",
      "learned_steps 3538: 0.0\n",
      "learned_steps 3539: 0.0\n",
      "learned_steps 3540: 0.0\n",
      "learned_steps 3541: 0.0\n",
      "learned_steps 3542: 0.0\n",
      "learned_steps 3543: 0.0\n",
      "learned_steps 3544: 0.0\n",
      "learned_steps 3545: 0.0\n",
      "learned_steps 3546: 0.0\n",
      "learned_steps 3547: 0.0\n",
      "learned_steps 3548: 0.0\n",
      "learned_steps 3549: 0.0\n",
      "learned_steps 3550: 0.0\n",
      "learned_steps 3551: 0.0\n",
      "learned_steps 3552: 0.0\n",
      "learned_steps 3553: 0.0\n",
      "learned_steps 3554: 0.0\n",
      "learned_steps 3555: 0.0\n",
      "learned_steps 3556: 0.0\n",
      "learned_steps 3557: 0.0\n",
      "learned_steps 3558: 0.0\n",
      "learned_steps 3559: 0.0\n",
      "learned_steps 3560: 0.0\n",
      "learned_steps 3561: 0.0\n",
      "learned_steps 3562: 0.0\n",
      "learned_steps 3563: 0.0\n",
      "learned_steps 3564: 0.0\n",
      "learned_steps 3565: 0.0\n",
      "learned_steps 3566: 0.0\n",
      "learned_steps 3567: 0.0\n",
      "learned_steps 3568: 0.0\n",
      "learned_steps 3569: 0.0\n",
      "learned_steps 3570: 0.0\n",
      "learned_steps 3571: 0.10000000149011612\n",
      "learned_steps 3572: 0.0\n",
      "learned_steps 3573: 0.0\n",
      "learned_steps 3574: 0.0\n",
      "learned_steps 3575: 0.0\n",
      "learned_steps 3576: 0.0\n",
      "learned_steps 3577: 0.0\n",
      "learned_steps 3578: 0.0\n",
      "learned_steps 3579: 0.0\n",
      "learned_steps 3580: 0.0\n",
      "learned_steps 3581: 0.0\n",
      "learned_steps 3582: 0.0\n",
      "learned_steps 3583: 0.0\n",
      "learned_steps 3584: 0.0\n",
      "learned_steps 3585: 0.10000000149011612\n",
      "learned_steps 3586: 0.0\n",
      "learned_steps 3587: 0.0\n",
      "learned_steps 3588: 0.0\n",
      "learned_steps 3589: 0.0\n",
      "learned_steps 3590: 0.0\n",
      "learned_steps 3591: 0.0\n",
      "learned_steps 3592: 0.0\n",
      "learned_steps 3593: 0.0\n",
      "learned_steps 3594: 0.0\n",
      "learned_steps 3595: 0.0\n",
      "learned_steps 3596: 0.0\n",
      "learned_steps 3597: 0.0\n",
      "learned_steps 3598: 0.0\n",
      "learned_steps 3599: 0.0\n",
      "learned_steps 3600: 0.10000000149011612\n",
      "learned_steps 3601: 0.0\n",
      "learned_steps 3602: 0.0\n",
      "learned_steps 3603: 0.0\n",
      "learned_steps 3604: 0.0\n",
      "learned_steps 3605: 0.0\n",
      "learned_steps 3606: 0.0\n",
      "learned_steps 3607: 0.0\n",
      "learned_steps 3608: 0.0\n",
      "learned_steps 3609: 0.0\n",
      "learned_steps 3610: 0.0\n",
      "learned_steps 3611: 0.0\n",
      "learned_steps 3612: 0.0\n",
      "learned_steps 3613: 0.0\n",
      "learned_steps 3614: 0.0\n",
      "learned_steps 3615: 0.0\n",
      "learned_steps 3616: 0.0\n",
      "learned_steps 3617: 0.0\n",
      "learned_steps 3618: 0.0\n",
      "learned_steps 3619: 0.0\n",
      "learned_steps 3620: 0.0\n",
      "learned_steps 3621: 0.0\n",
      "learned_steps 3622: 0.0\n",
      "learned_steps 3623: 0.0\n",
      "learned_steps 3624: 0.0\n",
      "learned_steps 3625: 0.0\n",
      "learned_steps 3626: 0.0\n",
      "learned_steps 3627: 0.0\n",
      "learned_steps 3628: 0.0\n",
      "learned_steps 3629: 0.0\n",
      "learned_steps 3630: 0.0\n",
      "learned_steps 3631: 0.10000000149011612\n",
      "learned_steps 3632: 0.0\n",
      "learned_steps 3633: 0.0\n",
      "learned_steps 3634: 0.0\n",
      "learned_steps 3635: 0.10000000149011612\n",
      "learned_steps 3636: 0.0\n",
      "learned_steps 3637: 0.0\n",
      "learned_steps 3638: 0.0\n",
      "learned_steps 3639: 0.0\n",
      "learned_steps 3640: 0.0\n",
      "learned_steps 3641: 0.0\n",
      "learned_steps 3642: 0.0\n",
      "learned_steps 3643: 0.0\n",
      "learned_steps 3644: 0.0\n",
      "learned_steps 3645: 0.0\n",
      "learned_steps 3646: 0.0\n",
      "learned_steps 3647: 0.0\n",
      "learned_steps 3648: 0.0\n",
      "learned_steps 3649: 0.0\n",
      "learned_steps 3650: 0.0\n",
      "learned_steps 3651: 0.0\n",
      "learned_steps 3652: 0.0\n",
      "learned_steps 3653: 0.0\n",
      "learned_steps 3654: 0.0\n",
      "learned_steps 3655: 0.0\n",
      "learned_steps 3656: 0.0\n",
      "learned_steps 3657: 0.0\n",
      "learned_steps 3658: 0.0\n",
      "learned_steps 3659: 0.0\n",
      "learned_steps 3660: 0.0\n",
      "learned_steps 3661: 0.0\n",
      "learned_steps 3662: 0.0\n",
      "learned_steps 3663: 0.0\n",
      "learned_steps 3664: 0.0\n",
      "learned_steps 3665: 0.0\n",
      "learned_steps 3666: 0.0\n",
      "learned_steps 3667: 0.0\n",
      "learned_steps 3668: 0.0\n",
      "learned_steps 3669: 0.0\n",
      "learned_steps 3670: 0.0\n",
      "learned_steps 3671: 0.0\n",
      "learned_steps 3672: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned_steps 3673: 0.0\n",
      "learned_steps 3674: 0.0\n",
      "learned_steps 3675: 0.0\n",
      "learned_steps 3676: 0.0\n",
      "learned_steps 3677: 0.0\n",
      "learned_steps 3678: 0.0\n",
      "learned_steps 3679: 0.0\n",
      "learned_steps 3680: 0.0\n",
      "learned_steps 3681: 0.0\n",
      "learned_steps 3682: 0.0\n",
      "learned_steps 3683: 0.0\n",
      "learned_steps 3684: 0.0\n",
      "learned_steps 3685: 0.0\n",
      "learned_steps 3686: 0.0\n",
      "learned_steps 3687: 0.0\n",
      "learned_steps 3688: 0.10000000149011612\n",
      "learned_steps 3689: 0.0\n",
      "learned_steps 3690: 0.0\n",
      "learned_steps 3691: 0.0\n",
      "learned_steps 3692: 0.0\n",
      "learned_steps 3693: 0.0\n",
      "learned_steps 3694: 0.0\n",
      "learned_steps 3695: 0.0\n",
      "learned_steps 3696: 0.0\n",
      "learned_steps 3697: 0.0\n",
      "learned_steps 3698: 0.0\n",
      "learned_steps 3699: 0.0\n",
      "learned_steps 3700: 0.0\n",
      "learned_steps 3701: 0.0\n",
      "learned_steps 3702: 0.0\n",
      "learned_steps 3703: 0.0\n",
      "learned_steps 3704: 0.0\n",
      "learned_steps 3705: 0.0\n",
      "learned_steps 3706: 0.0\n",
      "learned_steps 3707: 0.0\n",
      "learned_steps 3708: 0.0\n",
      "learned_steps 3709: 0.0\n",
      "learned_steps 3710: 0.0\n",
      "learned_steps 3711: 0.0\n",
      "learned_steps 3712: 0.0\n",
      "learned_steps 3713: 0.0\n",
      "learned_steps 3714: 0.0\n",
      "learned_steps 3715: 0.0\n",
      "learned_steps 3716: 0.0\n",
      "learned_steps 3717: 0.0\n",
      "learned_steps 3718: 0.0\n",
      "learned_steps 3719: 0.0\n",
      "learned_steps 3720: 0.0\n",
      "learned_steps 3721: 0.0\n",
      "learned_steps 3722: 0.0\n",
      "learned_steps 3723: 0.0\n",
      "learned_steps 3724: 0.0\n",
      "learned_steps 3725: 0.0\n",
      "learned_steps 3726: 0.0\n",
      "learned_steps 3727: 0.0\n",
      "learned_steps 3728: 0.0\n",
      "learned_steps 3729: 0.0\n",
      "learned_steps 3730: 0.0\n",
      "learned_steps 3731: 0.0\n",
      "learned_steps 3732: 0.0\n",
      "learned_steps 3733: 0.0\n",
      "learned_steps 3734: 0.0\n",
      "learned_steps 3735: 0.0\n",
      "learned_steps 3736: 0.0\n",
      "learned_steps 3737: 0.0\n",
      "learned_steps 3738: 0.0\n",
      "learned_steps 3739: 0.0\n",
      "learned_steps 3740: 0.0\n",
      "learned_steps 3741: 0.0\n",
      "learned_steps 3742: 0.0\n",
      "learned_steps 3743: 0.0\n",
      "learned_steps 3744: 0.0\n",
      "learned_steps 3745: 0.0\n",
      "learned_steps 3746: 0.0\n",
      "learned_steps 3747: 0.09000000357627869\n",
      "learned_steps 3748: 0.0\n",
      "learned_steps 3749: 0.0\n",
      "learned_steps 3750: 0.0\n",
      "learned_steps 3751: 0.0\n",
      "learned_steps 3752: 0.0\n",
      "learned_steps 3753: 0.0\n",
      "learned_steps 3754: 0.0\n",
      "learned_steps 3755: 0.0\n",
      "learned_steps 3756: 0.0\n",
      "learned_steps 3757: 0.0\n",
      "learned_steps 3758: 0.0\n",
      "learned_steps 3759: 0.0\n",
      "learned_steps 3760: 0.0\n",
      "learned_steps 3761: 0.0\n",
      "learned_steps 3762: 0.0\n",
      "learned_steps 3763: 0.0\n",
      "learned_steps 3764: 0.0\n",
      "learned_steps 3765: 0.0\n",
      "learned_steps 3766: 0.0\n",
      "learned_steps 3767: 0.0\n",
      "learned_steps 3768: 0.0\n",
      "learned_steps 3769: 0.0\n",
      "learned_steps 3770: 0.0\n",
      "learned_steps 3771: 0.0\n",
      "learned_steps 3772: 0.0\n",
      "learned_steps 3773: 0.0\n",
      "learned_steps 3774: 0.0\n",
      "learned_steps 3775: 0.0\n",
      "learned_steps 3776: 0.0\n",
      "learned_steps 3777: 0.0\n",
      "learned_steps 3778: 0.0\n",
      "learned_steps 3779: 0.0\n",
      "learned_steps 3780: 0.0\n",
      "learned_steps 3781: 0.0\n",
      "learned_steps 3782: 0.0\n",
      "learned_steps 3783: 0.0\n",
      "learned_steps 3784: 0.10000000149011612\n",
      "learned_steps 3785: 0.0\n",
      "learned_steps 3786: 0.0\n",
      "learned_steps 3787: 0.10000000149011612\n",
      "learned_steps 3788: 0.0\n",
      "learned_steps 3789: 0.0\n",
      "learned_steps 3790: 0.0\n",
      "learned_steps 3791: 0.0\n",
      "learned_steps 3792: 0.0\n",
      "learned_steps 3793: 0.0\n",
      "learned_steps 3794: 0.0\n",
      "learned_steps 3795: 0.0\n",
      "learned_steps 3796: 0.0\n",
      "learned_steps 3797: 0.0\n",
      "learned_steps 3798: 0.0\n",
      "learned_steps 3799: 0.10000000149011612\n",
      "learned_steps 3800: 0.0\n",
      "learned_steps 3801: 0.0\n",
      "learned_steps 3802: 0.0\n",
      "learned_steps 3803: 0.0\n",
      "learned_steps 3804: 0.0\n",
      "learned_steps 3805: 0.0\n",
      "learned_steps 3806: 0.0\n",
      "learned_steps 3807: 0.0\n",
      "learned_steps 3808: 0.0\n",
      "learned_steps 3809: 0.0\n",
      "learned_steps 3810: 0.0\n",
      "learned_steps 3811: 0.0\n",
      "learned_steps 3812: 0.10000000149011612\n",
      "learned_steps 3813: 0.0\n",
      "learned_steps 3814: 0.0\n",
      "learned_steps 3815: 0.0\n",
      "learned_steps 3816: 0.0\n",
      "learned_steps 3817: 0.0\n",
      "learned_steps 3818: 0.0\n",
      "learned_steps 3819: 0.0\n",
      "learned_steps 3820: 0.0\n",
      "learned_steps 3821: 0.0\n",
      "learned_steps 3822: 0.0\n",
      "learned_steps 3823: 0.0\n",
      "learned_steps 3824: 0.0\n",
      "learned_steps 3825: 0.0\n",
      "learned_steps 3826: 0.0\n",
      "learned_steps 3827: 0.0\n",
      "learned_steps 3828: 0.0\n",
      "learned_steps 3829: 0.0\n",
      "learned_steps 3830: 0.0\n",
      "learned_steps 3831: 0.0\n",
      "learned_steps 3832: 0.0\n",
      "learned_steps 3833: 0.0\n",
      "learned_steps 3834: 0.0\n",
      "learned_steps 3835: 0.0\n",
      "learned_steps 3836: 0.0\n",
      "learned_steps 3837: 0.0\n",
      "learned_steps 3838: 0.0\n",
      "learned_steps 3839: 0.0\n",
      "learned_steps 3840: 0.0\n",
      "learned_steps 3841: 0.0\n",
      "learned_steps 3842: 0.0\n",
      "learned_steps 3843: 0.0\n",
      "learned_steps 3844: 0.0\n",
      "learned_steps 3845: 0.0\n",
      "learned_steps 3846: 0.0\n",
      "learned_steps 3847: 0.0\n",
      "learned_steps 3848: 0.0\n",
      "learned_steps 3849: 0.0\n",
      "learned_steps 3850: 0.0\n",
      "learned_steps 3851: 0.0\n",
      "learned_steps 3852: 0.0\n",
      "learned_steps 3853: 0.0\n",
      "learned_steps 3854: 0.0\n",
      "learned_steps 3855: 0.0\n",
      "learned_steps 3856: 0.0\n",
      "learned_steps 3857: 0.0\n",
      "learned_steps 3858: 0.0\n",
      "learned_steps 3859: 0.0\n",
      "learned_steps 3860: 0.0\n",
      "learned_steps 3861: 0.0\n",
      "learned_steps 3862: 0.0\n",
      "learned_steps 3863: 0.0\n",
      "learned_steps 3864: 0.0\n",
      "learned_steps 3865: 0.0\n",
      "learned_steps 3866: 0.0\n",
      "learned_steps 3867: 0.0\n",
      "learned_steps 3868: 0.0\n",
      "learned_steps 3869: 0.0\n",
      "learned_steps 3870: 0.0\n",
      "learned_steps 3871: 0.0\n",
      "learned_steps 3872: 0.0\n",
      "learned_steps 3873: 0.0\n",
      "learned_steps 3874: 0.0\n",
      "learned_steps 3875: 0.0\n",
      "learned_steps 3876: 0.0\n",
      "learned_steps 3877: 0.0\n",
      "learned_steps 3878: 0.0\n",
      "learned_steps 3879: 0.0\n",
      "learned_steps 3880: 0.0\n",
      "learned_steps 3881: 0.0\n",
      "learned_steps 3882: 0.0\n",
      "learned_steps 3883: 0.0\n",
      "learned_steps 3884: 0.0\n",
      "learned_steps 3885: 0.0\n",
      "learned_steps 3886: 0.0\n",
      "learned_steps 3887: 0.0\n",
      "learned_steps 3888: 0.0\n",
      "learned_steps 3889: 0.0\n",
      "learned_steps 3890: 0.0\n",
      "learned_steps 3891: 0.0\n",
      "learned_steps 3892: 0.0\n",
      "learned_steps 3893: 0.0\n",
      "learned_steps 3894: 0.0\n",
      "learned_steps 3895: 0.0\n",
      "learned_steps 3896: 0.0\n",
      "learned_steps 3897: 0.0\n",
      "learned_steps 3898: 0.0\n",
      "learned_steps 3899: 0.0\n",
      "learned_steps 3900: 0.0\n",
      "learned_steps 3901: 0.0\n",
      "learned_steps 3902: 0.0\n",
      "learned_steps 3903: 0.0\n",
      "learned_steps 3904: 0.0\n",
      "learned_steps 3905: 0.0\n",
      "learned_steps 3906: 0.0\n",
      "learned_steps 3907: 0.0\n",
      "learned_steps 3908: 0.0\n",
      "learned_steps 3909: 0.0\n",
      "learned_steps 3910: 0.0\n",
      "learned_steps 3911: 0.10000000149011612\n",
      "learned_steps 3912: 0.10000000149011612\n",
      "learned_steps 3913: 0.0\n",
      "learned_steps 3914: 0.10000000149011612\n",
      "learned_steps 3915: 0.0\n",
      "learned_steps 3916: 0.0\n",
      "learned_steps 3917: 0.0\n",
      "learned_steps 3918: 0.0\n",
      "learned_steps 3919: 0.0\n",
      "learned_steps 3920: 0.0\n",
      "learned_steps 3921: 0.0\n",
      "learned_steps 3922: 0.0\n",
      "learned_steps 3923: 0.0\n",
      "learned_steps 3924: 0.0\n",
      "learned_steps 3925: 0.10000000149011612\n",
      "learned_steps 3926: 0.0\n",
      "learned_steps 3927: 0.0\n",
      "learned_steps 3928: 0.0\n",
      "learned_steps 3929: 0.0\n",
      "learned_steps 3930: 0.10000000149011612\n",
      "learned_steps 3931: 0.0\n",
      "learned_steps 3932: 0.0\n",
      "learned_steps 3933: 0.0\n",
      "learned_steps 3934: 0.0\n",
      "learned_steps 3935: 0.0\n",
      "learned_steps 3936: 0.0\n",
      "learned_steps 3937: 0.0\n",
      "learned_steps 3938: 0.0\n",
      "learned_steps 3939: 0.0\n",
      "learned_steps 3940: 0.0\n",
      "learned_steps 3941: 0.0\n",
      "learned_steps 3942: 0.0\n",
      "learned_steps 3943: 0.0\n",
      "learned_steps 3944: 0.0\n",
      "learned_steps 3945: 0.0\n",
      "learned_steps 3946: 0.0\n",
      "learned_steps 3947: 0.0\n",
      "learned_steps 3948: 0.0\n",
      "learned_steps 3949: 0.0\n",
      "learned_steps 3950: 0.0\n",
      "learned_steps 3951: 0.0\n",
      "learned_steps 3952: 0.0\n",
      "learned_steps 3953: 0.0\n",
      "learned_steps 3954: 0.0\n",
      "learned_steps 3955: 0.10000000149011612\n",
      "learned_steps 3956: 0.0\n",
      "learned_steps 3957: 0.0\n",
      "learned_steps 3958: 0.0\n",
      "learned_steps 3959: 0.0\n",
      "learned_steps 3960: 0.0\n",
      "learned_steps 3961: 0.0\n",
      "learned_steps 3962: 0.0\n",
      "learned_steps 3963: 0.0\n",
      "learned_steps 3964: 0.0\n",
      "learned_steps 3965: 0.10000000149011612\n",
      "learned_steps 3966: 0.0\n",
      "learned_steps 3967: 0.0\n",
      "learned_steps 3968: 0.0\n",
      "learned_steps 3969: 0.0\n",
      "learned_steps 3970: 0.0\n",
      "learned_steps 3971: 0.0\n",
      "learned_steps 3972: 0.10000000149011612\n",
      "learned_steps 3973: 0.10000000149011612\n",
      "learned_steps 3974: 0.0\n",
      "learned_steps 3975: 0.10000000149011612\n",
      "learned_steps 3976: 0.0\n",
      "learned_steps 3977: 0.0\n",
      "learned_steps 3978: 0.0\n",
      "learned_steps 3979: 0.0\n",
      "learned_steps 3980: 0.0\n",
      "learned_steps 3981: 0.0\n",
      "learned_steps 3982: 0.0\n",
      "learned_steps 3983: 0.0\n",
      "learned_steps 3984: 0.0\n",
      "learned_steps 3985: 0.0\n",
      "learned_steps 3986: 0.0\n",
      "learned_steps 3987: 0.0\n",
      "learned_steps 3988: 0.0\n",
      "learned_steps 3989: 0.0\n",
      "learned_steps 3990: 0.0\n",
      "learned_steps 3991: 0.0\n",
      "learned_steps 3992: 0.0\n",
      "learned_steps 3993: 0.0\n",
      "learned_steps 3994: 0.0\n",
      "learned_steps 3995: 0.10000000149011612\n",
      "learned_steps 3996: 0.0\n",
      "learned_steps 3997: 0.0\n",
      "learned_steps 3998: 0.10000000149011612\n",
      "learned_steps 3999: 0.0\n",
      "learned_steps 4000: 0.0\n",
      "learned_steps 4001: 0.0\n",
      "learned_steps 4002: 0.10000000149011612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned_steps 4003: 0.0\n",
      "learned_steps 4004: 0.0\n",
      "learned_steps 4005: 0.0\n",
      "learned_steps 4006: 0.0\n",
      "learned_steps 4007: 0.0\n",
      "learned_steps 4008: 0.0\n",
      "learned_steps 4009: 0.0\n",
      "learned_steps 4010: 0.0\n",
      "learned_steps 4011: 0.0\n",
      "learned_steps 4012: 0.0\n",
      "learned_steps 4013: 0.10000000149011612\n",
      "learned_steps 4014: 0.0\n",
      "learned_steps 4015: 0.0\n",
      "learned_steps 4016: 0.0\n",
      "learned_steps 4017: 0.0\n",
      "learned_steps 4018: 0.0\n",
      "learned_steps 4019: 0.0\n",
      "learned_steps 4020: 0.0\n",
      "learned_steps 4021: 0.0\n",
      "learned_steps 4022: 0.0\n",
      "learned_steps 4023: 0.0\n",
      "learned_steps 4024: 0.0\n",
      "learned_steps 4025: 0.0\n",
      "learned_steps 4026: 0.0\n",
      "learned_steps 4027: 0.0\n",
      "learned_steps 4028: 0.0\n",
      "learned_steps 4029: 0.0\n",
      "learned_steps 4030: 0.10000000149011612\n",
      "learned_steps 4031: 0.0\n",
      "learned_steps 4032: 0.0\n",
      "learned_steps 4033: 0.0\n",
      "learned_steps 4034: 0.0\n",
      "learned_steps 4035: 0.0\n",
      "learned_steps 4036: 0.0\n",
      "learned_steps 4037: 0.10000000149011612\n",
      "learned_steps 4038: 0.0\n",
      "learned_steps 4039: 0.0\n",
      "learned_steps 4040: 0.0\n",
      "learned_steps 4041: 0.0\n",
      "learned_steps 4042: 0.0\n",
      "learned_steps 4043: 0.0\n",
      "learned_steps 4044: 0.0\n",
      "learned_steps 4045: 0.0\n",
      "learned_steps 4046: 0.0\n",
      "learned_steps 4047: 0.0\n",
      "learned_steps 4048: 0.0\n",
      "learned_steps 4049: 0.10000000149011612\n",
      "learned_steps 4050: 0.0\n",
      "learned_steps 4051: 0.0\n",
      "learned_steps 4052: 0.10000000149011612\n",
      "learned_steps 4053: 0.0\n",
      "learned_steps 4054: 0.0\n",
      "learned_steps 4055: 0.0\n",
      "learned_steps 4056: 0.0\n",
      "learned_steps 4057: 0.0\n",
      "learned_steps 4058: 0.0\n",
      "learned_steps 4059: 0.0\n",
      "learned_steps 4060: 0.0\n",
      "learned_steps 4061: 0.0\n",
      "learned_steps 4062: 0.0\n",
      "learned_steps 4063: 0.0\n",
      "learned_steps 4064: 0.0\n",
      "learned_steps 4065: 0.0\n",
      "learned_steps 4066: 0.0\n",
      "learned_steps 4067: 0.0\n",
      "learned_steps 4068: 0.0\n",
      "learned_steps 4069: 0.0\n",
      "learned_steps 4070: 0.10000000149011612\n",
      "learned_steps 4071: 0.0\n",
      "learned_steps 4072: 0.0\n",
      "learned_steps 4073: 0.0\n",
      "learned_steps 4074: 0.0\n",
      "learned_steps 4075: 0.0\n",
      "learned_steps 4076: 0.0\n",
      "learned_steps 4077: 0.10000000149011612\n",
      "learned_steps 4078: 0.0\n",
      "learned_steps 4079: 0.0\n",
      "learned_steps 4080: 0.0\n",
      "learned_steps 4081: 0.0\n",
      "learned_steps 4082: 0.0\n",
      "learned_steps 4083: 0.0\n",
      "learned_steps 4084: 0.0\n",
      "learned_steps 4085: 0.0\n",
      "learned_steps 4086: 0.0\n",
      "learned_steps 4087: 0.0\n",
      "learned_steps 4088: 0.0\n",
      "learned_steps 4089: 0.0\n",
      "learned_steps 4090: 0.0\n",
      "learned_steps 4091: 0.0\n",
      "learned_steps 4092: 0.10000000149011612\n",
      "learned_steps 4093: 0.0\n",
      "learned_steps 4094: 0.0\n",
      "learned_steps 4095: 0.0\n",
      "learned_steps 4096: 0.0\n",
      "learned_steps 4097: 0.0\n",
      "learned_steps 4098: 0.0\n",
      "learned_steps 4099: 0.0\n",
      "learned_steps 4100: 0.0\n",
      "learned_steps 4101: 0.0\n",
      "learned_steps 4102: 0.0\n",
      "learned_steps 4103: 0.0\n",
      "learned_steps 4104: 0.0\n",
      "learned_steps 4105: 0.0\n",
      "learned_steps 4106: 0.0\n",
      "learned_steps 4107: 0.0\n",
      "learned_steps 4108: 0.0\n",
      "learned_steps 4109: 0.0\n",
      "learned_steps 4110: 0.0\n",
      "learned_steps 4111: 0.0\n",
      "learned_steps 4112: 0.0\n",
      "learned_steps 4113: 0.0\n",
      "learned_steps 4114: 0.0\n",
      "learned_steps 4115: 0.0\n",
      "learned_steps 4116: 0.0\n",
      "learned_steps 4117: 0.0\n",
      "learned_steps 4118: 0.0\n",
      "learned_steps 4119: 0.0\n",
      "learned_steps 4120: 0.0\n",
      "learned_steps 4121: 0.0\n",
      "learned_steps 4122: 0.10000000149011612\n",
      "learned_steps 4123: 0.0\n",
      "learned_steps 4124: 0.0\n",
      "learned_steps 4125: 0.0\n",
      "learned_steps 4126: 0.0\n",
      "learned_steps 4127: 0.0\n",
      "learned_steps 4128: 0.0\n",
      "learned_steps 4129: 0.0\n",
      "learned_steps 4130: 0.0\n",
      "learned_steps 4131: 0.0\n",
      "learned_steps 4132: 0.0\n",
      "learned_steps 4133: 0.0\n",
      "learned_steps 4134: 0.0\n",
      "learned_steps 4135: 0.10000000149011612\n",
      "learned_steps 4136: 0.0\n",
      "learned_steps 4137: 0.0\n",
      "learned_steps 4138: 0.0\n",
      "learned_steps 4139: 0.0\n",
      "learned_steps 4140: 0.0\n",
      "learned_steps 4141: 0.0\n",
      "learned_steps 4142: 0.0\n",
      "learned_steps 4143: 0.0\n",
      "learned_steps 4144: 0.0\n",
      "learned_steps 4145: 0.0\n",
      "learned_steps 4146: 0.0\n",
      "learned_steps 4147: 0.0\n",
      "learned_steps 4148: 0.0\n",
      "learned_steps 4149: 0.0\n",
      "learned_steps 4150: 0.0\n",
      "learned_steps 4151: 0.0\n",
      "learned_steps 4152: 0.0\n",
      "learned_steps 4153: 0.0\n",
      "learned_steps 4154: 0.0\n",
      "learned_steps 4155: 0.0\n",
      "learned_steps 4156: 0.0\n",
      "learned_steps 4157: 0.0\n",
      "learned_steps 4158: 0.0\n",
      "learned_steps 4159: 0.0\n",
      "learned_steps 4160: 0.0\n",
      "learned_steps 4161: 0.0\n",
      "learned_steps 4162: 0.10000000149011612\n",
      "learned_steps 4163: 0.0\n",
      "learned_steps 4164: 0.0\n",
      "learned_steps 4165: 0.0\n",
      "learned_steps 4166: 0.0\n",
      "learned_steps 4167: 0.0\n",
      "learned_steps 4168: 0.0\n",
      "learned_steps 4169: 0.0\n",
      "learned_steps 4170: 0.0\n",
      "learned_steps 4171: 0.0\n",
      "learned_steps 4172: 0.0\n",
      "learned_steps 4173: 0.0\n",
      "learned_steps 4174: 0.0\n",
      "learned_steps 4175: 0.0\n",
      "learned_steps 4176: 0.0\n",
      "learned_steps 4177: 0.0\n",
      "learned_steps 4178: 0.0\n",
      "learned_steps 4179: 0.0\n",
      "learned_steps 4180: 0.0\n",
      "learned_steps 4181: 0.0\n",
      "learned_steps 4182: 0.0\n",
      "learned_steps 4183: 0.0\n",
      "learned_steps 4184: 0.0\n",
      "learned_steps 4185: 0.0\n",
      "learned_steps 4186: 0.0\n",
      "learned_steps 4187: 0.0\n",
      "learned_steps 4188: 0.0\n",
      "learned_steps 4189: 0.0\n",
      "learned_steps 4190: 0.0\n",
      "learned_steps 4191: 0.0\n",
      "learned_steps 4192: 0.0\n",
      "learned_steps 4193: 0.0\n",
      "learned_steps 4194: 0.10000000149011612\n",
      "learned_steps 4195: 0.10000000149011612\n",
      "learned_steps 4196: 0.0\n",
      "learned_steps 4197: 0.10000000149011612\n",
      "learned_steps 4198: 0.0\n",
      "learned_steps 4199: 0.0\n",
      "learned_steps 4200: 0.0\n",
      "learned_steps 4201: 0.0\n",
      "learned_steps 4202: 0.0\n",
      "learned_steps 4203: 0.0\n",
      "learned_steps 4204: 0.0\n",
      "learned_steps 4205: 0.0\n",
      "learned_steps 4206: 0.10000000149011612\n",
      "learned_steps 4207: 0.0\n",
      "learned_steps 4208: 0.10000000149011612\n",
      "learned_steps 4209: 0.0\n",
      "learned_steps 4210: 0.0\n",
      "learned_steps 4211: 0.0\n",
      "learned_steps 4212: 0.0\n",
      "learned_steps 4213: 0.0\n",
      "learned_steps 4214: 0.10000000149011612\n",
      "learned_steps 4215: 0.0\n",
      "learned_steps 4216: 0.0\n",
      "learned_steps 4217: 0.0\n",
      "learned_steps 4218: 0.0\n",
      "learned_steps 4219: 0.0\n",
      "learned_steps 4220: 0.0\n",
      "learned_steps 4221: 0.0\n",
      "learned_steps 4222: 0.0\n",
      "learned_steps 4223: 0.0\n",
      "learned_steps 4224: 0.0\n",
      "learned_steps 4225: 0.0\n",
      "learned_steps 4226: 0.0\n",
      "learned_steps 4227: 0.0\n",
      "learned_steps 4228: 0.10000000149011612\n",
      "learned_steps 4229: 0.0\n",
      "learned_steps 4230: 0.0\n",
      "learned_steps 4231: 0.10000000149011612\n",
      "learned_steps 4232: 0.0\n",
      "learned_steps 4233: 0.0\n",
      "learned_steps 4234: 0.0\n",
      "learned_steps 4235: 0.0\n",
      "learned_steps 4236: 0.0\n",
      "learned_steps 4237: 0.0\n",
      "learned_steps 4238: 0.0\n",
      "learned_steps 4239: 0.0\n",
      "learned_steps 4240: 0.0\n",
      "learned_steps 4241: 0.0\n",
      "learned_steps 4242: 0.0\n",
      "learned_steps 4243: 0.0\n",
      "learned_steps 4244: 0.0\n",
      "learned_steps 4245: 0.0\n",
      "learned_steps 4246: 0.0\n",
      "learned_steps 4247: 0.0\n",
      "learned_steps 4248: 0.0\n",
      "learned_steps 4249: 0.0\n",
      "learned_steps 4250: 0.0\n",
      "learned_steps 4251: 0.0\n",
      "learned_steps 4252: 0.0\n",
      "learned_steps 4253: 0.0\n",
      "learned_steps 4254: 0.0\n",
      "learned_steps 4255: 0.0\n",
      "learned_steps 4256: 0.0\n",
      "learned_steps 4257: 0.0\n",
      "learned_steps 4258: 0.0\n",
      "learned_steps 4259: 0.0\n",
      "learned_steps 4260: 0.0\n",
      "learned_steps 4261: 0.0\n",
      "learned_steps 4262: 0.0\n",
      "learned_steps 4263: 0.0\n",
      "learned_steps 4264: 0.0\n",
      "learned_steps 4265: 0.0\n",
      "learned_steps 4266: 0.0\n",
      "learned_steps 4267: 0.0\n",
      "learned_steps 4268: 0.0\n",
      "learned_steps 4269: 0.0\n",
      "learned_steps 4270: 0.0\n",
      "learned_steps 4271: 0.0\n",
      "learned_steps 4272: 0.0\n",
      "learned_steps 4273: 0.0\n",
      "learned_steps 4274: 0.10000000149011612\n",
      "learned_steps 4275: 0.0\n",
      "learned_steps 4276: 0.0\n",
      "learned_steps 4277: 0.0\n",
      "learned_steps 4278: 0.0\n",
      "learned_steps 4279: 0.0\n",
      "learned_steps 4280: 0.0\n",
      "learned_steps 4281: 0.0\n",
      "learned_steps 4282: 0.0\n",
      "learned_steps 4283: 0.0\n",
      "learned_steps 4284: 0.0\n",
      "learned_steps 4285: 0.0\n",
      "learned_steps 4286: 0.10000000149011612\n",
      "learned_steps 4287: 0.0\n",
      "learned_steps 4288: 0.0\n",
      "learned_steps 4289: 0.0\n",
      "learned_steps 4290: 0.0\n",
      "learned_steps 4291: 0.0\n",
      "learned_steps 4292: 0.0\n",
      "learned_steps 4293: 0.0\n",
      "learned_steps 4294: 0.0\n",
      "learned_steps 4295: 0.0\n",
      "learned_steps 4296: 0.0\n",
      "learned_steps 4297: 0.0\n",
      "learned_steps 4298: 0.0\n",
      "learned_steps 4299: 0.0\n",
      "learned_steps 4300: 0.0\n",
      "learned_steps 4301: 0.0\n",
      "learned_steps 4302: 0.0\n",
      "learned_steps 4303: 0.0\n",
      "learned_steps 4304: 0.0\n",
      "learned_steps 4305: 0.0\n",
      "learned_steps 4306: 0.0\n",
      "learned_steps 4307: 0.0\n",
      "learned_steps 4308: 0.0\n",
      "learned_steps 4309: 0.0\n",
      "learned_steps 4310: 0.0\n",
      "learned_steps 4311: 0.0\n",
      "learned_steps 4312: 0.0\n",
      "learned_steps 4313: 0.0\n",
      "learned_steps 4314: 0.0\n",
      "learned_steps 4315: 0.0\n",
      "learned_steps 4316: 0.0\n",
      "learned_steps 4317: 0.0\n",
      "learned_steps 4318: 0.0\n",
      "learned_steps 4319: 0.0\n",
      "learned_steps 4320: 0.0\n",
      "learned_steps 4321: 0.0\n",
      "learned_steps 4322: 0.0\n",
      "learned_steps 4323: 0.0\n",
      "learned_steps 4324: 0.0\n",
      "learned_steps 4325: 0.0\n",
      "learned_steps 4326: 0.0\n",
      "learned_steps 4327: 0.0\n",
      "learned_steps 4328: 0.0\n",
      "learned_steps 4329: 0.0\n",
      "learned_steps 4330: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned_steps 4331: 0.0\n",
      "learned_steps 4332: 0.0\n",
      "learned_steps 4333: 0.0\n",
      "learned_steps 4334: 0.0\n",
      "learned_steps 4335: 0.0\n",
      "learned_steps 4336: 0.0\n",
      "learned_steps 4337: 0.0\n",
      "learned_steps 4338: 0.0\n",
      "learned_steps 4339: 0.0\n",
      "learned_steps 4340: 0.0\n",
      "learned_steps 4341: 0.10000000149011612\n",
      "learned_steps 4342: 0.0\n",
      "learned_steps 4343: 0.0\n",
      "learned_steps 4344: 0.0\n",
      "learned_steps 4345: 0.0\n",
      "learned_steps 4346: 0.0\n",
      "learned_steps 4347: 0.0\n",
      "learned_steps 4348: 0.10000000149011612\n",
      "learned_steps 4349: 0.0\n",
      "learned_steps 4350: 0.0\n",
      "learned_steps 4351: 0.0\n",
      "learned_steps 4352: 0.0\n",
      "learned_steps 4353: 0.0\n",
      "learned_steps 4354: 0.0\n",
      "learned_steps 4355: 0.10000000149011612\n",
      "learned_steps 4356: 0.10000000149011612\n",
      "learned_steps 4357: 0.0\n",
      "learned_steps 4358: 0.0\n",
      "learned_steps 4359: 0.0\n",
      "learned_steps 4360: 0.0\n",
      "learned_steps 4361: 0.0\n",
      "learned_steps 4362: 0.0\n",
      "learned_steps 4363: 0.0\n",
      "learned_steps 4364: 0.0\n",
      "learned_steps 4365: 0.0\n",
      "learned_steps 4366: 0.0\n",
      "learned_steps 4367: 0.0\n",
      "learned_steps 4368: 0.0\n",
      "learned_steps 4369: 0.0\n",
      "learned_steps 4370: 0.0\n",
      "learned_steps 4371: 0.0\n",
      "learned_steps 4372: 0.0\n",
      "learned_steps 4373: 0.0\n",
      "learned_steps 4374: 0.0\n",
      "learned_steps 4375: 0.0\n",
      "learned_steps 4376: 0.10000000149011612\n",
      "learned_steps 4377: 0.0\n",
      "learned_steps 4378: 0.0\n",
      "learned_steps 4379: 0.0\n",
      "learned_steps 4380: 0.0\n",
      "learned_steps 4381: 0.0\n",
      "learned_steps 4382: 0.0\n",
      "learned_steps 4383: 0.10000000149011612\n",
      "learned_steps 4384: 0.0\n",
      "learned_steps 4385: 0.0\n",
      "learned_steps 4386: 0.10000000149011612\n",
      "learned_steps 4387: 0.0\n",
      "learned_steps 4388: 0.0\n",
      "learned_steps 4389: 0.0\n",
      "learned_steps 4390: 0.0\n",
      "learned_steps 4391: 0.0\n",
      "learned_steps 4392: 0.0\n",
      "learned_steps 4393: 0.0\n",
      "learned_steps 4394: 0.0\n",
      "learned_steps 4395: 0.0\n",
      "learned_steps 4396: 0.0\n",
      "learned_steps 4397: 0.0\n",
      "learned_steps 4398: 0.0\n",
      "learned_steps 4399: 0.0\n",
      "learned_steps 4400: 0.0\n",
      "learned_steps 4401: 0.0\n",
      "learned_steps 4402: 0.0\n",
      "learned_steps 4403: 0.0\n",
      "learned_steps 4404: 0.0\n",
      "learned_steps 4405: 0.0\n",
      "learned_steps 4406: 0.0\n",
      "learned_steps 4407: 0.0\n",
      "learned_steps 4408: 0.0\n",
      "learned_steps 4409: 0.0\n",
      "learned_steps 4410: 0.0\n",
      "learned_steps 4411: 0.0\n",
      "learned_steps 4412: 0.0\n",
      "learned_steps 4413: 0.0\n",
      "learned_steps 4414: 0.10000000149011612\n",
      "learned_steps 4415: 0.0\n",
      "learned_steps 4416: 0.0\n",
      "learned_steps 4417: 0.0\n",
      "learned_steps 4418: 0.0\n",
      "learned_steps 4419: 0.0\n",
      "learned_steps 4420: 0.0\n",
      "learned_steps 4421: 0.0\n",
      "learned_steps 4422: 0.0\n",
      "learned_steps 4423: 0.0\n",
      "learned_steps 4424: 0.0\n",
      "learned_steps 4425: 0.0\n",
      "learned_steps 4426: 0.0\n",
      "learned_steps 4427: 0.0\n",
      "learned_steps 4428: 0.0\n",
      "learned_steps 4429: 0.0\n",
      "learned_steps 4430: 0.0\n",
      "learned_steps 4431: 0.0\n",
      "learned_steps 4432: 0.0\n",
      "learned_steps 4433: 0.0\n",
      "learned_steps 4434: 0.0\n",
      "learned_steps 4435: 0.0\n",
      "learned_steps 4436: 0.0\n",
      "learned_steps 4437: 0.0\n",
      "learned_steps 4438: 0.0\n",
      "learned_steps 4439: 0.0\n",
      "learned_steps 4440: 0.0\n",
      "learned_steps 4441: 0.0\n",
      "learned_steps 4442: 0.0\n",
      "learned_steps 4443: 0.0\n",
      "learned_steps 4444: 0.0\n",
      "learned_steps 4445: 0.0\n",
      "learned_steps 4446: 0.0\n",
      "learned_steps 4447: 0.0\n",
      "learned_steps 4448: 0.0\n",
      "learned_steps 4449: 0.0\n",
      "learned_steps 4450: 0.0\n",
      "learned_steps 4451: 0.0\n",
      "learned_steps 4452: 0.0\n",
      "learned_steps 4453: 0.0\n",
      "learned_steps 4454: 0.0\n",
      "learned_steps 4455: 0.0\n",
      "learned_steps 4456: 0.0\n",
      "learned_steps 4457: 0.0\n",
      "learned_steps 4458: 0.0\n",
      "learned_steps 4459: 0.0\n",
      "learned_steps 4460: 0.0\n",
      "learned_steps 4461: 0.0\n",
      "learned_steps 4462: 0.0\n",
      "learned_steps 4463: 0.0\n",
      "learned_steps 4464: 0.10000000149011612\n",
      "learned_steps 4465: 0.0\n",
      "learned_steps 4466: 0.0\n",
      "learned_steps 4467: 0.10000000149011612\n",
      "learned_steps 4468: 0.0\n",
      "learned_steps 4469: 0.0\n",
      "learned_steps 4470: 0.0\n",
      "learned_steps 4471: 0.0\n",
      "learned_steps 4472: 0.0\n",
      "learned_steps 4473: 0.0\n",
      "learned_steps 4474: 0.0\n",
      "learned_steps 4475: 0.0\n",
      "learned_steps 4476: 0.0\n",
      "learned_steps 4477: 0.0\n",
      "learned_steps 4478: 0.0\n",
      "learned_steps 4479: 0.0\n",
      "learned_steps 4480: 0.0\n",
      "learned_steps 4481: 0.0\n",
      "learned_steps 4482: 0.0\n",
      "learned_steps 4483: 0.0\n",
      "learned_steps 4484: 0.0\n",
      "learned_steps 4485: 0.0\n",
      "learned_steps 4486: 0.10000000149011612\n",
      "learned_steps 4487: 0.0\n",
      "learned_steps 4488: 0.0\n",
      "learned_steps 4489: 0.0\n",
      "learned_steps 4490: 0.0\n",
      "learned_steps 4491: 0.0\n",
      "learned_steps 4492: 0.0\n",
      "learned_steps 4493: 0.0\n",
      "learned_steps 4494: 0.0\n",
      "learned_steps 4495: 0.0\n",
      "learned_steps 4496: 0.0\n",
      "learned_steps 4497: 0.0\n",
      "learned_steps 4498: 0.0\n",
      "learned_steps 4499: 0.0\n",
      "learned_steps 4500: 0.0\n",
      "learned_steps 4501: 0.0\n",
      "learned_steps 4502: 0.0\n",
      "learned_steps 4503: 0.0\n",
      "learned_steps 4504: 0.0\n",
      "learned_steps 4505: 0.0\n",
      "learned_steps 4506: 0.0\n",
      "learned_steps 4507: 0.0\n",
      "learned_steps 4508: 0.0\n",
      "learned_steps 4509: 0.0\n",
      "learned_steps 4510: 0.0\n",
      "learned_steps 4511: 0.0\n",
      "learned_steps 4512: 0.0\n",
      "learned_steps 4513: 0.0\n",
      "learned_steps 4514: 0.0\n",
      "learned_steps 4515: 0.10000000149011612\n",
      "learned_steps 4516: 0.0\n",
      "learned_steps 4517: 0.0\n",
      "learned_steps 4518: 0.0\n",
      "learned_steps 4519: 0.0\n",
      "learned_steps 4520: 0.0\n",
      "learned_steps 4521: 0.0\n",
      "learned_steps 4522: 0.0\n",
      "learned_steps 4523: 0.0\n",
      "learned_steps 4524: 0.0\n",
      "learned_steps 4525: 0.0\n",
      "learned_steps 4526: 0.0\n",
      "learned_steps 4527: 0.0\n",
      "learned_steps 4528: 0.0\n",
      "learned_steps 4529: 0.0\n",
      "learned_steps 4530: 0.0\n",
      "learned_steps 4531: 0.0\n",
      "learned_steps 4532: 0.0\n",
      "learned_steps 4533: 0.0\n",
      "learned_steps 4534: 0.0\n",
      "learned_steps 4535: 0.0\n",
      "learned_steps 4536: 0.0\n",
      "learned_steps 4537: 0.0\n",
      "learned_steps 4538: 0.0\n",
      "learned_steps 4539: 0.0\n",
      "learned_steps 4540: 0.0\n",
      "learned_steps 4541: 0.0\n",
      "learned_steps 4542: 0.0\n",
      "learned_steps 4543: 0.10000000149011612\n",
      "learned_steps 4544: 0.0\n",
      "learned_steps 4545: 0.10000000149011612\n",
      "learned_steps 4546: 0.0\n",
      "learned_steps 4547: 0.0\n",
      "learned_steps 4548: 0.0\n",
      "learned_steps 4549: 0.0\n",
      "learned_steps 4550: 0.0\n",
      "learned_steps 4551: 0.0\n",
      "learned_steps 4552: 0.0\n",
      "learned_steps 4553: 0.0\n",
      "learned_steps 4554: 0.0\n",
      "learned_steps 4555: 0.10000000149011612\n",
      "learned_steps 4556: 0.0\n",
      "learned_steps 4557: 0.0\n",
      "learned_steps 4558: 0.0\n",
      "learned_steps 4559: 0.0\n",
      "learned_steps 4560: 0.0\n",
      "learned_steps 4561: 0.0\n",
      "learned_steps 4562: 0.0\n",
      "learned_steps 4563: 0.0\n",
      "learned_steps 4564: 0.0\n",
      "learned_steps 4565: 0.0\n",
      "learned_steps 4566: 0.0\n",
      "learned_steps 4567: 0.0\n",
      "learned_steps 4568: 0.0\n",
      "learned_steps 4569: 0.0\n",
      "learned_steps 4570: 0.0\n",
      "learned_steps 4571: 0.0\n",
      "learned_steps 4572: 0.0\n",
      "learned_steps 4573: 0.0\n",
      "learned_steps 4574: 0.0\n",
      "learned_steps 4575: 0.0\n",
      "learned_steps 4576: 0.0\n",
      "learned_steps 4577: 0.10000000149011612\n",
      "learned_steps 4578: 0.10000000149011612\n",
      "learned_steps 4579: 0.0\n",
      "learned_steps 4580: 0.0\n",
      "learned_steps 4581: 0.0\n",
      "learned_steps 4582: 0.0\n",
      "learned_steps 4583: 0.0\n",
      "learned_steps 4584: 0.0\n",
      "learned_steps 4585: 0.0\n",
      "learned_steps 4586: 0.0\n",
      "learned_steps 4587: 0.0\n",
      "learned_steps 4588: 0.0\n",
      "learned_steps 4589: 0.0\n",
      "learned_steps 4590: 0.0\n",
      "learned_steps 4591: 0.0\n",
      "learned_steps 4592: 0.0\n",
      "learned_steps 4593: 0.0\n",
      "learned_steps 4594: 0.0\n",
      "learned_steps 4595: 0.0\n",
      "learned_steps 4596: 0.0\n",
      "learned_steps 4597: 0.0\n",
      "learned_steps 4598: 0.0\n",
      "learned_steps 4599: 0.0\n",
      "learned_steps 4600: 0.10000000149011612\n",
      "learned_steps 4601: 0.0\n",
      "learned_steps 4602: 0.0\n",
      "learned_steps 4603: 0.0\n",
      "learned_steps 4604: 0.0\n",
      "learned_steps 4605: 0.0\n",
      "learned_steps 4606: 0.0\n",
      "learned_steps 4607: 0.0\n",
      "learned_steps 4608: 0.0\n",
      "learned_steps 4609: 0.0\n",
      "learned_steps 4610: 0.0\n",
      "learned_steps 4611: 0.0\n",
      "learned_steps 4612: 0.0\n",
      "learned_steps 4613: 0.0\n",
      "learned_steps 4614: 0.0\n",
      "learned_steps 4615: 0.0\n",
      "learned_steps 4616: 0.0\n",
      "learned_steps 4617: 0.10000000149011612\n",
      "learned_steps 4618: 0.0\n",
      "learned_steps 4619: 0.0\n",
      "learned_steps 4620: 0.0\n",
      "learned_steps 4621: 0.0\n",
      "learned_steps 4622: 0.0\n",
      "learned_steps 4623: 0.0\n",
      "learned_steps 4624: 0.0\n",
      "learned_steps 4625: 0.0\n",
      "learned_steps 4626: 0.0\n",
      "learned_steps 4627: 0.0\n",
      "learned_steps 4628: 0.0\n",
      "learned_steps 4629: 0.0\n",
      "learned_steps 4630: 0.10000000149011612\n",
      "learned_steps 4631: 0.0\n",
      "learned_steps 4632: 0.0\n",
      "learned_steps 4633: 0.0\n",
      "learned_steps 4634: 0.0\n",
      "learned_steps 4635: 0.0\n",
      "learned_steps 4636: 0.0\n",
      "learned_steps 4637: 0.0\n",
      "learned_steps 4638: 0.0\n",
      "learned_steps 4639: 0.0\n",
      "learned_steps 4640: 0.0\n",
      "learned_steps 4641: 0.0\n",
      "learned_steps 4642: 0.0\n",
      "learned_steps 4643: 0.0\n",
      "learned_steps 4644: 0.0\n",
      "learned_steps 4645: 0.0\n",
      "learned_steps 4646: 0.0\n",
      "learned_steps 4647: 0.10000000149011612\n",
      "learned_steps 4648: 0.0\n",
      "learned_steps 4649: 0.0\n",
      "learned_steps 4650: 0.0\n",
      "learned_steps 4651: 0.0\n",
      "learned_steps 4652: 0.0\n",
      "learned_steps 4653: 0.0\n",
      "learned_steps 4654: 0.0\n",
      "learned_steps 4655: 0.0\n",
      "learned_steps 4656: 0.0\n",
      "learned_steps 4657: 0.0\n",
      "learned_steps 4658: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned_steps 4659: 0.0\n",
      "learned_steps 4660: 0.0\n",
      "learned_steps 4661: 0.0\n",
      "learned_steps 4662: 0.0\n",
      "learned_steps 4663: 0.0\n",
      "learned_steps 4664: 0.0\n",
      "learned_steps 4665: 0.0\n",
      "learned_steps 4666: 0.0\n",
      "learned_steps 4667: 0.0\n",
      "learned_steps 4668: 0.0\n",
      "learned_steps 4669: 0.0\n",
      "learned_steps 4670: 0.0\n",
      "learned_steps 4671: 0.0\n",
      "learned_steps 4672: 0.0\n",
      "learned_steps 4673: 0.0\n",
      "learned_steps 4674: 0.10000000149011612\n",
      "learned_steps 4675: 0.0\n",
      "learned_steps 4676: 0.0\n",
      "learned_steps 4677: 0.0\n",
      "learned_steps 4678: 0.0\n",
      "learned_steps 4679: 0.10000000149011612\n",
      "learned_steps 4680: 0.0\n",
      "learned_steps 4681: 0.0\n",
      "learned_steps 4682: 0.0\n",
      "learned_steps 4683: 0.0\n",
      "learned_steps 4684: 0.10000000149011612\n",
      "learned_steps 4685: 0.0\n",
      "learned_steps 4686: 0.10000000149011612\n",
      "learned_steps 4687: 0.0\n",
      "learned_steps 4688: 0.0\n",
      "learned_steps 4689: 0.0\n",
      "learned_steps 4690: 0.0\n",
      "learned_steps 4691: 0.0\n",
      "learned_steps 4692: 0.0\n",
      "learned_steps 4693: 0.0\n",
      "learned_steps 4694: 0.0\n",
      "learned_steps 4695: 0.0\n",
      "learned_steps 4696: 0.10000000149011612\n",
      "learned_steps 4697: 0.0\n",
      "learned_steps 4698: 0.0\n",
      "learned_steps 4699: 0.0\n",
      "learned_steps 4700: 0.0\n",
      "learned_steps 4701: 0.0\n",
      "learned_steps 4702: 0.0\n",
      "learned_steps 4703: 0.0\n",
      "learned_steps 4704: 0.0\n",
      "learned_steps 4705: 0.0\n",
      "learned_steps 4706: 0.0\n",
      "learned_steps 4707: 0.0\n",
      "learned_steps 4708: 0.0\n",
      "learned_steps 4709: 0.10000000149011612\n",
      "learned_steps 4710: 0.0\n",
      "learned_steps 4711: 0.0\n",
      "learned_steps 4712: 0.10000000149011612\n",
      "learned_steps 4713: 0.0\n",
      "learned_steps 4714: 0.0\n",
      "learned_steps 4715: 0.0\n",
      "learned_steps 4716: 0.0\n",
      "learned_steps 4717: 0.0\n",
      "learned_steps 4718: 0.0\n",
      "learned_steps 4719: 0.0\n",
      "learned_steps 4720: 0.0\n",
      "learned_steps 4721: 0.0\n",
      "learned_steps 4722: 0.0\n",
      "learned_steps 4723: 0.0\n",
      "learned_steps 4724: 0.10000000149011612\n",
      "learned_steps 4725: 0.0\n",
      "learned_steps 4726: 0.0\n",
      "learned_steps 4727: 0.0\n",
      "learned_steps 4728: 0.0\n",
      "learned_steps 4729: 0.0\n",
      "learned_steps 4730: 0.0\n",
      "learned_steps 4731: 0.0\n",
      "learned_steps 4732: 0.0\n",
      "learned_steps 4733: 0.0\n",
      "learned_steps 4734: 0.0\n",
      "learned_steps 4735: 0.0\n",
      "learned_steps 4736: 0.0\n",
      "learned_steps 4737: 0.0\n",
      "learned_steps 4738: 0.0\n",
      "learned_steps 4739: 0.10000000149011612\n",
      "learned_steps 4740: 0.0\n",
      "learned_steps 4741: 0.0\n",
      "learned_steps 4742: 0.0\n",
      "learned_steps 4743: 0.0\n",
      "learned_steps 4744: 0.0\n",
      "learned_steps 4745: 0.0\n",
      "learned_steps 4746: 0.0\n",
      "learned_steps 4747: 0.0\n",
      "learned_steps 4748: 0.0\n",
      "learned_steps 4749: 0.0\n",
      "learned_steps 4750: 0.0\n",
      "learned_steps 4751: 0.0\n",
      "learned_steps 4752: 0.0\n",
      "learned_steps 4753: 0.0\n",
      "learned_steps 4754: 0.0\n",
      "learned_steps 4755: 0.0\n",
      "learned_steps 4756: 0.0\n",
      "learned_steps 4757: 0.0\n",
      "learned_steps 4758: 0.0\n",
      "learned_steps 4759: 0.0\n",
      "learned_steps 4760: 0.0\n",
      "learned_steps 4761: 0.0\n",
      "learned_steps 4762: 0.10000000149011612\n",
      "learned_steps 4763: 0.0\n",
      "learned_steps 4764: 0.0\n",
      "learned_steps 4765: 0.0\n",
      "learned_steps 4766: 0.0\n",
      "learned_steps 4767: 0.0\n",
      "learned_steps 4768: 0.0\n",
      "learned_steps 4769: 0.0\n",
      "learned_steps 4770: 0.0\n",
      "learned_steps 4771: 0.0\n",
      "learned_steps 4772: 0.0\n",
      "learned_steps 4773: 0.0\n",
      "learned_steps 4774: 0.0\n",
      "learned_steps 4775: 0.0\n",
      "learned_steps 4776: 0.0\n",
      "learned_steps 4777: 0.0\n",
      "learned_steps 4778: 0.0\n",
      "learned_steps 4779: 0.0\n",
      "learned_steps 4780: 0.0\n",
      "learned_steps 4781: 0.0\n",
      "learned_steps 4782: 0.0\n",
      "learned_steps 4783: 0.0\n",
      "learned_steps 4784: 0.0\n",
      "learned_steps 4785: 0.0\n",
      "learned_steps 4786: 0.0\n",
      "learned_steps 4787: 0.0\n",
      "learned_steps 4788: 0.0\n",
      "learned_steps 4789: 0.10000000149011612\n",
      "learned_steps 4790: 0.0\n",
      "learned_steps 4791: 0.0\n",
      "learned_steps 4792: 0.0\n",
      "learned_steps 4793: 0.0\n",
      "learned_steps 4794: 0.0\n",
      "learned_steps 4795: 0.0\n",
      "learned_steps 4796: 0.0\n",
      "learned_steps 4797: 0.0\n",
      "learned_steps 4798: 0.0\n",
      "learned_steps 4799: 0.0\n",
      "learned_steps 4800: 0.0\n",
      "learned_steps 4801: 0.0\n",
      "learned_steps 4802: 0.0\n",
      "learned_steps 4803: 0.0\n",
      "learned_steps 4804: 0.0\n",
      "learned_steps 4805: 0.0\n",
      "learned_steps 4806: 0.0\n",
      "learned_steps 4807: 0.0\n",
      "learned_steps 4808: 0.0\n",
      "learned_steps 4809: 0.0\n",
      "learned_steps 4810: 0.0\n",
      "learned_steps 4811: 0.0\n",
      "learned_steps 4812: 0.0\n",
      "learned_steps 4813: 0.0\n",
      "learned_steps 4814: 0.0\n",
      "learned_steps 4815: 0.0\n",
      "learned_steps 4816: 0.0\n",
      "learned_steps 4817: 0.0\n",
      "learned_steps 4818: 0.0\n",
      "learned_steps 4819: 0.10000000149011612\n",
      "learned_steps 4820: 0.0\n",
      "learned_steps 4821: 0.0\n",
      "learned_steps 4822: 0.0\n",
      "learned_steps 4823: 0.0\n",
      "learned_steps 4824: 0.0\n",
      "learned_steps 4825: 0.0\n",
      "learned_steps 4826: 0.0\n",
      "learned_steps 4827: 0.0\n",
      "learned_steps 4828: 0.0\n",
      "learned_steps 4829: 0.0\n",
      "learned_steps 4830: 0.0\n",
      "learned_steps 4831: 0.0\n",
      "learned_steps 4832: 0.0\n",
      "learned_steps 4833: 0.0\n",
      "learned_steps 4834: 0.0\n",
      "learned_steps 4835: 0.0\n",
      "learned_steps 4836: 0.10000000149011612\n",
      "learned_steps 4837: 0.0\n",
      "learned_steps 4838: 0.0\n",
      "learned_steps 4839: 0.0\n",
      "learned_steps 4840: 0.0\n",
      "learned_steps 4841: 0.0\n",
      "learned_steps 4842: 0.10000000149011612\n",
      "learned_steps 4843: 0.0\n",
      "learned_steps 4844: 0.0\n",
      "learned_steps 4845: 0.0\n",
      "learned_steps 4846: 0.0\n",
      "learned_steps 4847: 0.0\n",
      "learned_steps 4848: 0.0\n",
      "learned_steps 4849: 0.0\n",
      "learned_steps 4850: 0.0\n",
      "learned_steps 4851: 0.0\n",
      "learned_steps 4852: 0.0\n",
      "learned_steps 4853: 0.0\n",
      "learned_steps 4854: 0.0\n",
      "learned_steps 4855: 0.0\n",
      "learned_steps 4856: 0.10000000149011612\n",
      "learned_steps 4857: 0.0\n",
      "learned_steps 4858: 0.0\n",
      "learned_steps 4859: 0.0\n",
      "learned_steps 4860: 0.0\n",
      "learned_steps 4861: 0.0\n",
      "learned_steps 4862: 0.0\n",
      "learned_steps 4863: 0.0\n",
      "learned_steps 4864: 0.0\n",
      "learned_steps 4865: 0.0\n",
      "learned_steps 4866: 0.0\n",
      "learned_steps 4867: 0.0\n",
      "learned_steps 4868: 0.0\n",
      "learned_steps 4869: 0.0\n",
      "learned_steps 4870: 0.0\n",
      "learned_steps 4871: 0.0\n",
      "learned_steps 4872: 0.0\n",
      "learned_steps 4873: 0.0\n",
      "learned_steps 4874: 0.0\n",
      "learned_steps 4875: 0.10000000149011612\n",
      "learned_steps 4876: 0.0\n",
      "learned_steps 4877: 0.0\n",
      "learned_steps 4878: 0.0\n",
      "learned_steps 4879: 0.0\n",
      "learned_steps 4880: 0.0\n",
      "learned_steps 4881: 0.10000000149011612\n",
      "learned_steps 4882: 0.0\n",
      "learned_steps 4883: 0.0\n",
      "learned_steps 4884: 0.0\n",
      "learned_steps 4885: 0.0\n",
      "learned_steps 4886: 0.0\n",
      "learned_steps 4887: 0.0\n",
      "learned_steps 4888: 0.0\n",
      "learned_steps 4889: 0.0\n",
      "learned_steps 4890: 0.0\n",
      "learned_steps 4891: 0.0\n",
      "learned_steps 4892: 0.0\n",
      "learned_steps 4893: 0.0\n",
      "learned_steps 4894: 0.0\n",
      "learned_steps 4895: 0.0\n",
      "learned_steps 4896: 0.0\n",
      "learned_steps 4897: 0.0\n",
      "learned_steps 4898: 0.0\n",
      "learned_steps 4899: 0.0\n",
      "learned_steps 4900: 0.0\n",
      "learned_steps 4901: 0.0\n",
      "learned_steps 4902: 0.0\n",
      "learned_steps 4903: 0.0\n",
      "learned_steps 4904: 0.0\n",
      "learned_steps 4905: 0.0\n",
      "learned_steps 4906: 0.0\n",
      "learned_steps 4907: 0.0\n",
      "learned_steps 4908: 0.0\n",
      "learned_steps 4909: 0.0\n",
      "learned_steps 4910: 0.0\n",
      "learned_steps 4911: 0.0\n",
      "learned_steps 4912: 0.0\n",
      "learned_steps 4913: 0.0\n",
      "learned_steps 4914: 0.0\n",
      "learned_steps 4915: 0.0\n",
      "learned_steps 4916: 0.0\n",
      "learned_steps 4917: 0.0\n",
      "learned_steps 4918: 0.0\n",
      "learned_steps 4919: 0.0\n",
      "learned_steps 4920: 0.0\n",
      "learned_steps 4921: 0.0\n",
      "learned_steps 4922: 0.0\n",
      "learned_steps 4923: 0.0\n",
      "learned_steps 4924: 0.0\n",
      "learned_steps 4925: 0.0\n",
      "learned_steps 4926: 0.0\n",
      "learned_steps 4927: 0.0\n",
      "learned_steps 4928: 0.0\n",
      "learned_steps 4929: 0.0\n",
      "learned_steps 4930: 0.0\n",
      "learned_steps 4931: 0.0\n",
      "learned_steps 4932: 0.0\n",
      "learned_steps 4933: 0.0\n",
      "learned_steps 4934: 0.0\n",
      "learned_steps 4935: 0.0\n",
      "learned_steps 4936: 0.0\n",
      "learned_steps 4937: 0.0\n",
      "learned_steps 4938: 0.0\n",
      "learned_steps 4939: 0.0\n",
      "learned_steps 4940: 0.0\n",
      "learned_steps 4941: 0.0\n",
      "learned_steps 4942: 0.0\n",
      "learned_steps 4943: 0.0\n",
      "learned_steps 4944: 0.0\n",
      "learned_steps 4945: 0.0\n",
      "learned_steps 4946: 0.0\n",
      "learned_steps 4947: 0.0\n",
      "learned_steps 4948: 0.0\n",
      "learned_steps 4949: 0.0\n",
      "learned_steps 4950: 0.10000000149011612\n",
      "learned_steps 4951: 0.0\n",
      "learned_steps 4952: 0.0\n",
      "learned_steps 4953: 0.0\n",
      "learned_steps 4954: 0.0\n",
      "learned_steps 4955: 0.0\n",
      "learned_steps 4956: 0.0\n",
      "learned_steps 4957: 0.0\n",
      "learned_steps 4958: 0.0\n",
      "learned_steps 4959: 0.0\n",
      "learned_steps 4960: 0.0\n",
      "learned_steps 4961: 0.0\n",
      "learned_steps 4962: 0.0\n",
      "learned_steps 4963: 0.0\n",
      "learned_steps 4964: 0.0\n",
      "learned_steps 4965: 0.0\n",
      "learned_steps 4966: 0.0\n",
      "learned_steps 4967: 0.0\n",
      "learned_steps 4968: 0.0\n",
      "learned_steps 4969: 0.0\n",
      "learned_steps 4970: 0.0\n",
      "learned_steps 4971: 0.0\n",
      "learned_steps 4972: 0.0\n",
      "learned_steps 4973: 0.0\n",
      "learned_steps 4974: 0.0\n",
      "learned_steps 4975: 0.0\n",
      "learned_steps 4976: 0.0\n",
      "learned_steps 4977: 0.0\n",
      "learned_steps 4978: 0.0\n",
      "learned_steps 4979: 0.0\n",
      "learned_steps 4980: 0.0\n",
      "learned_steps 4981: 0.0\n",
      "learned_steps 4982: 0.0\n",
      "learned_steps 4983: 0.0\n",
      "learned_steps 4984: 0.0\n",
      "learned_steps 4985: 0.0\n",
      "learned_steps 4986: 0.0\n",
      "learned_steps 4987: 0.0\n",
      "learned_steps 4988: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned_steps 4989: 0.0\n",
      "learned_steps 4990: 0.0\n",
      "learned_steps 4991: 0.0\n",
      "learned_steps 4992: 0.0\n",
      "learned_steps 4993: 0.10000000149011612\n",
      "learned_steps 4994: 0.0\n",
      "learned_steps 4995: 0.0\n",
      "learned_steps 4996: 0.0\n",
      "learned_steps 4997: 0.0\n",
      "learned_steps 4998: 0.0\n",
      "learned_steps 4999: 0.0\n",
      "learned_steps 5000: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "def get_extra_obs(states, actions):\n",
    "    ''' \n",
    "        return a list contains other agents states and actions, len = num_agents\n",
    "        states: list of states by each agent\n",
    "        actions: List of action by each agent\n",
    "    '''\n",
    "    extra_obs = []\n",
    "    # print(f\"actions : {actions}\")\n",
    "    for i in range(states.shape[0]):\n",
    "        list_ = []\n",
    "        # states\n",
    "        list_.extend(states[i])\n",
    "        # agent's action\n",
    "        list_.extend(actions[i])\n",
    "        # other agent's actions\n",
    "        list_.extend(actions[np.arange(len(actions))!= i][0])\n",
    "        extra_obs.append(list_)\n",
    "    return extra_obs\n",
    "\n",
    "\n",
    "class Actor_critic_model(nn.Module):\n",
    "    def __init__(self, params_dir, input_dim, act_size, num_agents):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.act_size = act_size\n",
    "        self.num_agents = num_agents\n",
    "        self.params = parse_params(params_dir)\n",
    "        self.mu = [self.create_actor() for _ in range(self.num_agents)]\n",
    "        self.std = [nn.Parameter(torch.ones(1, act_size)) for _ in range(self.num_agents)]\n",
    "        self.val = self.create_critic()\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_in')\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def create_actor(self):\n",
    "        module_list = nn.ModuleList()\n",
    "        layer = nn.Sequential()\n",
    "        fc = nn.Linear(self.input_dim, self.params['hidden_dim'])\n",
    "        layer.add_module(f\"fc_layer_1\", fc)\n",
    "        # layer.add_module(f\"bn_layer_1\",\n",
    "                        # nn.BatchNorm1d(self.params['hidden_dim']))\n",
    "        # layer.add_module(f\"RELU_layer_1\", nn.LeakyReLU())\n",
    "        layer.add_module(f\"RELU_layer_1\", nn.ReLU())\n",
    "        module_list.append(layer)\n",
    "        # module_list.apply(self._init_weights)\n",
    "        self.add_hidden_layer(module_list, self.params['actor_h_num'],\n",
    "                         self.params['hidden_dim'], self.params['hidden_dim'])\n",
    "        module_list.append(nn.Sequential(nn.Linear(self.params['hidden_dim'],\n",
    "                                          self.act_size)))\n",
    "        # module_list.apply(self._init_weights)\n",
    "        return module_list\n",
    "    \n",
    "    def create_critic(self):\n",
    "        module_list = nn.ModuleList()\n",
    "        layer = nn.Sequential()\n",
    "        fc = nn.Linear(self.input_dim + self.act_size * 2, self.params['hidden_dim'])\n",
    "        layer.add_module(f\"fc_layer_1\", fc)\n",
    "        # layer.add_module(f\"bn_layer_1\",\n",
    "                        # nn.BatchNorm1d(self.params['hidden_dim']))\n",
    "        # layer.add_module(f\"RELU_layer_1\", nn.LeakyReLU())\n",
    "        layer.add_module(f\"RELU_layer_1\", nn.ReLU())\n",
    "        module_list.append(layer)\n",
    "        self.add_hidden_layer(module_list, self.params['critic_h_num'],\n",
    "                         self.params['hidden_dim'], self.params['hidden_dim'])\n",
    "        module_list.append(nn.Sequential(nn.Linear(self.params['hidden_dim'], 1)))\n",
    "        # module_list.apply(self._init_weights)\n",
    "        return module_list\n",
    "    \n",
    "    def add_hidden_layer(self, module_list, num_hidden_layer,\n",
    "                         input_dim, output_dim):\n",
    "        if num_hidden_layer == 0:\n",
    "            return\n",
    "        for i in range(1, num_hidden_layer+1):\n",
    "            layer = nn.Sequential()\n",
    "            fc = nn.Linear(input_dim, output_dim)\n",
    "            layer.add_module(f\"fc_layer_{i}\", fc)\n",
    "            # layer.add_module(f\"bn_layer_{i}\",\n",
    "                          #    nn.BatchNorm1d(output_dim))\n",
    "            # layer.add_module(f\"RELU_layer_{i}\", nn.LeakyReLU())\n",
    "            layer.add_module(f\"RELU_layer_{i}\", nn.ReLU())\n",
    "            module_list.append(layer)\n",
    "            \n",
    "    def forward(self, states, actor=True, train=True):\n",
    "        '''\n",
    "            If actor is True, output actions and log probabilities FloatTensor\n",
    "            If Critic (actor = False), output state value FloatTensor\n",
    "        '''\n",
    "        x_ = torch.FloatTensor(states).to(self.device)\n",
    "        if actor:\n",
    "            # forward in actor path\n",
    "            mus = torch.zeros(self.num_agents, self.act_size, dtype=torch.float)\n",
    "            dists = []\n",
    "            acts = torch.zeros(self.num_agents, self.act_size, dtype=torch.float)\n",
    "            lps = torch.zeros(self.num_agents, self.act_size, dtype=torch.float)\n",
    "            for i in range(self.num_agents):\n",
    "                \n",
    "                mu_ = x_[i]\n",
    "                for m in self.mu[i]:\n",
    "                    mu_ = m(mu_)\n",
    "                mus[i] = (mu_)\n",
    "                dists.append(torch.distributions.Normal(mus[i], self.std[i]))\n",
    "                act_ = dists[i].sample()\n",
    "                if train:\n",
    "                    # only return log probabilities in training phases\n",
    "                    lps[i] = dists[i].log_prob(act_)\n",
    "                    acts[i] = torch.clamp(act_, -1, 1)\n",
    "                    return acts, lps\n",
    "                # return only actions in executation phases\n",
    "                return torch.clamp(act_, -1, 1)\n",
    "        # forward in value path\n",
    "        for v in self.val:\n",
    "            x_ = v(x_)\n",
    "        return x_\n",
    "\n",
    "def parse_params(params_dir):\n",
    "    with open(params_dir) as fp:\n",
    "        params = json.load(fp)\n",
    "    return params\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, device, num_agents, params_dir, state_size, action_size):\n",
    "        self.model = Actor_critic_model(params_dir, state_size, action_size, num_agents).to(device)\n",
    "        \n",
    "        # I should try a version without target, just like A2C\n",
    "        self.target = Actor_critic_model(params_dir, state_size, action_size, num_agents).to(device)\n",
    "        self.device = device\n",
    "        self.num_agents = num_agents\n",
    "        self.params = self.model.params\n",
    "        self.optimizer = optim.Adam(self.model.parameters(),\n",
    "                                    lr=self.params['lr'])\n",
    "                                    # lr=0.0001)\n",
    "\n",
    "    def __call__(self, states):\n",
    "        # mu, std, val, etp = self.model(states)\n",
    "        actions, log_prob = self.model(states)\n",
    "        return actions, log_prob\n",
    "\n",
    "    def step(self, memories):\n",
    "        '''\n",
    "        second edition\n",
    "            experiences:\n",
    "                list with n_steps_taken * [actions, rewards, log_probs,\n",
    "                                           not_dones, state_values]:\n",
    "                    actions (tensor: num agents * num actions)\n",
    "                    rewards (list: size = num agents)\n",
    "                    log_probs (tensor: num agents * num actions)\n",
    "                    not_dones (np array: size = num agents)\n",
    "                    state_values (list: size = num agents)\n",
    "        '''\n",
    "        loss = [0.0] * self.num_agents\n",
    "        for idx in range(self.num_agents):\n",
    "            actions, rewards, log_probs, not_dones, states, next_states= memories[idx].spit()\n",
    "            # print(f\"state_values : {state_values}\")\n",
    "            rewards = torch.FloatTensor(rewards).view(-1, 1)\n",
    "            #print(f\"len(rewards[0]) - 1 : {len(rewards) - 1}\")\n",
    "            not_dones = torch.FloatTensor(not_dones).to(device).unsqueeze(1)\n",
    "            # print(rewards)\n",
    "            state_values = self.model(states, actor=False)\n",
    "            next_values = self.target(next_states, actor=False)\n",
    "            returns = rewards + self.params['gamma'] * not_dones * next_values.detach()\n",
    "            advantage_  = rewards + self.params['gamma'] * not_dones * next_values.detach() - state_values.detach()\n",
    "            # print(f\"log_probs.shape : {log_probs.shape}\")\n",
    "            # print(f\"advantage_.shape : {advantage_.shape}\")\n",
    "            # print(f\"state_values[i].shape : {state_values[i].shape}\")\n",
    "            # print(f\"return_.shape : {return_.shape}\")\n",
    "            # print(f\"processed_experience : {processed_experience}\")\n",
    "            log_probs = torch.stack(log_probs)\n",
    "            policy_loss = -(log_probs) * advantage_\n",
    "            value_loss = (0.5 * (returns - state_values).pow(2))\n",
    "            self.optimizer.zero_grad()\n",
    "            loss[idx] = ((policy_loss + value_loss.unsqueeze(1)).mean())\n",
    "            # print(f\"loss[idx] : {loss[idx]}\")\n",
    "            if torch.isnan(loss[idx]).any():\n",
    "                print('Nan in loss function')\n",
    "                pass\n",
    "            if idx == self.num_agents -1:\n",
    "                loss[idx].backward()\n",
    "            else:\n",
    "                loss[idx].backward(retain_graph=True)\n",
    "            nn.utils.clip_grad_norm_(self.model.parameters(), self.model.params['grad_clip'])\n",
    "            self.optimizer.step()\n",
    "        self.soft_udpate()\n",
    "        \n",
    "    def soft_udpate(self):\n",
    "        for tp, lp in zip(self.target.parameters(),\n",
    "                          self.model.parameters()):\n",
    "            tp.data.copy_(self.params['TAU']*lp.data +\n",
    "                          (1.0-self.params['TAU'])*tp.data)\n",
    "            \n",
    "    def hard_udpate(self):\n",
    "        for tp, lp in zip(self.target.parameters(),\n",
    "                          self.model.parameters()):\n",
    "            tp.data.copy_(lp.data)\n",
    "\n",
    "\n",
    "class Experience():\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        #self.extra_into = []\n",
    "        self.log_probs = []\n",
    "        self.not_dones = []\n",
    "        self.states = []\n",
    "        self.next_states = []\n",
    "        # self.etp = []\n",
    "\n",
    "    def add(self, actions, rewards, log_probs, not_dones,\n",
    "            states, next_states):\n",
    "        self.actions.append(actions)\n",
    "        self.rewards.append(rewards)\n",
    "        #self.extra_into.append(extra_into)\n",
    "        self.log_probs.append(log_probs)\n",
    "        self.not_dones.append(not_dones)\n",
    "        self.states.append(states)\n",
    "        self.next_states.append(next_states)\n",
    "        # self.etp.append(etp)\n",
    "\n",
    "    def spit(self):\n",
    "        return (self.actions[1:], self.rewards[1:], self.log_probs[1:],\n",
    "                self.not_dones[1:],\n",
    "                self.states[1:], self.next_states[1:])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rewards)\n",
    "    \n",
    "params_dir = f\"./params.txt\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = Agent(device, num_agents, params_dir, state_size, action_size)\n",
    "\n",
    "def plot_scores(scores):\n",
    "    fig, ax = plt.subplots(1, figsize=(8, 8))\n",
    "    plt.plot(np.arange(len(scores)), scores)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "scores_window = deque(maxlen=100)\n",
    "memories = [Experience() for _ in range(num_agents)]\n",
    "learned_steps = 0\n",
    "while learned_steps < 5000:\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    states_ = env_info.vector_observations\n",
    "    scores = np.zeros(num_agents)\n",
    "    done = [False] * num_agents\n",
    "    actions_ = np.random.random([num_agents, action_size])\n",
    "    log_prob_ = torch.rand(num_agents, action_size)\n",
    "    rewards_ = [0] * num_agents\n",
    "    states_plus = get_extra_obs(states, actions)\n",
    "    steps = 0\n",
    "    # while not np.any(done):\n",
    "    while steps < 20:\n",
    "        actions_next, log_prob_next = agent(states)\n",
    "        next_states_plus = get_extra_obs(states, actions_next.cpu().numpy())\n",
    "        env_info = env.step(actions_next.detach().cpu().numpy())[brain_name]\n",
    "        done = env_info.local_done\n",
    "        not_done_ = (1 - np.array(done))\n",
    "        for idx in range(num_agents):\n",
    "            memories[idx].add(actions_[idx], rewards_[idx], log_prob_[idx],\n",
    "                              not_done_[idx], states_plus[idx], next_states_plus[idx])\n",
    "        steps += 1\n",
    "\n",
    "        rewards_ = env_info.rewards\n",
    "        states = env_info.vector_observations\n",
    "        actions_ = actions_next\n",
    "        log_prob_ = log_prob_next\n",
    "        states_plus = next_states_plus\n",
    "        scores += rewards_\n",
    "        if (len(memories[0].actions) % 400 == 0) and (len(memories[0].actions) >1):\n",
    "            agent.step(memories)\n",
    "            learned_steps += 1\n",
    "            memories = [Experience() for _ in range(num_agents)]\n",
    "            print(f\"learned_steps {learned_steps}: {np.max(scores)}\")\n",
    "        if learned_steps % 1000 == 0:\n",
    "            agent.hard_udpate()\n",
    "        if np.any(done):\n",
    "            # memories = Experience()\n",
    "            # print(f\" steps : {steps}\")\n",
    "            break\n",
    "        # print(scores)\n",
    "\n",
    "    scores_window.append(np.max(scores))\n",
    "    if (len(scores_window)) == 100 and ((sum(scores_window) / len(scores_window)) > 0.5):\n",
    "        torch.save(agent.model.state_dict(), agent.params['working_dir'])\n",
    "        print(f\"Envinroment solved in episode{steps}!\")\n",
    "        print(f\"Score: {scores_window}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actor_critic_model(\n",
       "  (val): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (fc_layer_1): Linear(in_features=28, out_features=128, bias=True)\n",
       "      (RELU_layer_1): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (fc_layer_1): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (RELU_layer_1): ReLU()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (fc_layer_2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (RELU_layer_2): ReLU()\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts = torch.zeros(2, 2, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log_probs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-8a725997eb36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'log_probs' is not defined"
     ]
    }
   ],
   "source": [
    "log_probs = torch.stack(log_probs).to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "b = torch.stack([torch.rand(2, 3),\n",
    "                      torch.rand(2, 3)])\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(2).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_plus = get_extra_obs(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, -0.009999999776482582]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0.0 for _ in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "\n",
    "class Actor_critic_model(nn.Module):\n",
    "    def __init__(self, params_dir, input_dim, act_size, num_agents):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.act_size = act_size\n",
    "        self.num_agents = num_agents\n",
    "        self.params = parse_params(params_dir)\n",
    "        self.mu = [self.create_actor() for _ in range(self.num_agents)]\n",
    "        self.std = [nn.Parameter(torch.ones(1, act_size)) for _ in range(self.num_agents)]\n",
    "        self.val = self.create_critic()\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_in')\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def create_actor(self):\n",
    "        module_list = nn.ModuleList()\n",
    "        layer = nn.Sequential()\n",
    "        fc = nn.Linear(self.input_dim, self.params['hidden_dim'])\n",
    "        layer.add_module(f\"fc_layer_1\", fc)\n",
    "        # layer.add_module(f\"bn_layer_1\",\n",
    "                        # nn.BatchNorm1d(self.params['hidden_dim']))\n",
    "        # layer.add_module(f\"RELU_layer_1\", nn.LeakyReLU())\n",
    "        layer.add_module(f\"RELU_layer_1\", nn.ReLU())\n",
    "        module_list.append(layer)\n",
    "        # module_list.apply(self._init_weights)\n",
    "        self.add_hidden_layer(module_list, self.params['actor_h_num'],\n",
    "                         self.params['hidden_dim'], self.params['hidden_dim'])\n",
    "        module_list.append(nn.Sequential(nn.Linear(self.params['hidden_dim'],\n",
    "                                          self.act_size)))\n",
    "        # module_list.apply(self._init_weights)\n",
    "        return module_list\n",
    "    \n",
    "    def create_critic(self):\n",
    "        module_list = nn.ModuleList()\n",
    "        layer = nn.Sequential()\n",
    "        fc = nn.Linear(self.input_dim + self.act_size * 2, self.params['hidden_dim'])\n",
    "        layer.add_module(f\"fc_layer_1\", fc)\n",
    "        # layer.add_module(f\"bn_layer_1\",\n",
    "                        # nn.BatchNorm1d(self.params['hidden_dim']))\n",
    "        # layer.add_module(f\"RELU_layer_1\", nn.LeakyReLU())\n",
    "        layer.add_module(f\"RELU_layer_1\", nn.ReLU())\n",
    "        module_list.append(layer)\n",
    "        self.add_hidden_layer(module_list, self.params['critic_h_num'],\n",
    "                         self.params['hidden_dim'], self.params['hidden_dim'])\n",
    "        module_list.append(nn.Sequential(nn.Linear(self.params['hidden_dim'], 1)))\n",
    "        # module_list.apply(self._init_weights)\n",
    "        return module_list\n",
    "    \n",
    "    def add_hidden_layer(self, module_list, num_hidden_layer,\n",
    "                         input_dim, output_dim):\n",
    "        if num_hidden_layer == 0:\n",
    "            return\n",
    "        for i in range(1, num_hidden_layer+1):\n",
    "            layer = nn.Sequential()\n",
    "            fc = nn.Linear(input_dim, output_dim)\n",
    "            layer.add_module(f\"fc_layer_{i}\", fc)\n",
    "            # layer.add_module(f\"bn_layer_{i}\",\n",
    "                          #    nn.BatchNorm1d(output_dim))\n",
    "            # layer.add_module(f\"RELU_layer_{i}\", nn.LeakyReLU())\n",
    "            layer.add_module(f\"RELU_layer_{i}\", nn.ReLU())\n",
    "            module_list.append(layer)\n",
    "            \n",
    "    def forward(self, states, actor=True, train=True):\n",
    "        '''\n",
    "            If actor is True, output actions and log probabilities FloatTensor\n",
    "            If Critic (actor = False), output state value FloatTensor\n",
    "        '''\n",
    "        x_ = torch.FloatTensor(states).to(self.device)\n",
    "        if actor:\n",
    "            # forward in actor path\n",
    "            mus = torch.zeros(self.num_agents, self.act_size)\n",
    "            dists = []\n",
    "            acts = torch.zeros(self.num_agents, self.act_size)\n",
    "            lps = torch.zeros(self.num_agents, self.act_size)\n",
    "            for i in range(self.num_agents):\n",
    "                \n",
    "                mu_ = x_[i]\n",
    "                for m in self.mu[i]:\n",
    "                    mu_ = m(mu_)\n",
    "                mus[i] = (mu_)\n",
    "                dists.append(torch.distributions.Normal(mus[i], self.std[i]))\n",
    "                act_ = dists[i].sample()\n",
    "                if train:\n",
    "                    # only return log probabilities in training phases\n",
    "                    lps[i] = dists[i].log_prob(act_)\n",
    "                    acts[i] = torch.clamp(act_, -1, 1)\n",
    "                    return acts, lps\n",
    "                # return only actions in executation phases\n",
    "                return torch.clamp(act_, -1, 1)\n",
    "        # forward in value path\n",
    "        for v in self.val:\n",
    "            x_ = v(x_)\n",
    "        return x_\n",
    "\n",
    "def parse_params(params_dir):\n",
    "    with open(params_dir) as fp:\n",
    "        params = json.load(fp)\n",
    "    return params\n",
    "        \n",
    "params_dir = f\"./params.txt\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_a = Actor_critic_model(params_dir, len(states[0]), 2, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        , -7.43639946, -1.5       , -0.        ,  0.        ,\n",
       "         6.69487906,  5.96076012, -0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        , -7.73019552, -1.5       ,  0.        ,  0.        ,\n",
       "        -6.69487906,  5.96076012,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extra_obs(states, actions):\n",
    "    ''' \n",
    "        return a list contains other agents states and actions, len = num_agents\n",
    "        states: list of states by each agent\n",
    "        actions: List of action by each agent\n",
    "    '''\n",
    "    extra_obs = []\n",
    "    # print(f\"actions : {actions}\")\n",
    "    for i in range(states.shape[0]):\n",
    "        list_ = []\n",
    "        # states\n",
    "        list_.extend(states[i])\n",
    "        # agent's action\n",
    "        list_.extend(actions[i])\n",
    "        # other agent's actions\n",
    "        list_.extend(actions[np.arange(len(actions))!= i][0])\n",
    "        extra_obs.append(list_)\n",
    "    return extra_obs\n",
    "extra_obs =  get_extra_obs(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 1])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor(torch.rand(28,10)).unsqueeze(1).shape\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = model_a(torch.FloatTensor(torch.rand(10, 28)),actor=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = model_a(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "data type not understood",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e39b8454bad6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: data type not understood"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class Actor_critic_model(nn.Module):\n",
    "    def __init__(self, params_dir, input_dim, act_size, num_agents):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.act_size = act_size\n",
    "        self.num_agents = num_agents\n",
    "        self.params = parse_params(params_dir)\n",
    "        self.mu = [self.create_actor() for _ in range(self.num_agents)]\n",
    "        self.std = [nn.Parameter(torch.ones(1, act_size)) for _ in range(self.num_agents)]\n",
    "        self.val = self.create_critic()\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_in')\n",
    "            m.bias.data.fill_(0.01)\n",
    "            \n",
    "    def get_extra_obs(self, states, actions):\n",
    "        ''' \n",
    "            return a list contains other agents states and actions, len = num_agents\n",
    "            states: list of states by each agent\n",
    "            actions: List of action by each agent\n",
    "        '''\n",
    "        extra_obs = []\n",
    "        # print(f\"actions : {actions}\")\n",
    "        for i in range(states.shape[0]):\n",
    "            list_ = []\n",
    "            list_.extend(states[np.arange(len(states))!= i][0])\n",
    "            list_.extend(actions[np.arange(len(actions))!= i][0])\n",
    "            extra_obs.append(list_)\n",
    "        return extra_obs\n",
    "\n",
    "    def create_actor(self):\n",
    "        module_list = nn.ModuleList()\n",
    "        layer = nn.Sequential()\n",
    "        fc = nn.Linear(self.input_dim, self.params['hidden_dim'])\n",
    "        layer.add_module(f\"fc_layer_1\", fc)\n",
    "        # layer.add_module(f\"bn_layer_1\",\n",
    "                        # nn.BatchNorm1d(self.params['hidden_dim']))\n",
    "        # layer.add_module(f\"RELU_layer_1\", nn.LeakyReLU())\n",
    "        layer.add_module(f\"RELU_layer_1\", nn.ReLU())\n",
    "        module_list.append(layer)\n",
    "        # module_list.apply(self._init_weights)\n",
    "        self.add_hidden_layer(module_list, self.params['actor_h_num'],\n",
    "                         self.params['hidden_dim'], self.params['hidden_dim'])\n",
    "        module_list.append(nn.Sequential(nn.Linear(self.params['hidden_dim'],\n",
    "                                          self.act_size)))\n",
    "        # module_list.apply(self._init_weights)\n",
    "        return module_list\n",
    "    \n",
    "    def create_critic(self):\n",
    "        module_list = nn.ModuleList()\n",
    "        layer = nn.Sequential()\n",
    "        fc = nn.Linear(self.input_dim*2 + self.act_size, self.params['hidden_dim'])\n",
    "        layer.add_module(f\"fc_layer_1\", fc)\n",
    "        # layer.add_module(f\"bn_layer_1\",\n",
    "                        # nn.BatchNorm1d(self.params['hidden_dim']))\n",
    "        # layer.add_module(f\"RELU_layer_1\", nn.LeakyReLU())\n",
    "        layer.add_module(f\"RELU_layer_1\", nn.ReLU())\n",
    "        module_list.append(layer)\n",
    "        self.add_hidden_layer(module_list, self.params['critic_h_num'],\n",
    "                         self.params['hidden_dim'], self.params['hidden_dim'])\n",
    "        module_list.append(nn.Sequential(nn.Linear(self.params['hidden_dim'], 1)))\n",
    "        # module_list.apply(self._init_weights)\n",
    "        return module_list\n",
    "    \n",
    "    def add_hidden_layer(self, module_list, num_hidden_layer,\n",
    "                         input_dim, output_dim):\n",
    "        if num_hidden_layer == 0:\n",
    "            return\n",
    "        for i in range(1, num_hidden_layer+1):\n",
    "            layer = nn.Sequential()\n",
    "            fc = nn.Linear(input_dim, output_dim)\n",
    "            layer.add_module(f\"fc_layer_{i}\", fc)\n",
    "            # layer.add_module(f\"bn_layer_{i}\",\n",
    "                          #    nn.BatchNorm1d(output_dim))\n",
    "            # layer.add_module(f\"RELU_layer_{i}\", nn.LeakyReLU())\n",
    "            layer.add_module(f\"RELU_layer_{i}\", nn.ReLU())\n",
    "            module_list.append(layer)\n",
    "            \n",
    "    def forward(self, states):\n",
    "        x_ = states.copy()\n",
    "        x_ = torch.FloatTensor(x_).to(self.device)\n",
    "\n",
    "        mus = torch.zeros(self.num_agents, self.act_size)\n",
    "        dists = []\n",
    "        acts = torch.zeros(self.num_agents, self.act_size)\n",
    "        lps = torch.zeros(self.num_agents, self.act_size)\n",
    "        actions = np.zeros((self.num_agents, self.act_size))\n",
    "        for i in range(self.num_agents):\n",
    "            mu_ = x_[i]\n",
    "            for m in self.mu[i]:\n",
    "                mu_ = m(mu_)\n",
    "            mus[i] = (mu_)\n",
    "            dists.append(torch.distributions.Normal(mus[i], self.std[i]))\n",
    "            act_ = dists[i].sample()\n",
    "            lps[i] = dists[i].log_prob(act_)\n",
    "            actions[i]= torch.clamp(act_, -1, 1).numpy()\n",
    "            acts[i] = torch.clamp(act_, -1, 1)\n",
    "        \n",
    "        # print(f\"extra_obs: {extra_obs}\")\n",
    "        extra_obs = self.get_extra_obs(states, actions)\n",
    "        combined_states_ = torch.FloatTensor(extra_obs).detach().requires_grad_()\n",
    "        # print(f\"combined_states: {combined_states_}\")\n",
    "        val_x = torch.cat([x_, combined_states_], dim=1)\n",
    "        # print(f\"val_x shape: {val_x.shape}\")\n",
    "        for v in self.val:\n",
    "            val_x = v(val_x)\n",
    "        return acts, lps, val_x\n",
    "\n",
    "def parse_params(params_dir):\n",
    "    with open(params_dir) as fp:\n",
    "        params = json.load(fp)\n",
    "    return params\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, device, num_agents, params_dir, state_size, action_size):\n",
    "        self.model = Actor_critic_model(params_dir, state_size, action_size, num_agents).to(device)\n",
    "        self.device = device\n",
    "        self.num_agents = num_agents\n",
    "        self.params = self.model.params\n",
    "        self.optimizer = optim.Adam(self.model.parameters(),\n",
    "                                    # lr=self.params['lr'])\n",
    "                                    lr=0.0001)\n",
    "\n",
    "    def __call__(self, states):\n",
    "        # mu, std, val, etp = self.model(states)\n",
    "        actions, log_prob, val = self.model(states)\n",
    "        return actions, log_prob, val\n",
    "\n",
    "    def step(self, memories):\n",
    "        '''\n",
    "        second edition\n",
    "            experiences:\n",
    "                list with n_steps_taken * [actions, rewards, log_probs,\n",
    "                                           not_dones, state_values]:\n",
    "                    actions (tensor: num agents * num actions)\n",
    "                    rewards (list: size = num agents)\n",
    "                    log_probs (tensor: num agents * num actions)\n",
    "                    not_dones (np array: size = num agents)\n",
    "                    state_values (list: size = num agents)\n",
    "        '''\n",
    "        loss = 0.0\n",
    "        for idx in range(self.num_agents):\n",
    "            actions, rewards, log_probs, not_dones, state_values = memories[idx].spit()\n",
    "            # print(f\"state_values : {state_values}\")\n",
    "            rewards = torch.FloatTensor(rewards).view(-1, 1)\n",
    "            #print(f\"len(rewards[0]) - 1 : {len(rewards) - 1}\")\n",
    "\n",
    "            processed_experience = [None] * (len(rewards) - 1)\n",
    "            #print(f\"processed_experience : {processed_experience}\")\n",
    "            # return_  = state_values[-1].detach()   \n",
    "            return_  = state_values[-1].detach()\n",
    "            not_dones = torch.FloatTensor(not_dones).to(device).unsqueeze(1)\n",
    "            # print(rewards)\n",
    "            for i in reversed(range(len(rewards)-1)):\n",
    "                # print(f\"pnot_dones : {not_dones}\")\n",
    "                # print(f\"not_dones[i+1 : {not_dones[i+1]}\")\n",
    "                not_done_ = not_dones[i+1]\n",
    "                reward_ = rewards[i]\n",
    "                return_ = reward_ + self.params['gamma'] * not_done_ * return_\n",
    "                next_value_ = state_values[i+1]\n",
    "                advantage_  = reward_ + self.params['gamma'] * not_done_ * next_value_.detach() - state_values[i].detach()\n",
    "                # print(f\"log_probs[i].shape : {log_probs[i].shape}\")\n",
    "                # print(f\"advantage_.shape : {advantage_.shape}\")\n",
    "                # print(f\"state_values[i].shape : {state_values[i].shape}\")\n",
    "                # print(f\"return_.shape : {return_.shape}\")\n",
    "                processed_experience[i] = [log_probs[i].unsqueeze(0), advantage_, state_values[i], return_]\n",
    "            # print(f\"processed_experience : {processed_experience}\")\n",
    "            log_probs, advantages, values, returns = map(lambda x: torch.cat(x, dim=0), zip(*processed_experience))\n",
    "            policy_loss = (-log_probs * (advantages.unsqueeze(1)))\n",
    "            value_loss = (0.5 * (returns - values).pow(2))\n",
    "            self.optimizer.zero_grad()\n",
    "            loss += ((policy_loss + value_loss.unsqueeze(1)).mean())\n",
    "        if torch.isnan(loss).any():\n",
    "            print('Nan in loss function')\n",
    "            pass\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.model.parameters(), self.model.params['grad_clip'])\n",
    "        self.optimizer.step()\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "class Experience():\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        #self.extra_into = []\n",
    "        self.log_probs = []\n",
    "        self.not_dones = []\n",
    "        self.state_values = []\n",
    "        # self.etp = []\n",
    "\n",
    "    def add(self, actions, rewards, log_probs, not_dones, state_values):\n",
    "        self.actions.append(actions)\n",
    "        self.rewards.append(rewards)\n",
    "        #self.extra_into.append(extra_into)\n",
    "        self.log_probs.append(log_probs)\n",
    "        self.not_dones.append(not_dones)\n",
    "        self.state_values.append(state_values)\n",
    "        # self.etp.append(etp)\n",
    "\n",
    "    def spit(self):\n",
    "        return (self.actions, self.rewards, self.log_probs, self.not_dones,\n",
    "                self.state_values)\n",
    "    \n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, buffer_size, action_size, batch_size, seed, device):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        # create a named tuple object to store training samples\n",
    "        self.experience = namedtuple(\"Experience\",\n",
    "                                     field_names=['actions', \"rewards\", \"log_probs\",\n",
    "                                                  \"not_dones\", \"state_values\"])\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "    def add(self, actions, rewards, log_probs, not_dones, state_values):\n",
    "        '''\n",
    "            create a new namedtuple for each experience and append it to memory\n",
    "            All inputs are in numpy format\n",
    "        '''\n",
    "        self.memory.append(self.experience(\n",
    "                actions, rewards, log_probs, not_dones, state_values))\n",
    "\n",
    "    def sample(self):\n",
    "        sampled_exp = random.sample(self.memory, k=self.batch_size)\n",
    "        actions = torch.from_numpy(\n",
    "            np.vstack([e.actions for e in sampled_exp if e is not None])\n",
    "            ).float().to(self.device)\n",
    "        rewards = torch.from_numpy(\n",
    "            np.vstack([e.reward for e in sampled_exp if e is not None])\n",
    "            ).float().to(self.device)\n",
    "        log_probs = torch.from_numpy(\n",
    "            np.vstack([e.log_probs for e in sampled_exp if e is not None])\n",
    "            ).long().to(self.device)\n",
    "        not_dones = torch.from_numpy(\n",
    "            np.vstack([e.not_dones for e in sampled_exp if e is not None])\n",
    "            ).float().to(self.device)\n",
    "        state_values = torch.from_numpy(\n",
    "                 np.vstack(\n",
    "                         [e.state_values for e in sampled_exp if e is not None]\n",
    "                             ).astype(np.uint8)).float().to(self.device)\n",
    "        return (actions, rewards, log_probs, not_dones, state_values)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "params_dir = f\"./params.txt\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = Agent(device, num_agents, params_dir, state_size, action_size)\n",
    "\n",
    "def plot_scores(scores):\n",
    "    fig, ax = plt.subplots(1, figsize=(8, 8))\n",
    "    plt.plot(np.arange(len(scores)), scores)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "scores_window = deque(maxlen=100)\n",
    "batch_size = 100\n",
    "seed = 99\n",
    "# version 3\n",
    "\n",
    "for i in range(1):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    states_ = env_info.vector_observations\n",
    "    scores = np.zeros(num_agents)\n",
    "    memories = [ReplayBuffer(buffer_size, action_size, batch_size, seed, device) for _ in range(num_agents)]\n",
    "    done = [False] * num_agents\n",
    "    steps = 0\n",
    "    # while not np.any(done):\n",
    "    while steps < 99:\n",
    "        actions_, log_prob_, state_values_ = agent(states)\n",
    "        env_info = env.step(actions_.detach().cpu().numpy())[brain_name]\n",
    "        next_states_ = env_info.vector_observations\n",
    "        rewards_ = env_info.rewards\n",
    "        done = env_info.local_done\n",
    "        not_done_ = (1 - np.array(done))\n",
    "        for idx in range(num_agents):\n",
    "            memories[idx].add(actions_[idx], rewards_[idx], log_prob_[idx], not_done_[idx], state_values_[idx])\n",
    "        steps += 1\n",
    "        if np.any(done):\n",
    "            memories = Experience()\n",
    "            break\n",
    "        if steps % 5 == 0:\n",
    "            agent.step(memories)\n",
    "            memories = [Experience() for _ in range(num_agents)]\n",
    "        states = next_states_\n",
    "        scores += rewards_\n",
    "        # print(scores)\n",
    "    print(f\"Episode {i}: {np.max(scores)}\")\n",
    "    scores_window.append(np.max(scores))\n",
    "    if (len(scores_window)) == 100 and ((sum(scores_window) / len(scores_window)) > 0.5):\n",
    "        torch.save(agent.model.state_dict(), agent.params['working_dir'])\n",
    "        print(f\"Envinroment solved in episode{i}!\")\n",
    "        print(f\"Score: {scores_window}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TennisBrain'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TennisBrain\n"
     ]
    }
   ],
   "source": [
    "for env_ in envss:\n",
    "    print(env_.brain_names[0])\n",
    "    env_info = env_.reset(train_mode=True)[env_.brain_names[0]]\n",
    "    states_ = env_info.vector_observations\n",
    "    print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n",
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "envss = [UnityEnvironment(file_name=\"/data/Tennis_Linux_NoVis/Tennis\") for _ in range(num_agents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5711032491c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "agent.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dir = f\"./params.txt\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = Agent(device, num_agents, params_dir, state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0401],\n",
      "        [ 0.2980]])\n"
     ]
    }
   ],
   "source": [
    "print(state_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiences = memories.spit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 0.0],\n",
       " [0.0, 0.0],\n",
       " [0.0, 0.0],\n",
       " [0.0, 0.0],\n",
       " [0.0, 0.0],\n",
       " [0.0, -0.009999999776482582]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-a8c9799b79b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mmemories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mExperience\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_agents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_states_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-109-c7fae244036e>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, memories)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_agents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnot_dones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mprocessed_experience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "def plot_scores(scores):\n",
    "    fig, ax = plt.subplots(1, figsize=(8, 8))\n",
    "    plt.plot(np.arange(len(scores)), scores)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "scores_window = deque(maxlen=100)\n",
    "\n",
    "for i in range(1):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    states_ = env_info.vector_observations\n",
    "    scores = np.zeros(num_agents)\n",
    "    memories = [Experience() for _ in range(num_agents)]\n",
    "    done = [False] * num_agents\n",
    "    steps = 0\n",
    "    # while not np.any(done):\n",
    "    while True:\n",
    "        actions_, log_prob_, state_values_ = agent(states)\n",
    "        env_info = env.step(actions_.detach().cpu().numpy())[brain_name]\n",
    "        next_states_ = env_info.vector_observations\n",
    "        rewards_ = env_info.rewards\n",
    "        done = env_info.local_done\n",
    "        not_done_ = (1 - np.array(done))\n",
    "        for i in range(num_agents):\n",
    "            memories[i].add(actions_[i], rewards_[i], log_prob_[i], not_done_[i], state_values_[i])\n",
    "        steps += 1\n",
    "        if np.any(done):\n",
    "            memories = Experience()\n",
    "            break\n",
    "        if steps % 5 == 0:\n",
    "            agent.step(memories)\n",
    "            memories = [Experience() for _ in range(num_agents)]\n",
    "        states = next_states_\n",
    "        scores += rewards_\n",
    "        # print(scores)\n",
    "    print(f\"Episode {i}: {np.max(scores)}\")\n",
    "    scores_window.append(np.max(scores))\n",
    "    if (len(scores_window)) == 100 and ((sum(scores_window) / len(scores_window)) > 0.5):\n",
    "        torch.save(agent.model.state_dict(), agent.params['working_dir'])\n",
    "        print(f\"Envinroment solved in episode{i}!\")\n",
    "        print(f\"Score: {scores_window}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined_states: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000, -7.7612, -1.5000,  0.0000,  0.0000, -7.0182,\n",
      "          4.1753,  0.0000,  0.0000, -0.7648,  1.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000, -7.7354, -1.5000, -0.0000,  0.0000,  7.0182,\n",
      "          4.1753, -0.0000,  0.0000, -1.0000,  0.5948]])\n",
      "combined_states: tensor([[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,  -7.0355,  -1.5000,   0.0000,   0.0000,\n",
      "          -7.0182,   4.0537,   0.0000,   0.0000,  -9.4327,  -1.5589,\n",
      "         -23.9720,  -0.9810,  -7.0182,   3.3866, -23.9720,  -0.9810,\n",
      "          -0.7648,   1.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,  -7.3583,  -1.5000,  -0.0000,   0.0000,\n",
      "           7.0182,   4.0537,  -0.0000,   0.0000,  -7.1720,  -1.5589,\n",
      "           1.8623,  -0.9810,   7.0182,   3.3866,   1.8623,  -0.9810,\n",
      "          -1.0000,   0.5948]])\n",
      "combined_states: tensor([[ -7.0355,  -1.5000,   0.0000,   0.0000,  -7.0182,   4.0537,\n",
      "           0.0000,   0.0000,  -9.4327,  -1.5589, -23.9720,  -0.9810,\n",
      "          -7.0182,   3.3866, -23.9720,  -0.9810,  -6.6578,  -1.7158,\n",
      "          27.7490,  -1.9620,  -7.0182,   2.6214,  27.7490,  -1.9620,\n",
      "          -0.7648,   1.0000],\n",
      "        [ -7.3583,  -1.5000,  -0.0000,   0.0000,   7.0182,   4.0537,\n",
      "          -0.0000,   0.0000,  -7.1720,  -1.5589,   1.8623,  -0.9810,\n",
      "           7.0182,   3.3866,   1.8623,  -0.9810,  -6.2770,  -0.9177,\n",
      "           8.9500,   6.0190,   7.0182,   2.6214,   8.9500,   6.0190,\n",
      "          -1.0000,   0.5948]])\n",
      "combined_states: tensor([[ -9.4327,  -1.5589, -23.9720,  -0.9810,  -7.0182,   3.3866,\n",
      "         -23.9720,  -0.9810,  -6.6578,  -1.7158,  27.7490,  -1.9620,\n",
      "          -7.0182,   2.6214,  27.7490,  -1.9620,  -6.2350,  -1.8524,\n",
      "           4.2276,   0.0000,  -7.0182,   1.7582,   4.2276,   0.0000,\n",
      "          -0.7648,   1.0000],\n",
      "        [ -7.1720,  -1.5589,   1.8623,  -0.9810,   7.0182,   3.3866,\n",
      "           1.8623,  -0.9810,  -6.2770,  -0.9177,   8.9500,   6.0190,\n",
      "           7.0182,   2.6214,   8.9500,   6.0190,  -4.6109,  -0.3747,\n",
      "          16.6611,   5.0380,   7.0182,   1.7582,  16.6611,   5.0380,\n",
      "          -1.0000,   0.5948]])\n",
      "combined_states: tensor([[ -6.6578,  -1.7158,  27.7490,  -1.9620,  -7.0182,   2.6214,\n",
      "          27.7490,  -1.9620,  -6.2350,  -1.8524,   4.2276,   0.0000,\n",
      "          -7.0182,   1.7582,   4.2276,   0.0000,  -3.2350,  -1.8522,\n",
      "          30.0000,   0.0000,  -7.0182,   0.8385,  30.0000,   0.0000,\n",
      "          -0.7648,   1.0000],\n",
      "        [ -6.2770,  -0.9177,   8.9500,   6.0190,   7.0182,   2.6214,\n",
      "           8.9500,   6.0190,  -4.6109,  -0.3747,  16.6611,   5.0380,\n",
      "           7.0182,   1.7582,  16.6611,   5.0380,  -1.6109,   0.0703,\n",
      "          30.0000,   4.0570,   7.0182,   0.8385,  30.0000,   4.0570,\n",
      "          -1.0000,   0.5948]])\n",
      "0\n",
      "self.num_agents : 2\n",
      "im here with retain grad\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-5382fcea24ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_agents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mexperiences_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mmemories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExperience\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_states_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-100-8abb73a71214>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, experiences, idx)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"im here with retain grad\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"finished the first backward\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'grad_clip'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "def plot_scores(scores):\n",
    "    fig, ax = plt.subplots(1, figsize=(8, 8))\n",
    "    plt.plot(np.arange(len(scores)), scores)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "scores_window = deque(maxlen=100)\n",
    "\n",
    "for i in range(1):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    states_ = env_info.vector_observations\n",
    "    scores = np.zeros(num_agents)\n",
    "    memories = [Experience() for _ in range(num_agents)]\n",
    "    done = [False] * num_agents\n",
    "    steps = 0\n",
    "    # while not np.any(done):\n",
    "    while True:\n",
    "        actions_, log_prob_, state_values_ = agent(states)\n",
    "        env_info = env.step(actions_.detach().cpu().numpy())[brain_name]\n",
    "        next_states_ = env_info.vector_observations\n",
    "        rewards_ = env_info.rewards\n",
    "        done = env_info.local_done\n",
    "        not_done_ = (1 - np.array(done))\n",
    "        for i in range(num_agents):\n",
    "            memories[i].add(actions_[i], rewards_[i], log_prob_[i], not_done_[i], state_values_[i])\n",
    "        steps += 1\n",
    "        if np.any(done):\n",
    "            memories = Experience()\n",
    "            break\n",
    "        if steps % 5 == 0:\n",
    "            for i in range(num_agents):\n",
    "                experiences_ = memories[i].spit()\n",
    "                agent.step(experiences, i)\n",
    "            memories = Experience()\n",
    "        states = next_states_\n",
    "        scores += rewards_\n",
    "        # print(scores)\n",
    "    print(f\"Episode {i}: {np.max(scores)}\")\n",
    "    scores_window.append(np.max(scores))\n",
    "    if (len(scores_window)) == 100 and ((sum(scores_window) / len(scores_window)) > 0.5):\n",
    "        torch.save(agent.model.state_dict(), agent.params['working_dir'])\n",
    "        print(f\"Envinroment solved in episode{i}!\")\n",
    "        print(f\"Score: {scores_window}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores2 = np.zeros(num_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.rand(2,37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1089,  0.9336,  0.3417,  0.7516,  0.8893,  0.0894,  0.7528,\n",
      "          0.2575,  0.6720,  0.6967,  0.3517,  0.5943,  0.7501,  0.1009,\n",
      "          0.9530,  0.5058,  0.3703,  0.1080,  0.2512,  0.2847,  0.7189,\n",
      "          0.5767,  0.6629,  0.9613,  0.2417,  0.6889,  0.2270,  0.0936,\n",
      "          0.9169,  0.2505,  0.8809,  0.3256,  0.9014,  0.5382,  0.3768,\n",
      "          0.4793,  0.1952],\n",
      "        [ 0.7707,  0.2375,  0.8081,  0.4530,  0.2677,  0.4489,  0.9745,\n",
      "          0.5429,  0.0577,  0.6078,  0.6531,  0.8821,  0.8981,  0.0596,\n",
      "          0.4699,  0.6121,  0.1488,  0.7653,  0.5284,  0.0629,  0.9455,\n",
      "          0.6540,  0.9659,  0.5065,  0.9747,  0.0727,  0.9179,  0.9386,\n",
      "          0.5085,  0.9132,  0.0913,  0.4783,  0.9041,  0.7388,  0.6664,\n",
      "          0.7652,  0.9673]])\n",
      "torch.Size([2, 37])\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(a.shape)\n",
    "# print(a.view(-1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.         -7.98782539 -1.5        -0.          0.\n",
      "   6.14030886  5.99607611 -0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "  -7.28886175 -1.5         0.          0.         -6.14030886  5.99607611\n",
      "   0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "states_ = np.reshape(env_info.vector_observations, (1,num_agents*state_size))\n",
    "print(states_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 48)\n"
     ]
    }
   ],
   "source": [
    "a = np.reshape(env_info.vector_observations, (1,num_agents*state_size))\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i : 0 j :[ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -7.98782539 -1.5        -0.          0.\n",
      "  6.14030886  5.99607611 -0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      " -7.28886175 -1.5         0.          0.         -6.14030886  5.99607611\n",
      "  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "for i, j in enumerate(a):\n",
    "    print(f\"i : {i} j :{j}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.         -7.98782539 -1.5        -0.          0.\n",
      "  -7.11741829  5.96076012 -0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "  -7.28886175 -1.5         0.          0.          7.11741829  5.96076012\n",
      "   0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(np.reshape(env_info.vector_observations, (1,num_agents*state_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_states = env_info.vector_observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
